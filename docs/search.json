[
  {
    "objectID": "posts/perceptron-post/index.html",
    "href": "posts/perceptron-post/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "This blog post aims to investigate the function of the perceptron algorithm by performing various experiments with it. These experiments includes tests of the algorithm on linearly separable and inseparable data and data with dimensions greater than 2. These tests will show how the algorithm functions of different data, graphing the perceptron’s loss and decision boundaries over time. Then, the algorithm will be tested for performance with minibatching, examining loss convergance at different batch sizes.\n\n\nhttps://github.com/jblake05/jblake05.github.io/blob/main/posts/perceptron-post/perceptron.py\n\n\n\nMy perceptron class has two implementations of the grad function, one with minibatching and one without. The grad function \\(1[s_iy_i &lt; 0]y_ix_i\\) is implemented by first calculating \\(s_i\\) as \\(\\langle w_i, x_i\\rangle\\) using torch.inner. Then I cast the expression \\(s_iy_i &lt; 0\\) to a boolean by multiplying it by 1. Finally I multiplied that expression by y_i and the vector x_i, flattening the expression to one dimension to add it to the weight.\nFor the minibatching function grade_k, I implemented the equation \\(\\frac{α}{k} \\sum \\limits _{l=1} ^{k} 1[\\langle w, x_{i_k} \\rangle y_{i_k} &lt; 0]y_{i_k} x_{i_k}\\) The score was formed the same way as the previous equation. The boolean map and y vector are resized from size n to size (n, 1) using y[:, None] to allow for multiplication with X. This matrix is then summed along each column to create a tensor the same size as the weight. Finally, \\(\\frac{α}{k}\\) is multiplied by the tensor to complete the equation.\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nfrom perceptron import Perceptron, PerceptronOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\n\nTo test the perceptron, I first generated data to test using code from the perceptron lecture. Also using code from the lecture, I plotted the data, differentiating the groups of X (i.e. if they corrolate with y = 1 or -1) by color.\n\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nTo test my perceptron, I ran the minimal training loop provided in the perceptron lecture. In doing so, I generate random values of X and y to act as inputs for the perceptron’s optimizer. I also collect a vector of all unique (i.e. different from the previous) loss values, which are graphed below.\n\ntorch.manual_seed(130)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    if loss == 0:\n        break\n    \n    # not part of the update: just for tracking our progress    \n    prev_loss = loss\n\n    loss = p.loss(X, y) \n    if (prev_loss != loss):\n        loss_vec.append(loss)\n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nHere, we see that the perceptron’s loss converges to 0 on linearly separable data. Due to the randomly indexed data, the loss does not decrease monotonically, but trends lower over time.\nSimilar to the previous test, this experiment tests the perceptron’s performance on a 2D dataset that is linearly separable. Instead of just plotting the loss, this code uses the draw_line function to show the decision boundary change over time relative to the dataset.\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nLike the last experiment, this block of code keeps track of the loss over time, graphing the dataset, x_i, and decision boundary at each step.\n\n# adapted from lecture 7 (Perceptron)\ntorch.manual_seed(3619)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (10, 6)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    ax = axarr.ravel()[current_ax]\n\n    if loss == 0:\n        break\n    \n    # not part of the update: just for tracking our progress    \n    prev_loss = loss\n\n    loss = p.loss(X, y) \n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    old_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w)\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n\n    if (prev_loss != loss):\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\n\n\n\n\n\n\n\n\nThis data shows the decision boundary improve over time, though not consistently (see the jump in loss between the second and third plots). Eventually, the loss converges on zero, plotting the decision boundary directly between the two groups.\nTo test the perceptron’s behavior on linearly inseparable data, I create new data that has a higher amount of noise. This will result in data that doesn’t have as clear groups, with outliers from one group that appear to be in the other group based on their values.\n\nX2, y2 = perceptron_data(noise = 0.35)\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X2, y2, ax)\n\n\n\n\n\n\n\n\nHere, we graph the decision boundary over time in a similar way to the linearly separable experiment. Since the data is linearly inseparable, though, the perceptron cannot achieve a loss of 0. As such, using the same function would result in an infinite loop and thus needs to break after a certain amount of iterations (here I chose 1000). For better visibility, only the first and last loss changes’ decision boundaries were plotted. The number of iterations the plot was created at are included in the plot’s label.\n\n# adapted from lecture 7 (Perceptron)\ntorch.manual_seed(634)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X2.size()[0]\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (12, 4)\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\niter = 0\nmax_steps = 1000\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    if (iter == max_steps):\n        break\n\n    ax = axarr.ravel()[current_ax]\n\n    if loss == 0:\n        break\n    \n    # not part of the update: just for tracking our progress    \n    prev_loss = loss\n\n    loss = p.loss(X2, y2) \n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X2[[i],:]\n    y_i = y2[i]\n\n    old_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w)\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    if (prev_loss != loss):\n        loss = p.loss(X2, y2).item()\n        loss_vec.append(loss)\n        \n        if iter == 0 or iter == 949:\n            plot_perceptron_data(X2, y2, ax)\n            draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n            draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n            \n            ax.scatter(X2[i,0],X2[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n            ax.set_title(f\"loss = {loss:.3f}, iter = {iter}\")\n            ax.set(xlim = (-1, 2), ylim = (-1, 2))\n            \n            if iter == 0:\n                current_ax += 1\n    \n    iter += 1\n\n\n\n\n\n\n\n\nHere we can see that, although the data is linearly inseparable, the loss still decreases immensely over the course of 1000 iterations. The decision boundary, then, gets much closer to separating the data over time. The loss can be seen more clearly by plotting its values over time.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Loss Vector Entry\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAgain, the loss values decrease over time, but not steadily. Instead of decreasing to 0, of course, the loss stays around 0 without converging to it.\nFor the final experiment on non-minibatch data, I tested the perceptron’s results on data with more than 2 dimensions. To test the progress of the experiment, I kept track of the loss values over time and printed them out at the end of the loop.\n\nX3, y3 = perceptron_data(p_dims = 5)\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\nloss_vec = []\n\nstep = 0\nmax_steps = 10000\n\nwhile loss &gt; 0:\n    if step == max_steps:\n        break\n\n    prev_loss = loss\n\n    loss = p.loss(X3, y3) \n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X3[[i],:]\n    y_i = y3[i]\n\n    opt.step(x_i, y_i)\n\n    if prev_loss != loss:\n        loss = p.loss(X3, y3).item()\n        loss_vec.append(loss)\n    step += 1\n\nprint(loss_vec, step)\n\n[0.25333333015441895, 0.23333333432674408, 0.2566666603088379, 0.009999999776482582, 0.0] 46\n\n\nThis shows that the perceptron can converge on 5-dimensional data that is linearly separable. As the printed loss vector shows, the perceptron’s loss converges to 0 in 5 changes of loss over the course of 46 steps. As such, the model correctly predicts data at 100% accuracy for the training data.\nNext, I set up a graphing function for testing perceptron minibatching. Again, this function is similar to the other decision graphing functions above. Instead of randomly choosing one index for the data, however, the function chooses a set of indices using torch’s randperm function. This function also uses my perceptron’s step_k and grad_k functions to test the minibatch on batches of size k. For reference, the grad_k implementation was described at the top of this post.\n\ndef graph_k(X, y, k, alpha, rows=5, cols=5, xdim=10, ydim=6, seed=3619):\n    # adapted from lecture 7 (Perceptron)\n    torch.manual_seed(seed)\n\n    # instantiate a model and an optimizer\n    p = Perceptron() \n    opt = PerceptronOptimizer(p)\n\n    loss = 1.0\n\n    # for keeping track of loss values\n    loss_vec = []\n\n    n = X.size()[0]\n\n    # set up the figure\n    plt.rcParams[\"figure.figsize\"] = (xdim, ydim)\n    fig, axarr = plt.subplots(rows, cols, sharex = True, sharey = True)\n    markers = [\"o\", \",\"]\n    marker_map = {-1 : 0, 1 : 1}\n\n    # initialize for main loop\n    current_ax = 0\n    loss = 1\n    loss_vec = []\n    iter =  0\n\n    while loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n        ax = axarr.ravel()[current_ax]\n\n        if loss == 0:\n            break\n        \n        # not part of the update: just for tracking our progress    \n        prev_loss = loss\n\n        loss = p.loss(X, y) \n\n        # pick a random data point\n        ix = torch.randperm(X.size(0))[:k]\n        x_i = X[ix,:]\n        y_i = y[ix]\n\n        old_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w)\n        \n        # perform a perceptron update using the random data point\n        opt.step_k(x_i, y_i, alpha, k)\n\n        if (prev_loss != loss):\n            plot_perceptron_data(X, y, ax)\n            draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n            loss = p.loss(X, y).item()\n            loss_vec.append(loss)\n            draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n            # try ix instead of i here\n            ax.scatter(X[ix,0],X[ix,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n            # draw_line(w, -10, 10, ax, color = \"black\")\n            ax.set_title(f\"loss = {loss:.3f}\")\n            ax.set(xlim = (-1, 2), ylim = (-1, 2))\n            current_ax += 1\n            iter += 1\n\nFirst, I graphed the minibatch on a batch size of 1. This was done to test the performance of the batch gradient and step functions, particularly in comparison to the non-batched perceptron functions. As the graph shows, the perceptron finds the separating line between the two groups roughly as quickly as the original algorithm.\n\ngraph_k(X, y, 1, 0.1, 2, 4)\n\n\n\n\n\n\n\n\nSecond, this test shows that a minibatch with a batch size of 10 converges on a two-dimensional, linearly-seperable dataset given enough time. This test notably takes more time to converge than a perceptron with a batch size of 1.\n\ngraph_k(X, y, 10, 0.1, rows=5, cols=5, xdim=16, ydim=14, seed=9000)\n\n\n\n\n\n\n\n\nFinally, to test the minibatch performance on linearly inseparable data, I created a block of code that terminates after 10000 steps, calling opt.step_k() with a batch size of n (the size of the data). After running this code for 10000, the loss is plotted over time.\n\n# adapted from lecture 7 (Perceptron)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X2.size()[0]\n\n# initialize for main loop\nloss = 1\nloss_vec = []\n\nmax_steps = 10000\nstep = 0\n\nk = X2.shape[0]\nalpha = 0.01\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    if (step &gt;= max_steps):\n        break\n\n    ax = axarr.ravel()[current_ax]\n\n    if loss == 0:\n        break\n        \n    # not part of the update: just for tracking our progress    \n    prev_loss = loss\n\n    loss = p.loss(X2, y2) \n\n    # pick a random data point\n    # i = torch.randint(n, size = (1,))\n    ix = torch.randperm(X2.size(0))[:k]\n    x_i = X2[ix,:]\n    y_i = y2[ix]\n\n    old_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w)\n        \n    # perform a perceptron update using the random data point\n    opt.step_k(x_i, y_i, alpha, k)\n\n    loss = p.loss(X2, y2).item()\n    loss_vec.append(loss)\n    step += 1\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Minibatch k = n Perceptron Iterations\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\n\nloss_vec[len(loss_vec) - 1]\n\n0.03999999910593033\n\n\nWhile this test doesn’t converge at 0, it does show that running the minibatch with a batchsize of n converges to a low loss value.\n\nloss = p.loss(X, y) # ====\n# y_ = 2*y - 1 # O(n)\n# (1.0*(self.score(X)*y_ &lt; 0)).mean() ==== \n# 1 * torch.matmul(X, torch.t(self.w)) * y &lt; 0 = O(n)\n\n# pick a random data point\ni = torch.randint(n, size = (1,)) # O(1)\nx_i = X[[i],:] # O(1)\ny_i = y[i] # O(1)\n\nold_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w) # O(1) OR O(p)\n    \n# perform a perceptron update using the random data point\nopt.step(x_i, y_i) # grad could be O(p)\n\n\nloss = p.loss(X, y) # O(np)\n\n# pick a random data point\nix = torch.randperm(X.size(0))[:k] #O(k)\nx_i = X[ix,:] # O(k)\ny_i = y[ix] # O(k)\n        \n# perform a perceptron update using the random data point\nopt.step_k(x_i, y_i, alpha, k)\n\n# self.model.loss(X, y) # for an X of dim k * p and a y of size k,\n# this has a runtime of O(kp)\n# self.model.w += self.model.grad_k(X, y, alpha, k)\n\n# s_i = torch.inner(torch.t(self.w), X) # p * (k*p) = O(kp) results in size k*p\n# return torch.sum(((1 * (s_i*y &lt; 0))[:, None]*y[:, None]*X), axis=0)\n# k*p*k * k * k * p\n\nloss = p.loss(X, y).item() # O(np)\n\n\n\n\n\nFor the runtime of the perceptron, a single iteration without minibatching will have a runtime of O(n) if n &gt; p and a runtime of O(p) if p &gt; n. The O(n) appears in the loss calculation’s matrix multiplication and the O(p) appears in the gradient calculation. In the minibatch implementation, the runtime is O(kp) (seen in the grad_k calculation step) if kp &gt; n and O(n) (seen in the matrix multiplication) if n &gt; kp.\n\n\n\nIn the creation and testing of the perceptron algorithm, I found that – while it seems to work well for linearly separable data – it can be a bit awkward to run on linearly inseparable data. On two dimensional data, the perceptron converged to zero quickly for linearly separable data. While it isn’t possible for this to occur on linearly inseparable data, it did progressively improve over time before plateauing before zero. This ability to achieve zero loss with linearly separable data holds at higher dimensions, as well. Similarly, I found that a minibatch perceptron could converge to zero on linearly separable data (though at slower rates than non-batched perceptrons at higher batch sizes) and to a low non-zero value on inseparable data with a large batch size."
  },
  {
    "objectID": "posts/perceptron-post/index.html#abstract",
    "href": "posts/perceptron-post/index.html#abstract",
    "title": "Perceptron",
    "section": "",
    "text": "This blog post aims to investigate the function of the perceptron algorithm by performing various experiments with it. These experiments includes tests of the algorithm on linearly separable and inseparable data and data with dimensions greater than 2. These tests will show how the algorithm functions of different data, graphing the perceptron’s loss and decision boundaries over time. Then, the algorithm will be tested for performance with minibatching, examining loss convergance at different batch sizes.\n\n\nhttps://github.com/jblake05/jblake05.github.io/blob/main/posts/perceptron-post/perceptron.py\n\n\n\nMy perceptron class has two implementations of the grad function, one with minibatching and one without. The grad function \\(1[s_iy_i &lt; 0]y_ix_i\\) is implemented by first calculating \\(s_i\\) as \\(\\langle w_i, x_i\\rangle\\) using torch.inner. Then I cast the expression \\(s_iy_i &lt; 0\\) to a boolean by multiplying it by 1. Finally I multiplied that expression by y_i and the vector x_i, flattening the expression to one dimension to add it to the weight.\nFor the minibatching function grade_k, I implemented the equation \\(\\frac{α}{k} \\sum \\limits _{l=1} ^{k} 1[\\langle w, x_{i_k} \\rangle y_{i_k} &lt; 0]y_{i_k} x_{i_k}\\) The score was formed the same way as the previous equation. The boolean map and y vector are resized from size n to size (n, 1) using y[:, None] to allow for multiplication with X. This matrix is then summed along each column to create a tensor the same size as the weight. Finally, \\(\\frac{α}{k}\\) is multiplied by the tensor to complete the equation.\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nfrom perceptron import Perceptron, PerceptronOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\n\nTo test the perceptron, I first generated data to test using code from the perceptron lecture. Also using code from the lecture, I plotted the data, differentiating the groups of X (i.e. if they corrolate with y = 1 or -1) by color.\n\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nTo test my perceptron, I ran the minimal training loop provided in the perceptron lecture. In doing so, I generate random values of X and y to act as inputs for the perceptron’s optimizer. I also collect a vector of all unique (i.e. different from the previous) loss values, which are graphed below.\n\ntorch.manual_seed(130)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    if loss == 0:\n        break\n    \n    # not part of the update: just for tracking our progress    \n    prev_loss = loss\n\n    loss = p.loss(X, y) \n    if (prev_loss != loss):\n        loss_vec.append(loss)\n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nHere, we see that the perceptron’s loss converges to 0 on linearly separable data. Due to the randomly indexed data, the loss does not decrease monotonically, but trends lower over time.\nSimilar to the previous test, this experiment tests the perceptron’s performance on a 2D dataset that is linearly separable. Instead of just plotting the loss, this code uses the draw_line function to show the decision boundary change over time relative to the dataset.\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nLike the last experiment, this block of code keeps track of the loss over time, graphing the dataset, x_i, and decision boundary at each step.\n\n# adapted from lecture 7 (Perceptron)\ntorch.manual_seed(3619)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (10, 6)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    ax = axarr.ravel()[current_ax]\n\n    if loss == 0:\n        break\n    \n    # not part of the update: just for tracking our progress    \n    prev_loss = loss\n\n    loss = p.loss(X, y) \n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    old_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w)\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n\n    if (prev_loss != loss):\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\n\n\n\n\n\n\n\n\nThis data shows the decision boundary improve over time, though not consistently (see the jump in loss between the second and third plots). Eventually, the loss converges on zero, plotting the decision boundary directly between the two groups.\nTo test the perceptron’s behavior on linearly inseparable data, I create new data that has a higher amount of noise. This will result in data that doesn’t have as clear groups, with outliers from one group that appear to be in the other group based on their values.\n\nX2, y2 = perceptron_data(noise = 0.35)\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X2, y2, ax)\n\n\n\n\n\n\n\n\nHere, we graph the decision boundary over time in a similar way to the linearly separable experiment. Since the data is linearly inseparable, though, the perceptron cannot achieve a loss of 0. As such, using the same function would result in an infinite loop and thus needs to break after a certain amount of iterations (here I chose 1000). For better visibility, only the first and last loss changes’ decision boundaries were plotted. The number of iterations the plot was created at are included in the plot’s label.\n\n# adapted from lecture 7 (Perceptron)\ntorch.manual_seed(634)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X2.size()[0]\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (12, 4)\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\niter = 0\nmax_steps = 1000\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    if (iter == max_steps):\n        break\n\n    ax = axarr.ravel()[current_ax]\n\n    if loss == 0:\n        break\n    \n    # not part of the update: just for tracking our progress    \n    prev_loss = loss\n\n    loss = p.loss(X2, y2) \n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X2[[i],:]\n    y_i = y2[i]\n\n    old_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w)\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    if (prev_loss != loss):\n        loss = p.loss(X2, y2).item()\n        loss_vec.append(loss)\n        \n        if iter == 0 or iter == 949:\n            plot_perceptron_data(X2, y2, ax)\n            draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n            draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n            \n            ax.scatter(X2[i,0],X2[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n            ax.set_title(f\"loss = {loss:.3f}, iter = {iter}\")\n            ax.set(xlim = (-1, 2), ylim = (-1, 2))\n            \n            if iter == 0:\n                current_ax += 1\n    \n    iter += 1\n\n\n\n\n\n\n\n\nHere we can see that, although the data is linearly inseparable, the loss still decreases immensely over the course of 1000 iterations. The decision boundary, then, gets much closer to separating the data over time. The loss can be seen more clearly by plotting its values over time.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Loss Vector Entry\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAgain, the loss values decrease over time, but not steadily. Instead of decreasing to 0, of course, the loss stays around 0 without converging to it.\nFor the final experiment on non-minibatch data, I tested the perceptron’s results on data with more than 2 dimensions. To test the progress of the experiment, I kept track of the loss values over time and printed them out at the end of the loop.\n\nX3, y3 = perceptron_data(p_dims = 5)\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\nloss_vec = []\n\nstep = 0\nmax_steps = 10000\n\nwhile loss &gt; 0:\n    if step == max_steps:\n        break\n\n    prev_loss = loss\n\n    loss = p.loss(X3, y3) \n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X3[[i],:]\n    y_i = y3[i]\n\n    opt.step(x_i, y_i)\n\n    if prev_loss != loss:\n        loss = p.loss(X3, y3).item()\n        loss_vec.append(loss)\n    step += 1\n\nprint(loss_vec, step)\n\n[0.25333333015441895, 0.23333333432674408, 0.2566666603088379, 0.009999999776482582, 0.0] 46\n\n\nThis shows that the perceptron can converge on 5-dimensional data that is linearly separable. As the printed loss vector shows, the perceptron’s loss converges to 0 in 5 changes of loss over the course of 46 steps. As such, the model correctly predicts data at 100% accuracy for the training data.\nNext, I set up a graphing function for testing perceptron minibatching. Again, this function is similar to the other decision graphing functions above. Instead of randomly choosing one index for the data, however, the function chooses a set of indices using torch’s randperm function. This function also uses my perceptron’s step_k and grad_k functions to test the minibatch on batches of size k. For reference, the grad_k implementation was described at the top of this post.\n\ndef graph_k(X, y, k, alpha, rows=5, cols=5, xdim=10, ydim=6, seed=3619):\n    # adapted from lecture 7 (Perceptron)\n    torch.manual_seed(seed)\n\n    # instantiate a model and an optimizer\n    p = Perceptron() \n    opt = PerceptronOptimizer(p)\n\n    loss = 1.0\n\n    # for keeping track of loss values\n    loss_vec = []\n\n    n = X.size()[0]\n\n    # set up the figure\n    plt.rcParams[\"figure.figsize\"] = (xdim, ydim)\n    fig, axarr = plt.subplots(rows, cols, sharex = True, sharey = True)\n    markers = [\"o\", \",\"]\n    marker_map = {-1 : 0, 1 : 1}\n\n    # initialize for main loop\n    current_ax = 0\n    loss = 1\n    loss_vec = []\n    iter =  0\n\n    while loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n        ax = axarr.ravel()[current_ax]\n\n        if loss == 0:\n            break\n        \n        # not part of the update: just for tracking our progress    \n        prev_loss = loss\n\n        loss = p.loss(X, y) \n\n        # pick a random data point\n        ix = torch.randperm(X.size(0))[:k]\n        x_i = X[ix,:]\n        y_i = y[ix]\n\n        old_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w)\n        \n        # perform a perceptron update using the random data point\n        opt.step_k(x_i, y_i, alpha, k)\n\n        if (prev_loss != loss):\n            plot_perceptron_data(X, y, ax)\n            draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n            loss = p.loss(X, y).item()\n            loss_vec.append(loss)\n            draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n            # try ix instead of i here\n            ax.scatter(X[ix,0],X[ix,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n            # draw_line(w, -10, 10, ax, color = \"black\")\n            ax.set_title(f\"loss = {loss:.3f}\")\n            ax.set(xlim = (-1, 2), ylim = (-1, 2))\n            current_ax += 1\n            iter += 1\n\nFirst, I graphed the minibatch on a batch size of 1. This was done to test the performance of the batch gradient and step functions, particularly in comparison to the non-batched perceptron functions. As the graph shows, the perceptron finds the separating line between the two groups roughly as quickly as the original algorithm.\n\ngraph_k(X, y, 1, 0.1, 2, 4)\n\n\n\n\n\n\n\n\nSecond, this test shows that a minibatch with a batch size of 10 converges on a two-dimensional, linearly-seperable dataset given enough time. This test notably takes more time to converge than a perceptron with a batch size of 1.\n\ngraph_k(X, y, 10, 0.1, rows=5, cols=5, xdim=16, ydim=14, seed=9000)\n\n\n\n\n\n\n\n\nFinally, to test the minibatch performance on linearly inseparable data, I created a block of code that terminates after 10000 steps, calling opt.step_k() with a batch size of n (the size of the data). After running this code for 10000, the loss is plotted over time.\n\n# adapted from lecture 7 (Perceptron)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X2.size()[0]\n\n# initialize for main loop\nloss = 1\nloss_vec = []\n\nmax_steps = 10000\nstep = 0\n\nk = X2.shape[0]\nalpha = 0.01\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    if (step &gt;= max_steps):\n        break\n\n    ax = axarr.ravel()[current_ax]\n\n    if loss == 0:\n        break\n        \n    # not part of the update: just for tracking our progress    \n    prev_loss = loss\n\n    loss = p.loss(X2, y2) \n\n    # pick a random data point\n    # i = torch.randint(n, size = (1,))\n    ix = torch.randperm(X2.size(0))[:k]\n    x_i = X2[ix,:]\n    y_i = y2[ix]\n\n    old_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w)\n        \n    # perform a perceptron update using the random data point\n    opt.step_k(x_i, y_i, alpha, k)\n\n    loss = p.loss(X2, y2).item()\n    loss_vec.append(loss)\n    step += 1\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Minibatch k = n Perceptron Iterations\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\n\nloss_vec[len(loss_vec) - 1]\n\n0.03999999910593033\n\n\nWhile this test doesn’t converge at 0, it does show that running the minibatch with a batchsize of n converges to a low loss value.\n\nloss = p.loss(X, y) # ====\n# y_ = 2*y - 1 # O(n)\n# (1.0*(self.score(X)*y_ &lt; 0)).mean() ==== \n# 1 * torch.matmul(X, torch.t(self.w)) * y &lt; 0 = O(n)\n\n# pick a random data point\ni = torch.randint(n, size = (1,)) # O(1)\nx_i = X[[i],:] # O(1)\ny_i = y[i] # O(1)\n\nold_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w) # O(1) OR O(p)\n    \n# perform a perceptron update using the random data point\nopt.step(x_i, y_i) # grad could be O(p)\n\n\nloss = p.loss(X, y) # O(np)\n\n# pick a random data point\nix = torch.randperm(X.size(0))[:k] #O(k)\nx_i = X[ix,:] # O(k)\ny_i = y[ix] # O(k)\n        \n# perform a perceptron update using the random data point\nopt.step_k(x_i, y_i, alpha, k)\n\n# self.model.loss(X, y) # for an X of dim k * p and a y of size k,\n# this has a runtime of O(kp)\n# self.model.w += self.model.grad_k(X, y, alpha, k)\n\n# s_i = torch.inner(torch.t(self.w), X) # p * (k*p) = O(kp) results in size k*p\n# return torch.sum(((1 * (s_i*y &lt; 0))[:, None]*y[:, None]*X), axis=0)\n# k*p*k * k * k * p\n\nloss = p.loss(X, y).item() # O(np)"
  },
  {
    "objectID": "posts/perceptron-post/index.html#runtime-discussion",
    "href": "posts/perceptron-post/index.html#runtime-discussion",
    "title": "Perceptron",
    "section": "",
    "text": "For the runtime of the perceptron, a single iteration without minibatching will have a runtime of O(n) if n &gt; p and a runtime of O(p) if p &gt; n. The O(n) appears in the loss calculation’s matrix multiplication and the O(p) appears in the gradient calculation. In the minibatch implementation, the runtime is O(kp) (seen in the grad_k calculation step) if kp &gt; n and O(n) (seen in the matrix multiplication) if n &gt; kp."
  },
  {
    "objectID": "posts/perceptron-post/index.html#conclusion",
    "href": "posts/perceptron-post/index.html#conclusion",
    "title": "Perceptron",
    "section": "",
    "text": "In the creation and testing of the perceptron algorithm, I found that – while it seems to work well for linearly separable data – it can be a bit awkward to run on linearly inseparable data. On two dimensional data, the perceptron converged to zero quickly for linearly separable data. While it isn’t possible for this to occur on linearly inseparable data, it did progressively improve over time before plateauing before zero. This ability to achieve zero loss with linearly separable data holds at higher dimensions, as well. Similarly, I found that a minibatch perceptron could converge to zero on linearly separable data (though at slower rates than non-batched perceptrons at higher batch sizes) and to a low non-zero value on inseparable data with a large batch size."
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html",
    "href": "posts/optimal-decision-making/optimal.html",
    "title": "\"Optimal\" Decision Making",
    "section": "",
    "text": "This blog post will explore the data behind loan granting from various perspectives. The data will first be shown “as is” through visualizations and data tables. Then, a model will be trained on the data through the creation of a scoring function and threshold. Finally, more in-depth quantiative analysis will be done on the profits, costs, and biases that are created from the model. Through this study, a profitable model for the bank was found, but this came at the cost of differing acceptance rates across groups."
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#introduction",
    "href": "posts/optimal-decision-making/optimal.html#introduction",
    "title": "\"Optimal\" Decision Making",
    "section": "",
    "text": "This blog post will explore the data behind loan granting from various perspectives. The data will first be shown “as is” through visualizations and data tables. Then, a model will be trained on the data through the creation of a scoring function and threshold. Finally, more in-depth quantiative analysis will be done on the profits, costs, and biases that are created from the model. Through this study, a profitable model for the bank was found, but this came at the cost of differing acceptance rates across groups."
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#part-b-data-exploration",
    "href": "posts/optimal-decision-making/optimal.html#part-b-data-exploration",
    "title": "\"Optimal\" Decision Making",
    "section": "Part B: Data Exploration",
    "text": "Part B: Data Exploration\nFirst, the data is converted to a dataframe through pandas and checked using the head() function\n\nimport pandas as pd\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nimport warnings\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\nI first investigated the relationship between personal income, financial history, and loan amounts. I did this by creating a table that groups by income intervals of 10000 and checking credit history and the size of the loan. It shows that loan amounts granted increase as personal income increases, while one’s credit history tends to be higher as income increases, but this isn’t as drastic of an upward trend.\n\ndf_train[\"income_group\"] = df_train[\"person_income\"] // 10000\n\ndf_train.groupby([\"income_group\"])[[\"loan_amnt\", \"cb_person_cred_hist_length\"]].aggregate(\"mean\")[:20]\n\n\n\n\n\n\n\n\n\nloan_amnt\ncb_person_cred_hist_length\n\n\nincome_group\n\n\n\n\n\n\n0\n2093.333333\n5.566667\n\n\n1\n3837.235367\n5.282690\n\n\n2\n5731.833494\n5.532243\n\n\n3\n7298.140863\n5.541371\n\n\n4\n8220.600000\n5.630667\n\n\n5\n9237.460703\n5.595313\n\n\n6\n10044.702093\n5.689211\n\n\n7\n11152.793002\n5.836003\n\n\n8\n11724.719801\n5.957659\n\n\n9\n12059.118852\n6.175410\n\n\n10\n13020.365006\n6.000000\n\n\n11\n12653.756477\n6.257340\n\n\n12\n13438.675214\n6.220513\n\n\n13\n14132.446809\n5.984802\n\n\n14\n14319.277108\n6.799197\n\n\n15\n15255.932203\n6.673729\n\n\n16\n16399.778761\n7.106195\n\n\n17\n15436.965812\n7.427350\n\n\n18\n14102.445652\n6.923913\n\n\n19\n15325.520833\n6.666667\n\n\n\n\n\n\n\n\n\nData Visualizations\nI then mapped aspects about the loan with respect to whether or not the grantee has previously defaulted on their loan. I mapped this as a seaborn scatterplot using loan interest rate and percent income. This revealed that those with a prior default on history are almost never granted an interest rate lower than 12%. Further, it showed that there isn’t much of a relationship between loan as a percentage of income and previous defaults being on file.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ng1 = sns.scatterplot(data=df_train[:1000], x=\"loan_int_rate\", y=\"loan_percent_income\", hue=\"cb_person_default_on_file\")\ng1.set_title(\"Relation Between Loan Interest Rate, Percentage of Income, and Previous Defaults\")\ng1.set_xlabel(\"Loan Interest Rate\")\ng1.set_ylabel(\"Loan as Percentage of Income\")\nplt.legend(title=\"Previous Default on File\")\n                     \n\n\n\n\n\n\n\n\nFinally, I looked into the impact one’s age and home ownership status has on their loan status (whether or not they default) using a seaborn lineplot. It shows that defaulting on loans for those who rent stay fairly stable (and high) from around 20-70 years old. The default rate is much lower for those with mortgages than those who pay rent, and it tends to go down with age. While the rate of default for mortgages jumps up at 60, there are only 3 datapoints for this, so it may be an outlier. Home ownership default rates are the lowest, also trending downward over time (reaching a 0% default rate at 60). Finally, the “other” category fluctuates wildly, increasing drastically between those in their 20s and 30s, then decreasing even more between those in their 30s and 40s. No data exists on “other” home ownership exists in this data for those above their 40s.\n\ndf_train[\"age_group\"] = df_train[\"person_age\"] // 10\n\n# axis limiting information from: https://stackoverflow.com/questions/54822884/how-to-change-the-x-axis-range-in-seaborn\nfig, ax = plt.subplots()\n\ng2 = sns.barplot(data=df_train, x=\"age_group\", y=\"loan_status\", hue=\"person_home_ownership\", errorbar=None, ax=ax)\n\ng2.set_title(\"Chance of Default by Age Group and Home Ownership\")\ng2.set_xlabel(\"Age Group (Tens of Years)\")\ng2.set_ylabel(\"Loan Status (Average Chance of Default)\")\nplt.legend(title=\"Home Ownership Status\")\n\nax.set_xlim(-1, 6)\nplt.show()"
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#part-c-model-creation",
    "href": "posts/optimal-decision-making/optimal.html#part-c-model-creation",
    "title": "\"Optimal\" Decision Making",
    "section": "Part C: Model Creation",
    "text": "Part C: Model Creation\n\nData Preparation\nI then wanted to create a model to predict whether or not an applicant would default on their loan. To start, I processed the data to drop loan_grade (which was already a prediction) and loan_status (what the model is predicting). Further, I use pd.get_dummies to convert qualitative columns like “person_home_ownership” to become true/false columns for each possible answer of the column (e.g. person_home_ownership_RENT).\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df_train[\"loan_status\"])\n\ndef prepare_data(df):\n  df = df.drop([\"loan_grade\"], axis = 1)\n  df = df.dropna()\n  y = le.transform(df[\"loan_status\"])\n  df = df.drop([\"loan_status\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(df_train)\n\n\nX_train.head()\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nincome_group\nage_group\nperson_home_ownership_MORTGAGE\n...\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\n9\n2\nFalse\n...\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\n3\n2\nFalse\n...\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\n2\n2\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\n5\n2\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\n2\n2\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n5 rows × 21 columns\n\n\n\n\n\n\nCross Validation and Parameter Choice\nI then used a logistic regression (LR) model from SciKitLearn to figure out the most important variables for the model to consider and the weights of their importance. I did this by procedurally iterating through one qualitative column and different pairs of quantitative column and scoring the LR model through cross-validation. The best scoring columns were saved and output for future use.\n\nwarnings.filterwarnings(\"ignore\") # just ignores the convergence warning in this case\n\nresults_LR = []\n\nall_qual_cols = [\"person_home_ownership\", \"loan_intent\", \"cb_person_default_on_file\"]\nall_quant_cols = [\"person_age\", \"person_income\", \"person_emp_length\", \"loan_amnt\",\n                  \"loan_int_rate\", \"loan_percent_income\", \"cb_person_cred_hist_length\"]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\n    LR = LogisticRegression()\n\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    new_score_LR = cv_scores_LR.mean()\n    results_LR.append((new_score_LR, cols))\n\nresults_LR.sort(reverse=True, key=lambda x : x[0])\nprint(results_LR)\n\n[(0.8488229950993185, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'loan_percent_income']), (0.8486483607400082, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_percent_income', 'cb_person_cred_hist_length']), (0.8484738026069572, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_emp_length', 'loan_percent_income']), (0.8455928025641752, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_int_rate', 'loan_percent_income']), (0.8316669701424602, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_int_rate', 'loan_percent_income']), (0.8313616744469856, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'loan_amnt']), (0.8313612266177142, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_amnt', 'loan_percent_income']), (0.82359095998493, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_int_rate', 'loan_percent_income']), (0.8208840988307748, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_percent_income', 'cb_person_cred_hist_length']), (0.8208404497692298, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'loan_percent_income']), (0.8207968197642493, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_emp_length', 'loan_percent_income']), (0.8199676876888209, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'loan_percent_income']), (0.818526754130582, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'loan_percent_income']), (0.8179595259527067, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'loan_int_rate']), (0.8172610552134426, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_emp_length', 'loan_int_rate']), (0.816475362577347, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_emp_length', 'loan_int_rate']), (0.8160825305517229, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'loan_int_rate']), (0.8156022479504903, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'loan_int_rate']), (0.8154275850063331, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_int_rate', 'cb_person_cred_hist_length']), (0.8154274992517916, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_int_rate', 'cb_person_cred_hist_length']), (0.8153399724499243, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_amnt', 'loan_percent_income']), (0.8152528363076345, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'loan_int_rate']), (0.8151218795947164, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_emp_length', 'loan_percent_income']), (0.8146855128469355, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_amnt', 'loan_int_rate']), (0.8135071596942984, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_amnt', 'loan_percent_income']), (0.8134193089308305, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'loan_percent_income']), (0.8133756979824149, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_percent_income', 'cb_person_cred_hist_length']), (0.812721190738014, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_amnt', 'loan_int_rate']), (0.8117169193043094, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'loan_int_rate']), (0.8080935707819414, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'loan_amnt']), (0.8035536110236892, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_emp_length', 'loan_int_rate']), (0.8004103450086235, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'loan_amnt']), (0.7985767604621254, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'loan_int_rate']), (0.7984894528107527, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_int_rate', 'cb_person_cred_hist_length']), (0.7968743327224751, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_amnt', 'loan_int_rate']), (0.7925530661012202, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'loan_percent_income']), (0.7907191194799921, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'cb_person_cred_hist_length']), (0.7904572060541563, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'person_income']), (0.7898459476834412, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_emp_length', 'loan_amnt']), (0.7896713704938252, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'person_emp_length']), (0.7894529727335813, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'loan_amnt']), (0.7890163963635881, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_amnt', 'cb_person_cred_hist_length']), (0.7878378621735861, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'person_emp_length']), (0.7868337622489643, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'person_income']), (0.7866591660027835, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'cb_person_cred_hist_length']), (0.7860043347967822, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_emp_length', 'loan_amnt']), (0.7857860609042093, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'person_income']), (0.7853930859543494, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_amnt', 'cb_person_cred_hist_length']), (0.7853930668977847, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'loan_amnt']), (0.7850001586459016, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_emp_length', 'loan_amnt']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'person_emp_length']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'cb_person_cred_hist_length']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'person_emp_length']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'cb_person_cred_hist_length']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_emp_length', 'cb_person_cred_hist_length']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'person_emp_length']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'cb_person_cred_hist_length']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_emp_length', 'cb_person_cred_hist_length']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'person_emp_length']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'cb_person_cred_hist_length']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_emp_length', 'cb_person_cred_hist_length']), (0.7847382738049129, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_amnt', 'cb_person_cred_hist_length']), (0.7846946247433676, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'loan_amnt'])]\n\n\nWith the columns defined, we can get the weights of each column using LogisticRegression.coef_.\n\ncols = ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN',\n         'person_home_ownership_RENT', 'loan_percent_income', 'cb_person_cred_hist_length']\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\n\nLR.coef_\n\narray([[-7.67155908e-01, -1.12954673e-01, -1.80828780e+00,\n         2.68210537e-01,  8.27833450e+00, -4.48028544e-03]])\n\n\nA simple linear score function is defined here as the inner product between the values of the relevant columns (X) and the values of w (our weights, defined above). X_train is then given profit, cost, and score columns based on the profit and cost functions provided.\n\ndef linear_score(w, X):\n    return X@w\n\n\nX_train[\"profit\"] = X_train[\"loan_amnt\"]*(1 + 0.25*0.01*X_train[\"loan_int_rate\"])**10 - X_train[\"loan_amnt\"]\n\nX_train[\"cost\"] = X_train[\"loan_amnt\"]*(1 + 0.25*0.01*X_train[\"loan_int_rate\"])**3 - 1.7*X_train[\"loan_amnt\"]\n\nX_train[\"score\"] = linear_score(np.transpose(LR.coef_), X_train[cols])\n\nThe model is then optimized for maximum profit for the bank. This algorithm checks for each threshold value between the minimum and maximum scores in the training data. The profit for each threshold is calculated as 0 if the model predicts a default (the loan being denied), the value of the profit column if the value of loan_status is 0 (no default), and the value of the cost column is the value of loan_status is 1.\n\nX_train[\"score\"].min(), X_train[\"score\"].max()\n\n(-1.7747875900109253, 6.633567528408647)"
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#part-d-threshold-choice",
    "href": "posts/optimal-decision-making/optimal.html#part-d-threshold-choice",
    "title": "\"Optimal\" Decision Making",
    "section": "Part D: Threshold Choice",
    "text": "Part D: Threshold Choice\n\nmax_profit = float('-inf')\nthresh = -1.17\n\nfor t in np.arange(-1.17, 7.23, 0.01, dtype=float):\n    temp_prof = ((X_train[\"score\"] &lt; t) * ((y_train == 0) * X_train[\"profit\"] + (y_train == 1) * X_train[\"cost\"])).mean()\n    if (temp_prof &gt; max_profit):\n        max_profit = temp_prof\n        thresh = t\n\nmax_profit, thresh\n\n(1447.7771963146645, 2.7500000000000036)\n\n\n\nsns.scatterplot(data=X_train, x=\"profit\", y=\"cost\")"
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#part-e-evaluating-profit",
    "href": "posts/optimal-decision-making/optimal.html#part-e-evaluating-profit",
    "title": "\"Optimal\" Decision Making",
    "section": "Part E: Evaluating Profit",
    "text": "Part E: Evaluating Profit\nWith a max profit of $1448 and a threshold of 3.35, I apply the scoring function and threshold to the testing data. This threshold and set of weights achieved a testing profit of $1392.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\nX_test, y_test = prepare_data(df_test)\n\n\nthresh = 3.35\n\nX_test[\"profit\"] = X_test[\"loan_amnt\"]*(1 + 0.25*0.01*X_test[\"loan_int_rate\"])**10 - X_test[\"loan_amnt\"]\nX_test[\"cost\"] = X_test[\"loan_amnt\"]*(1 + 0.25*0.01*X_test[\"loan_int_rate\"])**3 - 1.7*X_test[\"loan_amnt\"]\n\nX_test[\"score\"] = linear_score(np.transpose(LR.coef_), X_test[cols])\n\n# if the score is under the threshold, the person is predicted not to default, so the loan is given\n((X_test[\"score\"] &lt; thresh) * ((y_test == 0) * X_test[\"profit\"] + (y_test == 1) * X_test[\"cost\"])).mean()\n\n1057.8815263645538\n\n\nI also added a “loan_granted” column, which just negates the prediction of the model (i.e. if no default is predicted, the loan is granted). This makes some of the logic easier by preventing negations of the prediction column later on.\n\n# prediction is whether or not a default will occur\nX_test[\"pred\"] = X_test[\"score\"] &gt; thresh\nX_test[\"loan_granted\"] = ~X_test[\"pred\"]\nX_test\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\nprofit\ncost\nscore\npred\nloan_granted\n\n\n\n\n0\n21\n42000\n5.0\n1000\n15.58\n0.02\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nTrue\nFalse\n465.367227\n-578.539601\n0.415856\nFalse\nTrue\n\n\n1\n32\n51000\n2.0\n15000\n11.36\n0.29\n9\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\n4847.780062\n-9185.361205\n1.593239\nFalse\nTrue\n\n\n2\n35\n54084\n2.0\n3000\n12.61\n0.06\n6\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\n1091.841800\n-1807.236578\n0.738029\nFalse\nTrue\n\n\n3\n28\n66300\n11.0\n12000\n14.11\n0.15\n6\nTrue\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nTrue\nFalse\n4972.214553\n-7084.777554\n0.447713\nFalse\nTrue\n\n\n4\n22\n70550\n0.0\n7000\n15.88\n0.08\n3\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nTrue\nFalse\n3331.859215\n-4032.764115\n0.917036\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6511\n29\n78000\n2.0\n18000\n6.62\n0.23\n5\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nTrue\nFalse\n3210.941787\n-11691.427669\n1.11446\nFalse\nTrue\n\n\n6513\n27\n44640\n0.0\n12800\n11.83\n0.29\n9\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nTrue\nFalse\n4331.281644\n-7790.401145\n2.628605\nFalse\nTrue\n\n\n6514\n24\n48000\n5.0\n10400\n7.37\n0.22\n3\nFalse\nFalse\nTrue\n...\nFalse\nFalse\nTrue\nTrue\nFalse\n2083.140437\n-6694.483153\n-0.000495\nFalse\nTrue\n\n\n6515\n26\n65000\n6.0\n6000\n9.07\n0.09\n3\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\n1508.058449\n-3782.525248\n-0.035547\nFalse\nTrue\n\n\n6516\n29\n61000\n12.0\n10000\n16.07\n0.16\n9\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nTrue\nFalse\n4827.369675\n-5745.680644\n1.552421\nFalse\nTrue\n\n\n\n\n5731 rows × 24 columns"
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#part-f-evaluating-from-a-borrowers-perspective",
    "href": "posts/optimal-decision-making/optimal.html#part-f-evaluating-from-a-borrowers-perspective",
    "title": "\"Optimal\" Decision Making",
    "section": "Part F: Evaluating from a Borrower’s Perspective",
    "text": "Part F: Evaluating from a Borrower’s Perspective\nWith a complete model, it’s important to do analysis on its impact. First, we can see through the creation of an age group column that acceptance rates tend to rise from ages 20 to 44. After this, we see a dip at 45-54 year olds before the rate rises again. Another large outlier is those in group 13 (the 65-69 year old age range), with an acceptance rate of 60%.\n\n# F1\nX_test[\"age_group\"] = X_test[\"person_age\"] // 5\nX_test.groupby(\"age_group\")[\"loan_granted\"].mean()\n\nage_group\n4     0.954733\n5     0.966223\n6     0.967136\n7     0.981043\n8     0.973822\n9     0.971014\n10    0.937500\n11    1.000000\n12    1.000000\n13    0.600000\n14    1.000000\nName: loan_granted, dtype: float64\n\n\nOne reason for these large flucuations and 100% acceptance rates, however, could be the low count of applicants in the higher age groups. Although group 13 has the lowest acceptance rate, they also have the lowest sample size with just 5 people. This rise is still notable, especially in groups with larger sample sizes like those in the 20-24 range compared to those in the 40-44 range.\n\nX_test.groupby(\"age_group\")[\"age_group\"].count()\n\nage_group\n4     2187\n5     1954\n6      852\n7      422\n8      191\n9       69\n10      32\n11      10\n12       7\n13       5\n14       2\nName: age_group, dtype: int64\n\n\nThe next test compares the model’s acceptance of those based on loan intent, comparing to its average acceptance rate. Here, we can see that loans for debt consolidaiton, medical expenses, and personal expenses are denied more often than average (where education, home improvement, and venture expenses are approved more often). Here, it is also notable that the model approves loans (i.e. sees no default) much more often than the data dictates that no default will occur.\n\n# F2\nintent_cols = [col for col in X_train.columns if \"loan_intent\" in col ]\n\nmodel_average = X_test[\"loan_granted\"].mean()\nprint(f'Model average acceptance: {round(model_average, 6)}\\n')\n\nsum = 0\nfor intent in intent_cols:\n    intent_count = X_test[intent].sum()\n    model_score = (X_test[intent] & X_test[\"loan_granted\"]).sum()/intent_count\n    result = (X_test[intent] & ~y_test).sum()/intent_count\n\n    sum += intent_count\n    print(f'{intent[12:]}:')\n    print(f'Model grant amount: {round(model_score, 6)}')\n    print(f'Actual grant amount: {round(result, 6)}')\n    print(f'Model\\'s group grant to averge grant ratio: {round(model_score/model_average, 6)}')\n    print(\"\")\n\nModel average acceptance: 0.963008\n\nDEBTCONSOLIDATION:\nModel grant amount: 0.961283\nActual grant amount: 0.712389\nModel's group grant to averge grant ratio: 0.998209\n\nEDUCATION:\nModel grant amount: 0.966837\nActual grant amount: 0.832483\nModel's group grant to averge grant ratio: 1.003976\n\nHOMEIMPROVEMENT:\nModel grant amount: 0.983766\nActual grant amount: 0.75\nModel's group grant to averge grant ratio: 1.021555\n\nMEDICAL:\nModel grant amount: 0.951538\nActual grant amount: 0.71575\nModel's group grant to averge grant ratio: 0.988089\n\nPERSONAL:\nModel grant amount: 0.957916\nActual grant amount: 0.779559\nModel's group grant to averge grant ratio: 0.994712\n\nVENTURE:\nModel grant amount: 0.96473\nActual grant amount: 0.853734\nModel's group grant to averge grant ratio: 1.001788\n\n\n\nFinally, I checked how the model grants loans to those of different income groups. I did this by (again) dividing the incomes into groups of 10000. When sorting by these groups, it becomes clear that the likelihood of being granted a loan by this model rises drastically as one’s income does. Above an income of 110000, this likelihood is 100% (the amount of income groups included is truncated for readability).\n\n# F3\nX_test[\"income_group\"] = X_test[\"person_income\"] // 10000\nX_test.groupby(\"income_group\")[\"loan_granted\"].mean()[:20]\n\nincome_group\n0     0.727273\n1     0.914729\n2     0.906926\n3     0.908788\n4     0.955900\n5     0.982804\n6     0.983607\n7     0.986328\n8     0.985251\n9     1.000000\n10    1.000000\n11    1.000000\n12    1.000000\n13    1.000000\n14    1.000000\n15    1.000000\n16    1.000000\n17    1.000000\n18    1.000000\n19    1.000000\nName: loan_granted, dtype: float64"
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#medical-credit",
    "href": "posts/optimal-decision-making/optimal.html#medical-credit",
    "title": "\"Optimal\" Decision Making",
    "section": "Medical Credit",
    "text": "Medical Credit\nI would say that it is not fair for those seeking loans for medical expenses to obtain access to credit. Here, I’m defining fairness in a line with a middle view of equality of opportunity; I tend to view these problems from the broad sense, but since we’re the taking the view of the decision maker, a broad approach is less applicabale. Specifically, we should act “to avoid perpetuating injustice” (from “Relative Notions of Fairness” in Fairness and Machine Learning).\nThe middle view here would counteract the unfairness of the medical (particularly healthcare/insurance) system, and the ability of many to access it. The decision maker (the bank) should then act in spite of the situation that left the applicants medically vulnerable and in support of their health and safety."
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#conclusion",
    "href": "posts/optimal-decision-making/optimal.html#conclusion",
    "title": "\"Optimal\" Decision Making",
    "section": "Conclusion",
    "text": "Conclusion\nThrough the creation of this model, I found that the creation of a model that maximizes profit will often deprioritize the health and safety of those affected by it. This was seen in the investigation of different groups the model decides on. Those with medical expenses and less income, for instance, are disproportionately denied by this model. Maximizing for profit, then, shouldn’t be seen as the total goal for decision makers. Through this investigation, I also learned that those with previous credit issues are saddled with higher interest rates, increasing the risk of default occuring again. Beyond domain specific findings, I gained practice in thresholding, weighting, and building decision-making models."
  },
  {
    "objectID": "posts/Logistic/index.html",
    "href": "posts/Logistic/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/Logistic/index.html#abstract",
    "href": "posts/Logistic/index.html#abstract",
    "title": "Logistic Regression",
    "section": "Abstract",
    "text": "Abstract\nThis blog post explores the logistic regression model. This first involved the creation of the model, which is documented above. Then, I use the logistic regression model in a variety of experiments, including those with differing momentum and dimensionality values. Specifically, these experiments are a default test (no momentum, 2-dimensions), a test with momentum (keeping other values equal), and a test with significantly more dimensions than data points (p = 100, n = 50). By doing so, we can explore how the logistic regression model can be successfully used and how it reacts to different parameters."
  },
  {
    "objectID": "posts/Logistic/index.html#data-creation",
    "href": "posts/Logistic/index.html#data-creation",
    "title": "Logistic Regression",
    "section": "Data Creation",
    "text": "Data Creation\nFirst, I import my model alongside torch, numpy, and matplotlib.\n\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nThen, using the classification_data method provided in the assignment, I generate linearly inseparable data to test the logstic regression model on.\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)"
  },
  {
    "objectID": "posts/Logistic/index.html#visualization-methods",
    "href": "posts/Logistic/index.html#visualization-methods",
    "title": "Logistic Regression",
    "section": "Visualization Methods",
    "text": "Visualization Methods\nNext, I bring in methods from the perceptron assignment to plot the data and draw lines for decision boundaries. This will be important for the first experiment to show the model’s weight vector changing over time.\n\n# from perceptron lecture\n\ndef plot_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nThese next two methods, also taken from the perceptron assignment, are adapted to better suit these experiments. The plot_decisions function only shows the first and last five decision boundary changes (instead of all changes like in the perceptron). Both were also changed to take in their necessary vectors in order to reduce the amount of times they’re repeated in the code.\n\n# plots the first and last 5 iterations of the decision boundary\ndef plot_decisions(X, y, weight_vec, loss_vec):\n        # set up the figure\n        current_ax = 0\n        plt.rcParams[\"figure.figsize\"] = (10, 8)\n        fig, axarr = plt.subplots(2, 5, sharex = True, sharey = True)\n\n        # iter through weight_vec\n\n        for i in range(5):\n                ax = axarr.ravel()[current_ax]\n                plot_data(X, y, ax)\n                draw_line(weight_vec[i], x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n                draw_line(weight_vec[i+1], x_min = -1, x_max = 2, ax = ax, color = \"black\")\n                ax.set_title(f\"loss = {loss_vec[i]:.3f}\")\n                ax.set(xlim = (-1, 2), ylim = (-1, 2))\n                current_ax += 1\n        \n        for i in range(len(weight_vec) - 6, len(weight_vec) - 1):\n                ax = axarr.ravel()[current_ax]\n                plot_data(X, y, ax)\n                draw_line(weight_vec[i], x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n                draw_line(weight_vec[i+1], x_min = -1, x_max = 2, ax = ax, color = \"black\")\n                ax.set_title(f\"loss = {loss_vec[i]:.3f}\")\n                ax.set(xlim = (-1, 2), ylim = (-1, 2))\n                current_ax += 1\n        fig.suptitle(\"LR Decision Boundaries over Time\")\n\n\ndef plot_loss(loss_vec):\n    plt.plot(loss_vec, color = \"slategrey\")\n    plt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\n    labs = plt.gca().set(xlabel = \"LR Iteration (Updates Only)\", ylabel = \"loss\")\n    plt.title(\"LR Loss Iterations over Time\")"
  },
  {
    "objectID": "posts/Logistic/index.html#training-method",
    "href": "posts/Logistic/index.html#training-method",
    "title": "Logistic Regression",
    "section": "Training Method",
    "text": "Training Method\nThis training method takes in the data X, the target values y, the learning rate alpha, and the momentum value beta (alongside an optional num_steps parameter). It then runs the optimizer on the model for num_steps amount of time, updating the model’s weight vector each step. Further, there is included code to keep track of the weight and loss changes. The model, its losses, and its weights are returned for later use and visualization.\n\ndef train_LR(X, y, alpha, beta, num_steps=100): \n    LR = LogisticRegression() \n    opt = GradientDescentOptimizer(LR)\n\n    # initialize for main loop\n    loss = 1\n    loss_vec = []\n    weight_vec = [torch.rand(X.size()[1])]\n\n    for _ in range(num_steps):\n        prev_loss = loss\n        loss = LR.loss(X, y)\n\n        opt.step(X, y, alpha, beta)\n\n        if (prev_loss != loss):\n            loss_vec.append(loss)\n            weight_vec.append(LR.w)\n            \n    return LR, (loss_vec, weight_vec)"
  },
  {
    "objectID": "posts/Logistic/index.html#part-b-experiments",
    "href": "posts/Logistic/index.html#part-b-experiments",
    "title": "Logistic Regression",
    "section": "Part B: Experiments",
    "text": "Part B: Experiments\nFor the first experiment, the training method is run with two dimensional data, a learning rate of 0.1, and no momentum. To achieve a low loss value, the training loop is run for 1000 steps.\n\nLR, (loss_vec, weight_vec) = train_LR(X, y, 0.1, 0.0, num_steps=1000)\n\nPlotting the decision boundaries, we can see a significant increase in accuracy over time with a decision boundary that more cleanly separates the data.\n\nplot_decisions(X, y, weight_vec, loss_vec)\n\n\n\n\n\n\n\n\nIf we plot the loss over time, we similarly see its reduction. This plot makes it clear how the loss decreases in a logistic manner, starting to plateau near the minimum loss value (since the data is linearly inseparable, the minimum is not 0).\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nFor the next experiment, I used the same data and learning rate, but changed the momentum from 0 to 0.9. Here, we can see that the loss reaches a low value much faster than the model without momentum (in 100 steps rather than 1000).\n\nLR, (loss_vec, weight_vec) = train_LR(X, y, 0.1, 0.9, num_steps=100)\n\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nFinally, this experiment is made to show the effect on high-dimensional data. Since the experiment intends to test accuracy, I first made a function that determines a model’s accuracy (rather than loss) by comparing the model’s prediction results to the y target data. Then, I made a separate training loop that stops once the accuracy on the training data is 100%.\n\ndef accuracy(model, X, y):\n    return (1.0 * (model.predict(X) == y)).mean()\n\ndef train_LR_acc(X, y, alpha, beta): \n    LR = LogisticRegression() \n    opt = GradientDescentOptimizer(LR)\n\n    # initialize for main loop\n    acc_vec = []\n\n    while (True):\n        opt.step(X, y, alpha, beta)\n\n        acc = accuracy(LR, X, y)\n        acc_vec.append(acc)\n\n        if (accuracy(LR, X, y) == 1.):\n            break\n            \n    return LR, acc_vec\n\nSince this test was for overfitting on data where the number of points is half the amount of dimensions (p == 2n), I then created new data (training and testing), both with 100 dimensions and 50 points of data.\n\nX_train, y_train = classification_data(p_dims = 100, n_points = 50)\nX_test, y_test = classification_data(p_dims = 100, n_points = 50)\n\nNext, I trained the model on the training data using the new accuracy-halting training function.\n\nLR, acc_vec = train_LR_acc(X_train, y_train, 0.1, 0.8)\n\nWe can then check the accuracy of the model on the training and testing data by calling the accuracy function. Of course, since the function ensured that training accuracy is equal to 100 before returning, the first function call returns 1.\n\naccuracy(LR, X_train, y_train)\n\ntensor(1.)\n\n\nHere, we can see that a small amount of overfitting has occurred, the model is not 100% accurate on both the training and testing data:\n\naccuracy(LR, X_test, y_test)\n\ntensor(0.9400)\n\n\nI also adapted the plot_loss function to plot accuracy over time:\n\ndef plot_acc(acc_vec):\n    plt.plot(acc_vec, color = \"slategrey\")\n    plt.scatter(torch.arange(len(acc_vec)), acc_vec, color = \"slategrey\")\n    labs = plt.gca().set(xlabel = \"LR Iteration (Updates Only)\", ylabel = \"accuracy\")\n    plt.title(\"LR Accuracy Iterations over Time\")\n\nFor reference, we can see how the accuracy increases over time, noting that it does not take many iterations for the model to achieve 100% accuracy on this low-size training data.\n\nplot_acc(acc_vec)\n\n\n\n\n\n\n\n\nAs we increase the amount of noise in the data (the default being 0.2), the level of overfitting to the training data increases. As we can see here, when the model achieves 100% accuracy on the training data, it only yields 62% accuracy on the testing data.\n\nX_train, y_train = classification_data(p_dims = 100, n_points = 50, noise=5)\nX_test, y_test = classification_data(p_dims = 100, n_points = 50, noise=5)\nLR, acc_vec = train_LR_acc(X_train, y_train, 0.1, 0.8)\naccuracy(LR, X_train, y_train)\n\ntensor(1.)\n\n\n\naccuracy(LR, X_test, y_test)\n\ntensor(0.6200)"
  },
  {
    "objectID": "posts/Logistic/index.html#discussion",
    "href": "posts/Logistic/index.html#discussion",
    "title": "Logistic Regression",
    "section": "Discussion:",
    "text": "Discussion:\nOver the course of this blog post, I built a custom logistic regression model and performed experiments to figure out how it performs under different parameters. In doing so, I found that adding momentum drastically increases the speed of the model. For instance, a model with no momentum achieved a similar loss with 1000 training steps compared to a model using momentum with 100 steps. I also tested the model’s behavior on data where the number of dimensions is double the number of data points. In doing so, I found that the amount of overfitting is often dependent on the amount of noise in the data. While there is some overfitting in less noisy data, the amount of overfitting (and resulting lack of accuracy on the testing data) drastically increases when noise does. In the process of making this model, I learned more about gradient functions and step functions and got to practice the full pipeline of creating, training, and testing models in Python."
  },
  {
    "objectID": "posts/bias-replication/bias_replication.html",
    "href": "posts/bias-replication/bias_replication.html",
    "title": "Bias Replication Study",
    "section": "",
    "text": "This blog post aims to replicate the study by Obermeyer et al. on the racial bias that exists in healthcare systems. In doing so, I compare the risk percentile assigned to black patients and white patients alongside their number of chronic illnesses. This recreation found that black patients were given lower risk scores even if they had the same number of chronic illnesses as white patients. I also compared medical expenditure with percentile risk score and number of chronic illnesses separately. This showed that, while costs were around the same by race when looking at different risk percentages, there was less expenditures for black patients when compared to white patients on the axis of chronic illnesses. This is further proven through the modelling done on the logarithmic cost, which estimates an expenditure for black patients at 75% of white patients. If care is supplied based on cost, then, we can see how unfair outcomes by race can arise.\nFirst, I imported the dataset into a dataframe with pandas.\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\nLet’s look at the data’s columns.\n\ndf.head()\n\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\nThen, I created a risk percentile out of the data’s risk score by using pandas’ qcut feature. I grouped the data by this percentile and race, then looked observed the mean number of chronic illnesses per group.\n\nimport numpy as np\n\ndf[\"risk_percentile\"] = pd.qcut(df[\"risk_score_t\"], 101, labels=False)\n\ndf[\"risk_percentile\"]\n\ngb = df.groupby([\"risk_percentile\", \"race\"])[\"gagne_sum_t\"].mean().unstack(level=1)\ngb\n\n\n\n\n\n\n\n\nrace\nblack\nwhite\n\n\nrisk_percentile\n\n\n\n\n\n\n0\n0.069444\n0.051724\n\n\n1\n0.240741\n0.149847\n\n\n2\n0.277778\n0.105495\n\n\n3\n0.266667\n0.073810\n\n\n4\n0.200000\n0.097514\n\n\n...\n...\n...\n\n\n96\n5.460526\n3.809877\n\n\n97\n4.882353\n4.137681\n\n\n98\n5.761194\n4.821853\n\n\n99\n6.135802\n4.891688\n\n\n100\n7.319149\n6.038012\n\n\n\n\n101 rows × 2 columns\n\n\n\n\nNext, I graphed the risk percentile and mean number of chronic conditions grouped by race, recreating figure 1 of the study.\n\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nplt.style.use('seaborn-v0_8-whitegrid')\n\nmarkers = [\"o\" , \",\"]\n\ncolumns = [\"black\", \"white\"]\n\nfig, ax = plt.subplots(1, 1)\n\nfor i in range(2):\n    to_plot = gb[columns[i]]\n    ax.scatter(to_plot.values, np.arange(0, 101, 1), c = f\"{['blue', 'orange'][i]}\", vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['black', 'white'][i]}\", cmap = \"BrBG\", marker = markers[i])\n    ax.legend()\n    ax.set(xlabel=\"Mean Number of Chronic Conditions\", ylabel = \"Percentile Risk Score\")\n\n\n\n\n\n\n\n\nThis graph shows that, if two patients have the same number of chronic illnesses but one is white and one is black, the white patient is much more likely to have a higher risk score. This, in turn, makes them more likely to be reccomended to the high-risk care management program when compared to a black patient with the same number of chronic conditions.\nAfter this, I used the cost column alongside the number of chronic illnesses and the previously created risk percentile to make two more graphs. The first graph shows the mean cost compared to the risk percentile grouped by race. The second compares the cost and the number of chronic illnesses, also grouped by race.\n\ngb_cost = df.groupby([\"risk_percentile\", \"race\"])[\"cost_t\"].mean().unstack(level=1)\nfig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n\nplt.yscale(\"log\")\n\ngb_chronic = df.groupby([\"gagne_sum_t\", \"race\"])[\"cost_t\"].mean().unstack(level=1)\n\nfor i in range(2):    \n    to_plot = gb_cost[columns[i]]\n    ax1.scatter(np.arange(0, 101, 1), to_plot.values, c = f\"{['blue', 'orange'][i]}\", vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['black', 'white'][i]}\", cmap = \"BrBG\", marker = markers[i])\n    ax1.legend()\n    ax1.set(xlabel = \"Percentile Risk Score\", ylabel=\"Mean Medical Expenditure\")\n\n    to_plot = gb_chronic[columns[i]]\n    ax2.scatter(np.arange(0, 18, 1), to_plot.values, c = f\"{['blue', 'orange'][i]}\", vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['black', 'white'][i]}\", cmap = \"BrBG\", marker = markers[i])\n    ax2.legend()\n    ax2.set(xlabel = \"Number of Chronic Illnesses\")\n\n\n\n\n\n\n\n\nThis graph shows that, while there isn’t a cost disparity across race when comparing againsst percentile risk score, there is one when looking at the number of chronic illnesses. If we look at the datapoints at and below 5 chronic illnesses, where most patients lie, there is a lower average cost per black patients when compared to white patients. Considering the risk assessment model is done according to cost rather than illness, one can see how black patients would be disproportionately denied access to the program.\nFinally, we can model this cost disparity by looking at the data. This modelling experiment is informed on the equation:\n\\(logcost \\approx w_b\\) \\(*\\) (patient is black) \\(+\\) intercept \\(+\\) $_{i=1} ^{k} w_k * \\((gagne sum)\\)^k$\nWhere \\(w_b\\) is the weight a model gives to the variable that a patient is black and \\(w_k\\) are the weights given to the polynomial data of the number of chronic illnesses.\nFrom this we also find that \\(e^{w_b}\\) is the percentage of average cost incurred by black patients compared to white patients\nTo start, we observe that 95% of the patients have 5 or less chronic illnesses, making it a good subset of the data to generalize over.\n\ndf[df[\"gagne_sum_t\"] &lt;= 5][\"gagne_sum_t\"].count()/df.shape[0]\n\n0.9553952115447688\n\n\nNext, we can create a new dataframe eithin that subset that creates a log-transform of the cost (ignoring any 0-cost patients). This allows us to look at data that spans orders of magnitude. Further, for the model to work, we must encode race as a number. In this case, 0 is coded to be white and 1 black.\n\ndf_log = df.query('gagne_sum_t &lt;= 5 & cost_t != 0')\ndf_log[\"log_cost\"] = np.log(df_log[\"cost_t\"])\n\ndf_log[\"race_num\"] = 1 * (df_log[\"race\"] == \"black\")\n\nThen we can take predictor and target columns of this data, where race and number of chronic illness are the predictors (X) and the log cost is the target (y).\n\npred_X = df_log[[\"race_num\", \"gagne_sum_t\"]]\ntarget_y = df_log[\"log_cost\"]\n\n\ndf_log\n\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\nrisk_percentile\nlog_cost\nrace_num\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n35\n7.090077\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n4\n3\n86\n7.863267\n0\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n3\n6.214608\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n11\n7.170120\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n1\n1\n98\n7.003065\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48779\n0.611517\n0\n800.0\n0.0\nNaN\nNaN\nNaN\n1.090000\n148.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n7\n6.684612\n0\n\n\n48780\n2.615933\n0\n2200.0\n0.0\n112.0\nNaN\n41.4\n0.810000\n172.0\nwhite\n...\n0\n1\n0\n0\n1\n1\n1\n46\n7.696213\n0\n\n\n48781\n1.358926\n0\n800.0\n0.0\n105.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n1\n0\n0\n1\n1\n0\n23\n6.684612\n0\n\n\n48782\n10.990318\n0\n1300.0\n0.0\n132.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n3\n3\n94\n7.170120\n0\n\n\n48783\n1.681671\n0\n4400.0\n0.0\n115.0\n5.6\n36.6\n0.940000\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n29\n8.389360\n0\n\n\n\n\n44748 rows × 163 columns\n\n\n\n\nThe function add_polynomial_features returns a dataset with exponential values of the number of chronic illnesses. This is useful for training models that act on non-linear data.\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nPerforming cross-validation on the degrees of the polynomial finds the most accurate to be a degree of 6.\n\nfrom sklearn.linear_model import LinearRegression\n\nrecord = (0, 0)\n\nfor degree in range (1, 15):\n    poly_X = add_polynomial_features(pred_X, degree)\n\n    LinR = LinearRegression()\n    LinR.fit(poly_X, target_y)\n    tempScore = LinR.score(poly_X, target_y)\n    # print(degree, tempScore)\n    if tempScore &gt; record[1]:\n        record = (degree, tempScore)\nrecord\n\n(6, 0.08800788381979874)\n\n\nIf we fit the model to this polynomial dataset and the target variable, we find can find the value of wb by looking at the first coefficient of our linear regression model.\n\npoly_X = add_polynomial_features(pred_X, 6)\nLinR = LinearRegression()\nLinR.fit(poly_X, target_y)\n# LinR.score(poly_X, target_y)\nLinR.coef_[0]\n\n-0.2827181024769897\n\n\nFinally, we can get an percentage of average cost paid by black patients when compared to white patients by calculating \\(e^{w_b}\\).\n\npercent = np.exp(LinR.coef_[0])\npercent\n\n0.7537322331639498\n\n\nThis percentage of around 75% makes sense with regard to the study. Specifically, the study notes that “Blacks generate lower costs than Whites – on average, $1801 less per year…” “… or $1144 less, if we instead hold constant the specific individual illnesses that contribute to the sum.” This roughly lines up with the model’s prediction that black people will have a 75% lower mean total medical expenditure compared to white people."
  },
  {
    "objectID": "posts/bias-replication/bias_replication.html#abstract",
    "href": "posts/bias-replication/bias_replication.html#abstract",
    "title": "Bias Replication Study",
    "section": "",
    "text": "This blog post aims to replicate the study by Obermeyer et al. on the racial bias that exists in healthcare systems. In doing so, I compare the risk percentile assigned to black patients and white patients alongside their number of chronic illnesses. This recreation found that black patients were given lower risk scores even if they had the same number of chronic illnesses as white patients. I also compared medical expenditure with percentile risk score and number of chronic illnesses separately. This showed that, while costs were around the same by race when looking at different risk percentages, there was less expenditures for black patients when compared to white patients on the axis of chronic illnesses. This is further proven through the modelling done on the logarithmic cost, which estimates an expenditure for black patients at 75% of white patients. If care is supplied based on cost, then, we can see how unfair outcomes by race can arise.\nFirst, I imported the dataset into a dataframe with pandas.\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\nLet’s look at the data’s columns.\n\ndf.head()\n\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\nThen, I created a risk percentile out of the data’s risk score by using pandas’ qcut feature. I grouped the data by this percentile and race, then looked observed the mean number of chronic illnesses per group.\n\nimport numpy as np\n\ndf[\"risk_percentile\"] = pd.qcut(df[\"risk_score_t\"], 101, labels=False)\n\ndf[\"risk_percentile\"]\n\ngb = df.groupby([\"risk_percentile\", \"race\"])[\"gagne_sum_t\"].mean().unstack(level=1)\ngb\n\n\n\n\n\n\n\n\nrace\nblack\nwhite\n\n\nrisk_percentile\n\n\n\n\n\n\n0\n0.069444\n0.051724\n\n\n1\n0.240741\n0.149847\n\n\n2\n0.277778\n0.105495\n\n\n3\n0.266667\n0.073810\n\n\n4\n0.200000\n0.097514\n\n\n...\n...\n...\n\n\n96\n5.460526\n3.809877\n\n\n97\n4.882353\n4.137681\n\n\n98\n5.761194\n4.821853\n\n\n99\n6.135802\n4.891688\n\n\n100\n7.319149\n6.038012\n\n\n\n\n101 rows × 2 columns\n\n\n\n\nNext, I graphed the risk percentile and mean number of chronic conditions grouped by race, recreating figure 1 of the study.\n\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nplt.style.use('seaborn-v0_8-whitegrid')\n\nmarkers = [\"o\" , \",\"]\n\ncolumns = [\"black\", \"white\"]\n\nfig, ax = plt.subplots(1, 1)\n\nfor i in range(2):\n    to_plot = gb[columns[i]]\n    ax.scatter(to_plot.values, np.arange(0, 101, 1), c = f\"{['blue', 'orange'][i]}\", vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['black', 'white'][i]}\", cmap = \"BrBG\", marker = markers[i])\n    ax.legend()\n    ax.set(xlabel=\"Mean Number of Chronic Conditions\", ylabel = \"Percentile Risk Score\")\n\n\n\n\n\n\n\n\nThis graph shows that, if two patients have the same number of chronic illnesses but one is white and one is black, the white patient is much more likely to have a higher risk score. This, in turn, makes them more likely to be reccomended to the high-risk care management program when compared to a black patient with the same number of chronic conditions.\nAfter this, I used the cost column alongside the number of chronic illnesses and the previously created risk percentile to make two more graphs. The first graph shows the mean cost compared to the risk percentile grouped by race. The second compares the cost and the number of chronic illnesses, also grouped by race.\n\ngb_cost = df.groupby([\"risk_percentile\", \"race\"])[\"cost_t\"].mean().unstack(level=1)\nfig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n\nplt.yscale(\"log\")\n\ngb_chronic = df.groupby([\"gagne_sum_t\", \"race\"])[\"cost_t\"].mean().unstack(level=1)\n\nfor i in range(2):    \n    to_plot = gb_cost[columns[i]]\n    ax1.scatter(np.arange(0, 101, 1), to_plot.values, c = f\"{['blue', 'orange'][i]}\", vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['black', 'white'][i]}\", cmap = \"BrBG\", marker = markers[i])\n    ax1.legend()\n    ax1.set(xlabel = \"Percentile Risk Score\", ylabel=\"Mean Medical Expenditure\")\n\n    to_plot = gb_chronic[columns[i]]\n    ax2.scatter(np.arange(0, 18, 1), to_plot.values, c = f\"{['blue', 'orange'][i]}\", vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['black', 'white'][i]}\", cmap = \"BrBG\", marker = markers[i])\n    ax2.legend()\n    ax2.set(xlabel = \"Number of Chronic Illnesses\")\n\n\n\n\n\n\n\n\nThis graph shows that, while there isn’t a cost disparity across race when comparing againsst percentile risk score, there is one when looking at the number of chronic illnesses. If we look at the datapoints at and below 5 chronic illnesses, where most patients lie, there is a lower average cost per black patients when compared to white patients. Considering the risk assessment model is done according to cost rather than illness, one can see how black patients would be disproportionately denied access to the program.\nFinally, we can model this cost disparity by looking at the data. This modelling experiment is informed on the equation:\n\\(logcost \\approx w_b\\) \\(*\\) (patient is black) \\(+\\) intercept \\(+\\) $_{i=1} ^{k} w_k * \\((gagne sum)\\)^k$\nWhere \\(w_b\\) is the weight a model gives to the variable that a patient is black and \\(w_k\\) are the weights given to the polynomial data of the number of chronic illnesses.\nFrom this we also find that \\(e^{w_b}\\) is the percentage of average cost incurred by black patients compared to white patients\nTo start, we observe that 95% of the patients have 5 or less chronic illnesses, making it a good subset of the data to generalize over.\n\ndf[df[\"gagne_sum_t\"] &lt;= 5][\"gagne_sum_t\"].count()/df.shape[0]\n\n0.9553952115447688\n\n\nNext, we can create a new dataframe eithin that subset that creates a log-transform of the cost (ignoring any 0-cost patients). This allows us to look at data that spans orders of magnitude. Further, for the model to work, we must encode race as a number. In this case, 0 is coded to be white and 1 black.\n\ndf_log = df.query('gagne_sum_t &lt;= 5 & cost_t != 0')\ndf_log[\"log_cost\"] = np.log(df_log[\"cost_t\"])\n\ndf_log[\"race_num\"] = 1 * (df_log[\"race\"] == \"black\")\n\nThen we can take predictor and target columns of this data, where race and number of chronic illness are the predictors (X) and the log cost is the target (y).\n\npred_X = df_log[[\"race_num\", \"gagne_sum_t\"]]\ntarget_y = df_log[\"log_cost\"]\n\n\ndf_log\n\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\nrisk_percentile\nlog_cost\nrace_num\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n35\n7.090077\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n4\n3\n86\n7.863267\n0\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n3\n6.214608\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n11\n7.170120\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n1\n1\n98\n7.003065\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48779\n0.611517\n0\n800.0\n0.0\nNaN\nNaN\nNaN\n1.090000\n148.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n7\n6.684612\n0\n\n\n48780\n2.615933\n0\n2200.0\n0.0\n112.0\nNaN\n41.4\n0.810000\n172.0\nwhite\n...\n0\n1\n0\n0\n1\n1\n1\n46\n7.696213\n0\n\n\n48781\n1.358926\n0\n800.0\n0.0\n105.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n1\n0\n0\n1\n1\n0\n23\n6.684612\n0\n\n\n48782\n10.990318\n0\n1300.0\n0.0\n132.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n3\n3\n94\n7.170120\n0\n\n\n48783\n1.681671\n0\n4400.0\n0.0\n115.0\n5.6\n36.6\n0.940000\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n29\n8.389360\n0\n\n\n\n\n44748 rows × 163 columns\n\n\n\n\nThe function add_polynomial_features returns a dataset with exponential values of the number of chronic illnesses. This is useful for training models that act on non-linear data.\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nPerforming cross-validation on the degrees of the polynomial finds the most accurate to be a degree of 6.\n\nfrom sklearn.linear_model import LinearRegression\n\nrecord = (0, 0)\n\nfor degree in range (1, 15):\n    poly_X = add_polynomial_features(pred_X, degree)\n\n    LinR = LinearRegression()\n    LinR.fit(poly_X, target_y)\n    tempScore = LinR.score(poly_X, target_y)\n    # print(degree, tempScore)\n    if tempScore &gt; record[1]:\n        record = (degree, tempScore)\nrecord\n\n(6, 0.08800788381979874)\n\n\nIf we fit the model to this polynomial dataset and the target variable, we find can find the value of wb by looking at the first coefficient of our linear regression model.\n\npoly_X = add_polynomial_features(pred_X, 6)\nLinR = LinearRegression()\nLinR.fit(poly_X, target_y)\n# LinR.score(poly_X, target_y)\nLinR.coef_[0]\n\n-0.2827181024769897\n\n\nFinally, we can get an percentage of average cost paid by black patients when compared to white patients by calculating \\(e^{w_b}\\).\n\npercent = np.exp(LinR.coef_[0])\npercent\n\n0.7537322331639498\n\n\nThis percentage of around 75% makes sense with regard to the study. Specifically, the study notes that “Blacks generate lower costs than Whites – on average, $1801 less per year…” “… or $1144 less, if we instead hold constant the specific individual illnesses that contribute to the sum.” This roughly lines up with the model’s prediction that black people will have a 75% lower mean total medical expenditure compared to white people."
  },
  {
    "objectID": "posts/bias-replication/bias_replication.html#discussion",
    "href": "posts/bias-replication/bias_replication.html#discussion",
    "title": "Bias Replication Study",
    "section": "Discussion",
    "text": "Discussion\nIn the process of replicating this study, I confirmed the findings that black patients are more likely to be placed in lower risk percentiles given equal prevalence of chronic illness when compared to white patients. I also confirmed that, while costs are relatively similar between patients of the same risk percentile and different races, costs are disparate (black patients have less cost) when comparing between chronic illness. This was then quantified through a model, with a prediction of black patients having 75% of the cost of white patients. With the risk score being calculated with respect to cost, then, disparate outcomes across races are seen considering a more realistic risk variable like chronic illness count.\nAs such, the discrimination criteria of calibration bias is present in the formation of the risk score. This is seen through the dissection of risk score when compared against chronic illness versus cost. While there isn’t much bias in calculating risk score across race when cost is considering factor, there is bias when considering risk score in relation to number of chronic illness. That is to say, those with similar risk scores will have similar costs across race (the second graph in the blog post and figure 3A in the study), but will have dissimilar numbers of chronic illnesses, with black patients having more chronic illnesses at similar risk scores on average (the first graph and figure 1 in the study). The model is then calibrated to the variable of cost rather than health outcome, a fact that is mentioned at the beginning of the study: “…assessing how well the algorithmic risk score is calibrated across race for health outcomes \\(H_{i,t}\\). We also ask how well the algorithm is calibrated for costs \\(C_{i,t}\\).”\nThrough this blog post, I improved my graphing skills, especially when needing to graph different groups on the same graph. I also improved my ability in modelling with polynomial datasets and transforming data to be more useful to my models and graphs (log-transforms, qcuts, etc.)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Testing quarto!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Newton’s Method for Logistic Regression\n\n\n\n\n\nCreating Newton’s optimizer for logistic regression and comparing it to standard gradient descent optimization.\n\n\n\n\n\nMay 16, 2024\n\n\nJeff Blake\n\n\n\n\n\n\n\n\n\n\n\n\nGenre Deep Learning\n\n\n\n\n\nCreating 3 deep learning models that predict the genre of a song based on spotify data and lyrics.\n\n\n\n\n\nMay 15, 2024\n\n\nJeff Blake\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n\n\n\n\n\nCreating a model for logistic regression in Python.\n\n\n\n\n\nMay 12, 2024\n\n\nJeff Blake\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptron\n\n\n\n\n\nCreating a perceptron implementation and experimenting on its performance on different datasets (linearly separable/non-separable), alongside experiments on minibatch sizes\n\n\n\n\n\nApr 28, 2024\n\n\nJeff Blake\n\n\n\n\n\n\n\n\n\n\n\n\nBias Replication Study\n\n\n\n\n\nReplicating a study on the biases of predictive healthcare models\n\n\n\n\n\nApr 15, 2024\n\n\nJeff Blake\n\n\n\n\n\n\n\n\n\n\n\n\n\"Optimal\" Decision Making\n\n\n\n\n\nMaking “optimal” decisions based on loan-default data and model training. The definition of optimal, though, becomes flawed with profit incentives.\n\n\n\n\n\nMar 27, 2024\n\n\nJeff Blake\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nHow I made models that could predict the species from the Palmer Penguins dataset.\n\n\n\n\n\nFeb 28, 2023\n\n\nJeff Blake\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/genre-deep-learning/index.html",
    "href": "posts/genre-deep-learning/index.html",
    "title": "Genre Deep Learning",
    "section": "",
    "text": "In this blog post, I create three different machine learning models to predict the genre of music based on data from Spotify. These models are trained on lyric data, feature data, and both respectively. In the creation of these models, I make utilities to facilitate the training and processing, including a tokenized vocabulary and a dataloader. In the process of this blog post, I will examine the different ways that these models are built and their accuracy on the data.\n\n\n\nFirst, I import relevant libraries for data processing and read the data into a pandas dataframe. I also determine my device to allow for faster processing if a GPU is available.\n\nimport pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/tcc_ceds_music.csv\"\ndf = pd.read_csv(url)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nLet’s check out the data, noting the columns with relevant information (particularly 4 and 5 holding genre and lyrics):\n\ndf.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nartist_name\ntrack_name\nrelease_date\ngenre\nlyrics\nlen\ndating\nviolence\nworld/life\n...\nsadness\nfeelings\ndanceability\nloudness\nacousticness\ninstrumentalness\nvalence\nenergy\ntopic\nage\n\n\n\n\n0\n0\nmukesh\nmohabbat bhi jhoothi\n1950\npop\nhold time feel break feel untrue convince spea...\n95\n0.000598\n0.063746\n0.000598\n...\n0.380299\n0.117175\n0.357739\n0.454119\n0.997992\n0.901822\n0.339448\n0.137110\nsadness\n1.0\n\n\n1\n4\nfrankie laine\ni believe\n1950\npop\nbelieve drop rain fall grow believe darkest ni...\n51\n0.035537\n0.096777\n0.443435\n...\n0.001284\n0.001284\n0.331745\n0.647540\n0.954819\n0.000002\n0.325021\n0.263240\nworld/life\n1.0\n\n\n2\n6\njohnnie ray\ncry\n1950\npop\nsweetheart send letter goodbye secret feel bet...\n24\n0.002770\n0.002770\n0.002770\n...\n0.002770\n0.225422\n0.456298\n0.585288\n0.840361\n0.000000\n0.351814\n0.139112\nmusic\n1.0\n\n\n3\n10\npérez prado\npatricia\n1950\npop\nkiss lips want stroll charm mambo chacha merin...\n54\n0.048249\n0.001548\n0.001548\n...\n0.225889\n0.001548\n0.686992\n0.744404\n0.083935\n0.199393\n0.775350\n0.743736\nromantic\n1.0\n\n\n4\n12\ngiorgos papadopoulos\napopse eida oneiro\n1950\npop\ntill darling till matter know till dream live ...\n48\n0.001350\n0.001350\n0.417772\n...\n0.068800\n0.001350\n0.291671\n0.646489\n0.975904\n0.000246\n0.597073\n0.394375\nromantic\n1.0\n\n\n\n\n5 rows × 31 columns\n\n\n\n\nTo check the different genres that exist in this dataset, we can use the function .unique() to select for non-duplicate instances in the genre column.\n\ndf[\"genre\"].unique()\n\narray(['pop', 'country', 'blues', 'jazz', 'reggae', 'rock', 'hip hop'],\n      dtype=object)\n\n\n\n\n\nWith this information, I can create a numbered dictionary of the genres. This dictionary will make the conversion from string to integer (needed for processing by our models) easier, as seen below. I also save the engineered features in a separate array to be used later.\n\ngenres = {\n    \"pop\" : 0,\n    \"country\" : 1,\n    \"blues\" : 2,\n    \"jazz\" : 3,\n    \"reggae\" : 4,\n    \"rock\" : 5,\n    \"hip hop\" : 6\n}\n\n\ndf[\"genre\"] = df[\"genre\"].apply(genres.get)\n\n\nengineered_features = ['dating', 'violence', 'world/life', 'night/time','shake the audience','family/gospel', 'romantic', 'communication','obscene', 'music', 'movement/places', 'light/visual perceptions','family/spiritual', 'like/girls', 'sadness', 'feelings', 'danceability','loudness', 'acousticness', 'instrumentalness', 'valence', 'energy']\n\nFor the dataset, I adapt the example from our lecture on text classification to instead index the lyrics, features, and genre.\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass TextDataFromDF(Dataset):\n    def __init__(self, df):\n        self.df = df\n\n    def __getitem__(self, index):\n        # 5 is the lyrics column, 4 is the number encoded genre\n        return self.df.iloc[index, 5], np.array(self.df.iloc[index, 7:29].values, dtype=float), self.df.iloc[index, 4]\n\n    def __len__(self):\n        return len(self.df)\n\nNext, I create training and testing dataframes using scikit-learn’s train_test_split function and convert them into torch datasets using the class I created above.\n\ndf_train, df_val = train_test_split(df, shuffle = True, test_size = 0.2)\ntrain_data = TextDataFromDF(df_train)\nval_data = TextDataFromDF(df_val)\n\nNow, I can check individual pieces of data that only hold the relevant information for the model – the potential predictors of lyrics and features alongside the target genre.\n\ntrain_data[194]\n\n('gentle touch stop world spin natural thing believe reason life suddenly look differently bring peace spell need search world know perfect suddenly live know doubt simple word convey feel',\n array([2.28832956e-03, 2.28832958e-03, 6.48654422e-01, 2.28832953e-03,\n        2.28832974e-03, 2.28832954e-03, 3.30286892e-02, 1.30058079e-01,\n        2.28832959e-03, 2.28832961e-03, 2.28832959e-03, 6.43196833e-02,\n        2.28832970e-03, 2.28832972e-03, 2.28832957e-03, 3.99656779e-02,\n        6.36087945e-01, 7.55531626e-01, 4.01605825e-01, 4.59514170e-06,\n        3.29142622e-01, 5.12497287e-01]),\n 1)\n\n\nUsing torchtext’s get_tokenizer function to split the lyrics into individual words.\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntokenizer = get_tokenizer('basic_english')\n\ntokenized = tokenizer(train_data[194][0])\ntokenized\n\n['gentle',\n 'touch',\n 'stop',\n 'world',\n 'spin',\n 'natural',\n 'thing',\n 'believe',\n 'reason',\n 'life',\n 'suddenly',\n 'look',\n 'differently',\n 'bring',\n 'peace',\n 'spell',\n 'need',\n 'search',\n 'world',\n 'know',\n 'perfect',\n 'suddenly',\n 'live',\n 'know',\n 'doubt',\n 'simple',\n 'word',\n 'convey',\n 'feel']\n\n\nTo determine a good max length for my text pipeline, I created a column in my dataframe that converted the lyrics into tokens. From there, I could check the length of each array of tokens, finding the largest in my dataset. The maximum length in the dataframe is 199, so I’ll set my max length to be 200.\n\ndf[\"lens\"] = df[\"lyrics\"].apply(tokenizer).apply(len)\ndf[\"lens\"].max()\n\n199\n\n\nI then create a vocabulary using a similar setup to the text classification lecture, but necessitating a minimum frequency of 50 for a word to be included in the vocabulary to reduce the vocabulary’s size. This creates an indexed vocabulary from all frequent, tokenized training data, using the character unknown as a default value.\n\ndef yield_tokens(data_iter):\n    for text, _, _ in data_iter:\n        yield tokenizer(text)\n\nvocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"&lt;unk&gt;\"], min_freq = 50)\nvocab.set_default_index(vocab[\"&lt;unk&gt;\"])\n\nLet’s take a look at the vocabulary using get_itos():\n\nvocab.get_itos()[0:10]\n\n['&lt;unk&gt;',\n 'know',\n 'like',\n 'time',\n 'come',\n 'go',\n 'away',\n 'heart',\n 'feel',\n 'yeah']\n\n\n\n\n\nSimilarly, I use the lecture’s code for the text_pipeline function, changing the max length to fit this dataset. This function creates a sequence of integers from the vocabulary, ensuring that the data instance is always of size max_len (either padding or truncating as needed). Similarly, a label pipeline just ensures that the genre label is correctly typed as an integer.\n\nmax_len = 200\nnum_tokens = len(vocab.get_itos())\ndef text_pipeline(x):\n    tokens = vocab(tokenizer(x))\n    y = torch.zeros(max_len, dtype=torch.int64) + num_tokens\n    if len(tokens) &gt; max_len:\n        tokens = tokens[0:max_len]\n    y[0:len(tokens)] = torch.tensor(tokens,dtype=torch.int64)\n    return y\n\nlabel_pipeline = lambda x: int(x)\n\nCollate batch is also adapted from the lecture, adding a list for the feature predictors in the process. This function uses the pipelining tools created above to turn a batch of data into torch tensors.\n\ndef collate_batch(batch):\n    label_list, text_list, feature_list = [], [], []\n    for (_text, _features, _label) in batch:\n\n         # add label to list\n         label_list.append(label_pipeline(_label))\n\n         # add text (as sequence of integers) to list\n         processed_text = text_pipeline(_text)\n         text_list.append(processed_text)\n\n         # add features (as sequence of floats) to list\n         feature_list.append(_features)\n\n    label_list = torch.tensor(label_list, dtype=torch.int64)\n    text_list = torch.stack(text_list)\n    feature_list = torch.tensor(np.array(feature_list), dtype=torch.float64)\n    return text_list.to(device), feature_list.to(device), label_list.to(device)\n\nNow, I can create two dataloaders, one for the training data and one for the test data. When I run next(iter()) on the dataloaders, I receive a tuple with the predictor tensors (with batch_size instances in each tensor) and target tensor.\n\ntrain_loader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate_batch)\nval_loader = DataLoader(val_data, batch_size=8, shuffle=True, collate_fn=collate_batch)\nnext(iter(train_loader))\n\n(tensor([[  15,  164,   78,  ..., 2901, 2901, 2901],\n         [   0,    0,    0,  ..., 2901, 2901, 2901],\n         [   9,    0,    0,  ..., 2901, 2901, 2901],\n         ...,\n         [ 584,  584,  584,  ..., 2901, 2901, 2901],\n         [   0,    0,  311,  ..., 2901, 2901, 2901],\n         [   0,  533,   48,  ..., 2901, 2901, 2901]]),\n tensor([[4.3860e-03, 4.3860e-03, 4.3860e-03, 4.3860e-03, 4.3860e-03, 4.3860e-03,\n          2.0704e-01, 4.0865e-01, 4.3860e-03, 4.3860e-03, 4.3860e-03, 4.3860e-03,\n          4.3860e-03, 4.3860e-03, 3.1413e-01, 4.3860e-03, 6.5883e-01, 6.1159e-01,\n          7.9217e-01, 0.0000e+00, 6.4138e-01, 3.5133e-01],\n         [2.6316e-02, 2.6316e-02, 2.6316e-02, 2.6316e-02, 2.6316e-02, 2.6316e-02,\n          2.6316e-02, 2.6316e-02, 5.2632e-01, 2.6316e-02, 2.6316e-02, 2.6316e-02,\n          2.6316e-02, 2.6316e-02, 2.6316e-02, 2.6316e-02, 5.8302e-01, 4.9068e-01,\n          3.3233e-01, 7.4899e-01, 4.3219e-01, 4.2641e-01],\n         [6.0496e-04, 2.1894e-02, 2.8639e-01, 3.8409e-02, 2.5017e-02, 3.8173e-02,\n          1.3884e-02, 6.0496e-04, 3.0673e-01, 6.0496e-04, 6.0496e-04, 6.0496e-04,\n          1.6003e-01, 6.0496e-04, 7.5811e-02, 6.0496e-04, 6.1118e-01, 5.0201e-01,\n          2.8714e-02, 4.7267e-03, 6.7230e-01, 4.3141e-01],\n         [5.8480e-04, 4.8245e-02, 8.4931e-02, 5.8480e-04, 1.7697e-02, 5.8480e-04,\n          5.8480e-04, 1.8973e-01, 5.7024e-01, 2.5208e-02, 5.8480e-04, 5.8480e-04,\n          5.8480e-04, 5.8480e-04, 5.8480e-04, 2.8871e-02, 7.8772e-01, 7.0172e-01,\n          2.4497e-02, 0.0000e+00, 2.5495e-01, 6.0860e-01],\n         [4.1579e-01, 2.1778e-02, 5.7208e-04, 3.8229e-02, 1.1558e-02, 5.7208e-04,\n          1.0301e-01, 3.9343e-02, 5.7208e-04, 3.0452e-01, 5.7208e-04, 5.7208e-04,\n          5.7208e-04, 5.9471e-02, 5.7208e-04, 5.7208e-04, 3.7182e-01, 6.7938e-01,\n          3.0522e-01, 1.2247e-04, 4.7341e-01, 5.4653e-01],\n         [6.1920e-04, 6.1920e-04, 6.1920e-04, 1.1353e-01, 6.1920e-04, 6.1920e-04,\n          6.1920e-04, 6.1920e-04, 7.5624e-01, 6.1920e-04, 6.1920e-04, 6.1920e-04,\n          6.1920e-04, 6.1920e-04, 1.0648e-01, 6.1920e-04, 7.7472e-01, 5.8962e-01,\n          3.2219e-03, 7.5405e-01, 5.2597e-01, 3.0729e-01],\n         [1.2837e-03, 3.3413e-01, 2.5204e-01, 1.2837e-03, 1.2837e-03, 1.2837e-03,\n          1.2837e-03, 1.2837e-03, 3.0053e-01, 1.2837e-03, 1.2837e-03, 1.2837e-03,\n          1.2837e-03, 1.2837e-03, 1.2837e-03, 1.2837e-03, 4.6171e-01, 7.7986e-01,\n          1.3152e-02, 8.3401e-01, 4.7960e-01, 8.4084e-01],\n         [1.1198e-03, 1.1198e-03, 6.2542e-01, 1.1198e-03, 1.1198e-03, 1.1198e-03,\n          1.1198e-03, 1.1198e-03, 1.1198e-03, 2.1673e-01, 5.4501e-02, 1.1198e-03,\n          2.2755e-02, 1.1198e-03, 1.1198e-03, 4.2350e-02, 4.0864e-01, 4.1399e-01,\n          9.2671e-01, 4.9089e-04, 1.2613e-01, 1.2009e-01]], dtype=torch.float64),\n tensor([0, 3, 2, 0, 0, 4, 3, 3]))\n\n\n\n\n\nWith the data processed, a training function can be made. This function initializes an optimizer and iterates through a batch of the data, clearing the gradients, receiving predictions from the model, computing the loss and gradient, then performing a step of the optimizer. This function also checks for the tags to use lyrics or features to determine what data to pass each model. The lyrics tensor can be indexed with data[0] while the features can be accessed with data[1].\n\nimport time\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\ndef train(model, dataloader, use_lyrics, use_features, lr=.01,):\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    epoch_start_time = time.time()\n    # keep track of some counts for measuring accuracy\n    total_acc, total_count = 0, 0\n    log_interval = 300\n    start_time = time.time()\n\n    for idx, data in enumerate(dataloader):\n        # clear gradients\n        optimizer.zero_grad()\n\n        # slice data, form prediction on batch\n        if (use_lyrics and use_features):\n          predicted_label = model(data[0], data[1])\n        elif use_lyrics:\n          predicted_label = model(data[0])\n        elif use_features:\n          predicted_label = model(data[1])\n        else:\n          print(\"Please indicate which data will be used\")\n        label = data[2]\n\n        # evaluate loss on prediction\n        loss = loss_fn(predicted_label, label)\n        # compute gradient\n        loss.backward()\n        # take an optimization step\n        optimizer.step()\n\n        # for printing accuracy\n        total_acc   += (predicted_label.argmax(1) == label).sum().item()\n        total_count += label.size(0)\n    # print(model.lin1.weight)\n    print(f'| epoch {epoch:3d} | train accuracy {total_acc/total_count:8.3f} | time: {time.time() - epoch_start_time:5.2f}s')\n\ndef evaluate(model, dataloader, use_lyrics, use_features):\n\n    total_acc, total_count = 0, 0\n\n    with torch.no_grad():\n        for idx, (text, features, label) in enumerate(dataloader):\n          if (use_lyrics and use_features):\n            predicted_label = model(text, features)\n          elif use_lyrics:\n            predicted_label = model(text)\n          elif use_features:\n            predicted_label = model(features)\n          else:\n            print(\"Please indicate which data will be used\")\n          total_acc += (predicted_label.argmax(1) == label).sum().item()\n          total_count += label.size(0)\n    return total_acc/total_count\n\nBefore training the different models, I want to check the base rate to see if my model performs better. This can be done by grouping the dataframe by genre and dividing each group’s size by the total number of data entries. The highest frequency of genre is pop with a probability of 25%.\n\ndf.groupby(\"genre\").size() / len(df)\n\ngenre\n0    0.248202\n1    0.191915\n2    0.162273\n3    0.135521\n4    0.088045\n5    0.142182\n6    0.031862\ndtype: float64\n\n\n\n\n\nThe first model I tested was the lyrics-only model. This was done with an embedding layer that was then dropped out and averaged across the embedding dimensions. Finally, more fully-connected layers were added, ending by converting the dimension to the 7 possible genres.\n\nfrom torch import nn\nfrom torch.nn import ReLU\n\nclass GenreByLyrics(nn.Module):\n\n    def __init__(self, vocab_size, embedding_dim, max_len, num_class):\n        super().__init__()\n\n        self.embed_pipeline = nn.Sequential(\n          nn.Embedding(vocab_size+1, embedding_dim),\n          nn.Dropout(p=0.2)\n        )\n\n        self.linear_pipeline = nn.Sequential(\n\n          nn.Linear(embedding_dim, 512),\n          ReLU(),\n\n          nn.Linear(512, 256),\n          ReLU(),\n\n          nn.Linear(256, 32),\n          ReLU(),\n\n          nn.Linear(32, 16),\n          ReLU(),\n\n          nn.Linear(16, 8),\n          ReLU(),\n\n          nn.Linear(8, num_class)\n        )\n\n    def forward(self, x):\n      x = self.embed_pipeline(x)\n      x = x.mean(axis=1)\n\n      return(self.linear_pipeline(x))\nvocab_size = len(vocab)\nembedding_dim = 3\nlyrics_model = GenreByLyrics(vocab_size, embedding_dim, max_len, 7).to(device)\n\nIn training this model, I achieved an accuracy of 33% on the testing data. There seems to be a fair amount of overfitting, considering the ~38% training accuracy.\n\n#---\nEPOCHS = 20\nfor epoch in range(1, EPOCHS + 1):\n    train(lyrics_model, train_loader, True, False, lr=0.005)\n#---\n\n| epoch   1 | train accuracy    0.254 | time: 37.79s\n| epoch   2 | train accuracy    0.280 | time: 33.23s\n| epoch   3 | train accuracy    0.298 | time: 35.02s\n| epoch   4 | train accuracy    0.315 | time: 32.46s\n| epoch   5 | train accuracy    0.327 | time: 33.51s\n| epoch   6 | train accuracy    0.326 | time: 32.48s\n| epoch   7 | train accuracy    0.331 | time: 38.45s\n| epoch   8 | train accuracy    0.341 | time: 47.11s\n| epoch   9 | train accuracy    0.351 | time: 43.49s\n| epoch  10 | train accuracy    0.352 | time: 43.63s\n| epoch  11 | train accuracy    0.356 | time: 42.64s\n| epoch  12 | train accuracy    0.357 | time: 45.15s\n| epoch  13 | train accuracy    0.357 | time: 43.58s\n| epoch  14 | train accuracy    0.363 | time: 43.70s\n| epoch  15 | train accuracy    0.367 | time: 43.65s\n| epoch  16 | train accuracy    0.372 | time: 48.58s\n| epoch  17 | train accuracy    0.374 | time: 53.36s\n| epoch  18 | train accuracy    0.376 | time: 54.66s\n| epoch  19 | train accuracy    0.383 | time: 48.10s\n| epoch  20 | train accuracy    0.382 | time: 48.77s\n\n\n\nevaluate(lyrics_model, val_loader, True, False)\n\n0.33198237885462556\n\n\n\n\nWe can examine the embedding layer using processes from the lecture. First, we take weights from the model’s embedding pipeline:\n\nembedding_matrix = lyrics_model.embed_pipeline[0].cpu().weight.data.numpy()\nembedding_matrix\n\narray([[-9.5287114e-01,  1.8091378e-01, -3.0193642e-01],\n       [ 2.5800321e-01, -1.8974684e-01, -5.5249542e-01],\n       [-4.6847956e-03, -2.0129973e-02,  7.3867464e-01],\n       ...,\n       [-4.3154783e+00, -5.9403486e+00, -1.3008388e+00],\n       [-4.9993868e+00, -3.2404058e+00, -1.9059108e+00],\n       [ 8.8998146e-02, -4.9715936e-02,  3.2505211e-01]], dtype=float32)\n\n\nNext, we can use principal component analysis to convert the embedding matrix to 2-dimensional data. This data can then be converted to a dataframe using the tokens information.\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nweights = pca.fit_transform(embedding_matrix)\n\ntokens = vocab.get_itos()\ntokens.append(\" \")\nembedding_df = pd.DataFrame({\n    'word' : tokens,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\n\nembedding_df\n\n\n\n\n\n\n\n\n\nword\nx0\nx1\n\n\n\n\n0\n&lt;unk&gt;\n0.415862\n-0.077251\n\n\n1\nknow\n0.013589\n-0.474422\n\n\n2\nlike\n-0.676194\n0.546537\n\n\n3\ntime\n-0.382320\n-0.284132\n\n\n4\ncome\n-0.090594\n0.622675\n\n\n...\n...\n...\n...\n\n\n2897\nthighs\n4.808725\n-2.019642\n\n\n2898\ntint\n6.167381\n0.360619\n\n\n2899\nwobble\n6.274076\n3.378024\n\n\n2900\nwrath\n5.639404\n1.722761\n\n\n2901\n\n-0.474028\n0.203610\n\n\n\n\n2902 rows × 3 columns\n\n\n\n\nWe can then plot this data iteractively using plotly. Compared to the visualization in the lecture, it is harder to find distinct points for each target variable. This is likely due to the fact that this model is trained to predict 7 target variables compared to the lecture’s three. Also interestingly, reggae and country seem to be the most defined points on the plot (i.e. lying on the wider edges of the plot), lying far in the right and left respectively. There are also huge outliers like “lyric” and “commercial” that lie far to the top right of the plot. These genres extend to around 10 points on the x-axis further than most words plotted. Similarly, it’s interesting how wide of a range exists on the x-axis compared to the y-axis (around -20 to 20 versus -10 to 10).\n\nimport plotly.express as px\nimport plotly.io as pio\n\n# for VSCode plotly rendering\npio.renderers.default = \"notebook\"\n\npio.templates.default = \"plotly_white\"\n\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 10,\n                 hover_name = \"word\")\n\nfig.show()\n\n                                                \n\n\n\n\n\nNext, I trained a model on the engineered features. This was done using only fully-connected layers, including Linear layers and ReLU. This model performed better than the lyrics-only model, achieving a testing accuracy of 35%. Interestingly, this was despite a training accuracy of 34%.\n\nclass GenreByFeatures(nn.Module):\n  def __init__(self, num_class):\n    super().__init__()\n    self.pipeline = nn.Sequential(\n        nn.Linear(22, 512, dtype=torch.float64),\n        nn.ReLU(),\n\n        nn.Linear(512, 256, dtype=torch.float64),\n        nn.ReLU(),\n\n        nn.Linear(256, 128, dtype=torch.float64),\n        nn.ReLU(),\n\n        nn.Linear(128, 64, dtype=torch.float64),\n        nn.ReLU(),\n\n        nn.Linear(64, 32, dtype=torch.float64),\n        nn.ReLU(),\n\n        nn.Flatten(1),\n        nn.Linear(32, num_class, dtype=torch.float64),\n    )\n\n  def forward(self, x):\n    return self.pipeline(x)\n\n\nfeatures_model = GenreByFeatures(7).to(device)\nfor epoch in range(1, EPOCHS + 1):\n    train(features_model, train_loader, False, True, lr=0.01)\n\n| epoch   1 | train accuracy    0.259 | time: 55.45s\n| epoch   2 | train accuracy    0.310 | time: 52.04s\n| epoch   3 | train accuracy    0.319 | time: 57.36s\n| epoch   4 | train accuracy    0.324 | time: 51.28s\n| epoch   5 | train accuracy    0.327 | time: 49.28s\n| epoch   6 | train accuracy    0.328 | time: 53.08s\n| epoch   7 | train accuracy    0.331 | time: 52.06s\n| epoch   8 | train accuracy    0.331 | time: 55.44s\n| epoch   9 | train accuracy    0.336 | time: 53.19s\n| epoch  10 | train accuracy    0.330 | time: 51.03s\n| epoch  11 | train accuracy    0.336 | time: 49.87s\n| epoch  12 | train accuracy    0.338 | time: 49.01s\n| epoch  13 | train accuracy    0.298 | time: 51.94s\n| epoch  14 | train accuracy    0.290 | time: 53.90s\n| epoch  15 | train accuracy    0.318 | time: 58.61s\n| epoch  16 | train accuracy    0.328 | time: 53.22s\n| epoch  17 | train accuracy    0.322 | time: 48.93s\n| epoch  18 | train accuracy    0.329 | time: 49.14s\n| epoch  19 | train accuracy    0.336 | time: 49.22s\n| epoch  20 | train accuracy    0.342 | time: 53.90s\n\n\n\nevaluate(features_model, val_loader, False, True)\n\n0.3501321585903084\n\n\nFinally, I trained a model using both the lyrics and engineered features. This uses the first model’s embedding and dropout layers for processing the lyrics alongside a few linear and ReLU layers for the engineered features. The two tensors of data are then concatenated and run through more linear layers before returning the prediction. This model performed significantly better than the other models, achieving a training accuracy of almost 50% and a testing accuracy of 43%.\n\nclass GenreByLyricsAndFeatures(GenreByFeatures):\n  def __init__(self, vocab_size, embedding_dim, max_len, num_class):\n        super().__init__(num_class)\n\n        self.embed_pipeline = nn.Sequential(\n          nn.Embedding(vocab_size+1, embedding_dim),\n          nn.Dropout(p=0.2)\n        )\n\n        self.linear_pipeline = nn.Sequential(\n          nn.Linear(embedding_dim, 512),\n          ReLU(),\n\n          nn.Linear(512, 256),\n          ReLU(),\n\n          nn.Linear(256, 32),\n          ReLU(),\n\n          nn.Linear(32, 16),\n          ReLU(),\n\n          nn.Linear(16, 8),\n          ReLU(),\n\n          nn.Linear(8, num_class)\n        )\n\n        self.fc = nn.Linear(max_len*embedding_dim, num_class)\n        self.num_class = num_class\n\n        self.cat_pipeline = nn.Sequential(\n          nn.Linear(10, 64, dtype=torch.float64),\n          nn.ReLU(),\n\n          nn.Linear(64, 16, dtype=torch.float64),\n          nn.ReLU(),\n\n          nn.Flatten(1),\n          nn.Linear(16, self.num_class, dtype=torch.float64)\n        )\n\n  def forward(self, x1, x2):\n    # embed x1\n    x1 = self.embed_pipeline(x1)\n    x1 = x1.mean(axis=1)\n\n    # fully connected layers for x2\n    x2 = self.pipeline(x2)\n\n    # flatten each to be 2d tensors before concatenating\n    x = torch.cat((x1, x2), 1)\n\n    return self.cat_pipeline(x)\n\n\ndual_model = GenreByLyricsAndFeatures(vocab_size, embedding_dim, max_len, 7).to(device)\nfor epoch in range(1, EPOCHS + 1):\n    train(dual_model, train_loader, True, True, lr=0.001)\n\n| epoch   1 | train accuracy    0.267 | time: 54.33s\n| epoch   2 | train accuracy    0.319 | time: 44.52s\n| epoch   3 | train accuracy    0.363 | time: 44.24s\n| epoch   4 | train accuracy    0.387 | time: 46.43s\n| epoch   5 | train accuracy    0.395 | time: 44.80s\n| epoch   6 | train accuracy    0.408 | time: 45.50s\n| epoch   7 | train accuracy    0.412 | time: 44.31s\n| epoch   8 | train accuracy    0.421 | time: 47.05s\n| epoch   9 | train accuracy    0.431 | time: 44.49s\n| epoch  10 | train accuracy    0.440 | time: 43.84s\n| epoch  11 | train accuracy    0.445 | time: 44.33s\n| epoch  12 | train accuracy    0.450 | time: 46.05s\n| epoch  13 | train accuracy    0.455 | time: 44.49s\n| epoch  14 | train accuracy    0.464 | time: 44.18s\n| epoch  15 | train accuracy    0.466 | time: 44.80s\n| epoch  16 | train accuracy    0.475 | time: 45.80s\n| epoch  17 | train accuracy    0.477 | time: 44.06s\n| epoch  18 | train accuracy    0.488 | time: 44.01s\n| epoch  19 | train accuracy    0.491 | time: 44.15s\n| epoch  20 | train accuracy    0.498 | time: 46.03s\n\n\n\nevaluate(dual_model, val_loader, True, True)\n\n0.4299559471365639\n\n\n\n\n\n\nIn the process of training these models, I found that the model trained with both the lyrics and features resulted in the highest accuracy. Further, I found that embeddings at higher dimensions are harder to visualize in a way that conveys consistently clear information about the target variable. In doing this blog post, I gained a lot of knowledge about data processing and loading (the creation of torch datasets, vocabularies, dataloaders, etc.) and practiced the creation of layers in different models. In particular, I learned more about model embeddings and how they can be used in tandem with fully-connected linear layers. The training process also showed me that including varied types of data (lyric versus engineered features) is extremely important in model creation."
  },
  {
    "objectID": "posts/genre-deep-learning/index.html#introduction",
    "href": "posts/genre-deep-learning/index.html#introduction",
    "title": "Genre Deep Learning",
    "section": "",
    "text": "In this blog post, I create three different machine learning models to predict the genre of music based on data from Spotify. These models are trained on lyric data, feature data, and both respectively. In the creation of these models, I make utilities to facilitate the training and processing, including a tokenized vocabulary and a dataloader. In the process of this blog post, I will examine the different ways that these models are built and their accuracy on the data."
  },
  {
    "objectID": "posts/genre-deep-learning/index.html#data-viewing",
    "href": "posts/genre-deep-learning/index.html#data-viewing",
    "title": "Genre Deep Learning",
    "section": "",
    "text": "First, I import relevant libraries for data processing and read the data into a pandas dataframe. I also determine my device to allow for faster processing if a GPU is available.\n\nimport pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/tcc_ceds_music.csv\"\ndf = pd.read_csv(url)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nLet’s check out the data, noting the columns with relevant information (particularly 4 and 5 holding genre and lyrics):\n\ndf.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nartist_name\ntrack_name\nrelease_date\ngenre\nlyrics\nlen\ndating\nviolence\nworld/life\n...\nsadness\nfeelings\ndanceability\nloudness\nacousticness\ninstrumentalness\nvalence\nenergy\ntopic\nage\n\n\n\n\n0\n0\nmukesh\nmohabbat bhi jhoothi\n1950\npop\nhold time feel break feel untrue convince spea...\n95\n0.000598\n0.063746\n0.000598\n...\n0.380299\n0.117175\n0.357739\n0.454119\n0.997992\n0.901822\n0.339448\n0.137110\nsadness\n1.0\n\n\n1\n4\nfrankie laine\ni believe\n1950\npop\nbelieve drop rain fall grow believe darkest ni...\n51\n0.035537\n0.096777\n0.443435\n...\n0.001284\n0.001284\n0.331745\n0.647540\n0.954819\n0.000002\n0.325021\n0.263240\nworld/life\n1.0\n\n\n2\n6\njohnnie ray\ncry\n1950\npop\nsweetheart send letter goodbye secret feel bet...\n24\n0.002770\n0.002770\n0.002770\n...\n0.002770\n0.225422\n0.456298\n0.585288\n0.840361\n0.000000\n0.351814\n0.139112\nmusic\n1.0\n\n\n3\n10\npérez prado\npatricia\n1950\npop\nkiss lips want stroll charm mambo chacha merin...\n54\n0.048249\n0.001548\n0.001548\n...\n0.225889\n0.001548\n0.686992\n0.744404\n0.083935\n0.199393\n0.775350\n0.743736\nromantic\n1.0\n\n\n4\n12\ngiorgos papadopoulos\napopse eida oneiro\n1950\npop\ntill darling till matter know till dream live ...\n48\n0.001350\n0.001350\n0.417772\n...\n0.068800\n0.001350\n0.291671\n0.646489\n0.975904\n0.000246\n0.597073\n0.394375\nromantic\n1.0\n\n\n\n\n5 rows × 31 columns\n\n\n\n\nTo check the different genres that exist in this dataset, we can use the function .unique() to select for non-duplicate instances in the genre column.\n\ndf[\"genre\"].unique()\n\narray(['pop', 'country', 'blues', 'jazz', 'reggae', 'rock', 'hip hop'],\n      dtype=object)"
  },
  {
    "objectID": "posts/genre-deep-learning/index.html#data-processing",
    "href": "posts/genre-deep-learning/index.html#data-processing",
    "title": "Genre Deep Learning",
    "section": "",
    "text": "With this information, I can create a numbered dictionary of the genres. This dictionary will make the conversion from string to integer (needed for processing by our models) easier, as seen below. I also save the engineered features in a separate array to be used later.\n\ngenres = {\n    \"pop\" : 0,\n    \"country\" : 1,\n    \"blues\" : 2,\n    \"jazz\" : 3,\n    \"reggae\" : 4,\n    \"rock\" : 5,\n    \"hip hop\" : 6\n}\n\n\ndf[\"genre\"] = df[\"genre\"].apply(genres.get)\n\n\nengineered_features = ['dating', 'violence', 'world/life', 'night/time','shake the audience','family/gospel', 'romantic', 'communication','obscene', 'music', 'movement/places', 'light/visual perceptions','family/spiritual', 'like/girls', 'sadness', 'feelings', 'danceability','loudness', 'acousticness', 'instrumentalness', 'valence', 'energy']\n\nFor the dataset, I adapt the example from our lecture on text classification to instead index the lyrics, features, and genre.\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass TextDataFromDF(Dataset):\n    def __init__(self, df):\n        self.df = df\n\n    def __getitem__(self, index):\n        # 5 is the lyrics column, 4 is the number encoded genre\n        return self.df.iloc[index, 5], np.array(self.df.iloc[index, 7:29].values, dtype=float), self.df.iloc[index, 4]\n\n    def __len__(self):\n        return len(self.df)\n\nNext, I create training and testing dataframes using scikit-learn’s train_test_split function and convert them into torch datasets using the class I created above.\n\ndf_train, df_val = train_test_split(df, shuffle = True, test_size = 0.2)\ntrain_data = TextDataFromDF(df_train)\nval_data = TextDataFromDF(df_val)\n\nNow, I can check individual pieces of data that only hold the relevant information for the model – the potential predictors of lyrics and features alongside the target genre.\n\ntrain_data[194]\n\n('gentle touch stop world spin natural thing believe reason life suddenly look differently bring peace spell need search world know perfect suddenly live know doubt simple word convey feel',\n array([2.28832956e-03, 2.28832958e-03, 6.48654422e-01, 2.28832953e-03,\n        2.28832974e-03, 2.28832954e-03, 3.30286892e-02, 1.30058079e-01,\n        2.28832959e-03, 2.28832961e-03, 2.28832959e-03, 6.43196833e-02,\n        2.28832970e-03, 2.28832972e-03, 2.28832957e-03, 3.99656779e-02,\n        6.36087945e-01, 7.55531626e-01, 4.01605825e-01, 4.59514170e-06,\n        3.29142622e-01, 5.12497287e-01]),\n 1)\n\n\nUsing torchtext’s get_tokenizer function to split the lyrics into individual words.\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntokenizer = get_tokenizer('basic_english')\n\ntokenized = tokenizer(train_data[194][0])\ntokenized\n\n['gentle',\n 'touch',\n 'stop',\n 'world',\n 'spin',\n 'natural',\n 'thing',\n 'believe',\n 'reason',\n 'life',\n 'suddenly',\n 'look',\n 'differently',\n 'bring',\n 'peace',\n 'spell',\n 'need',\n 'search',\n 'world',\n 'know',\n 'perfect',\n 'suddenly',\n 'live',\n 'know',\n 'doubt',\n 'simple',\n 'word',\n 'convey',\n 'feel']\n\n\nTo determine a good max length for my text pipeline, I created a column in my dataframe that converted the lyrics into tokens. From there, I could check the length of each array of tokens, finding the largest in my dataset. The maximum length in the dataframe is 199, so I’ll set my max length to be 200.\n\ndf[\"lens\"] = df[\"lyrics\"].apply(tokenizer).apply(len)\ndf[\"lens\"].max()\n\n199\n\n\nI then create a vocabulary using a similar setup to the text classification lecture, but necessitating a minimum frequency of 50 for a word to be included in the vocabulary to reduce the vocabulary’s size. This creates an indexed vocabulary from all frequent, tokenized training data, using the character unknown as a default value.\n\ndef yield_tokens(data_iter):\n    for text, _, _ in data_iter:\n        yield tokenizer(text)\n\nvocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"&lt;unk&gt;\"], min_freq = 50)\nvocab.set_default_index(vocab[\"&lt;unk&gt;\"])\n\nLet’s take a look at the vocabulary using get_itos():\n\nvocab.get_itos()[0:10]\n\n['&lt;unk&gt;',\n 'know',\n 'like',\n 'time',\n 'come',\n 'go',\n 'away',\n 'heart',\n 'feel',\n 'yeah']"
  },
  {
    "objectID": "posts/genre-deep-learning/index.html#data-pipelining-and-loading",
    "href": "posts/genre-deep-learning/index.html#data-pipelining-and-loading",
    "title": "Genre Deep Learning",
    "section": "",
    "text": "Similarly, I use the lecture’s code for the text_pipeline function, changing the max length to fit this dataset. This function creates a sequence of integers from the vocabulary, ensuring that the data instance is always of size max_len (either padding or truncating as needed). Similarly, a label pipeline just ensures that the genre label is correctly typed as an integer.\n\nmax_len = 200\nnum_tokens = len(vocab.get_itos())\ndef text_pipeline(x):\n    tokens = vocab(tokenizer(x))\n    y = torch.zeros(max_len, dtype=torch.int64) + num_tokens\n    if len(tokens) &gt; max_len:\n        tokens = tokens[0:max_len]\n    y[0:len(tokens)] = torch.tensor(tokens,dtype=torch.int64)\n    return y\n\nlabel_pipeline = lambda x: int(x)\n\nCollate batch is also adapted from the lecture, adding a list for the feature predictors in the process. This function uses the pipelining tools created above to turn a batch of data into torch tensors.\n\ndef collate_batch(batch):\n    label_list, text_list, feature_list = [], [], []\n    for (_text, _features, _label) in batch:\n\n         # add label to list\n         label_list.append(label_pipeline(_label))\n\n         # add text (as sequence of integers) to list\n         processed_text = text_pipeline(_text)\n         text_list.append(processed_text)\n\n         # add features (as sequence of floats) to list\n         feature_list.append(_features)\n\n    label_list = torch.tensor(label_list, dtype=torch.int64)\n    text_list = torch.stack(text_list)\n    feature_list = torch.tensor(np.array(feature_list), dtype=torch.float64)\n    return text_list.to(device), feature_list.to(device), label_list.to(device)\n\nNow, I can create two dataloaders, one for the training data and one for the test data. When I run next(iter()) on the dataloaders, I receive a tuple with the predictor tensors (with batch_size instances in each tensor) and target tensor.\n\ntrain_loader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate_batch)\nval_loader = DataLoader(val_data, batch_size=8, shuffle=True, collate_fn=collate_batch)\nnext(iter(train_loader))\n\n(tensor([[  15,  164,   78,  ..., 2901, 2901, 2901],\n         [   0,    0,    0,  ..., 2901, 2901, 2901],\n         [   9,    0,    0,  ..., 2901, 2901, 2901],\n         ...,\n         [ 584,  584,  584,  ..., 2901, 2901, 2901],\n         [   0,    0,  311,  ..., 2901, 2901, 2901],\n         [   0,  533,   48,  ..., 2901, 2901, 2901]]),\n tensor([[4.3860e-03, 4.3860e-03, 4.3860e-03, 4.3860e-03, 4.3860e-03, 4.3860e-03,\n          2.0704e-01, 4.0865e-01, 4.3860e-03, 4.3860e-03, 4.3860e-03, 4.3860e-03,\n          4.3860e-03, 4.3860e-03, 3.1413e-01, 4.3860e-03, 6.5883e-01, 6.1159e-01,\n          7.9217e-01, 0.0000e+00, 6.4138e-01, 3.5133e-01],\n         [2.6316e-02, 2.6316e-02, 2.6316e-02, 2.6316e-02, 2.6316e-02, 2.6316e-02,\n          2.6316e-02, 2.6316e-02, 5.2632e-01, 2.6316e-02, 2.6316e-02, 2.6316e-02,\n          2.6316e-02, 2.6316e-02, 2.6316e-02, 2.6316e-02, 5.8302e-01, 4.9068e-01,\n          3.3233e-01, 7.4899e-01, 4.3219e-01, 4.2641e-01],\n         [6.0496e-04, 2.1894e-02, 2.8639e-01, 3.8409e-02, 2.5017e-02, 3.8173e-02,\n          1.3884e-02, 6.0496e-04, 3.0673e-01, 6.0496e-04, 6.0496e-04, 6.0496e-04,\n          1.6003e-01, 6.0496e-04, 7.5811e-02, 6.0496e-04, 6.1118e-01, 5.0201e-01,\n          2.8714e-02, 4.7267e-03, 6.7230e-01, 4.3141e-01],\n         [5.8480e-04, 4.8245e-02, 8.4931e-02, 5.8480e-04, 1.7697e-02, 5.8480e-04,\n          5.8480e-04, 1.8973e-01, 5.7024e-01, 2.5208e-02, 5.8480e-04, 5.8480e-04,\n          5.8480e-04, 5.8480e-04, 5.8480e-04, 2.8871e-02, 7.8772e-01, 7.0172e-01,\n          2.4497e-02, 0.0000e+00, 2.5495e-01, 6.0860e-01],\n         [4.1579e-01, 2.1778e-02, 5.7208e-04, 3.8229e-02, 1.1558e-02, 5.7208e-04,\n          1.0301e-01, 3.9343e-02, 5.7208e-04, 3.0452e-01, 5.7208e-04, 5.7208e-04,\n          5.7208e-04, 5.9471e-02, 5.7208e-04, 5.7208e-04, 3.7182e-01, 6.7938e-01,\n          3.0522e-01, 1.2247e-04, 4.7341e-01, 5.4653e-01],\n         [6.1920e-04, 6.1920e-04, 6.1920e-04, 1.1353e-01, 6.1920e-04, 6.1920e-04,\n          6.1920e-04, 6.1920e-04, 7.5624e-01, 6.1920e-04, 6.1920e-04, 6.1920e-04,\n          6.1920e-04, 6.1920e-04, 1.0648e-01, 6.1920e-04, 7.7472e-01, 5.8962e-01,\n          3.2219e-03, 7.5405e-01, 5.2597e-01, 3.0729e-01],\n         [1.2837e-03, 3.3413e-01, 2.5204e-01, 1.2837e-03, 1.2837e-03, 1.2837e-03,\n          1.2837e-03, 1.2837e-03, 3.0053e-01, 1.2837e-03, 1.2837e-03, 1.2837e-03,\n          1.2837e-03, 1.2837e-03, 1.2837e-03, 1.2837e-03, 4.6171e-01, 7.7986e-01,\n          1.3152e-02, 8.3401e-01, 4.7960e-01, 8.4084e-01],\n         [1.1198e-03, 1.1198e-03, 6.2542e-01, 1.1198e-03, 1.1198e-03, 1.1198e-03,\n          1.1198e-03, 1.1198e-03, 1.1198e-03, 2.1673e-01, 5.4501e-02, 1.1198e-03,\n          2.2755e-02, 1.1198e-03, 1.1198e-03, 4.2350e-02, 4.0864e-01, 4.1399e-01,\n          9.2671e-01, 4.9089e-04, 1.2613e-01, 1.2009e-01]], dtype=torch.float64),\n tensor([0, 3, 2, 0, 0, 4, 3, 3]))"
  },
  {
    "objectID": "posts/genre-deep-learning/index.html#training",
    "href": "posts/genre-deep-learning/index.html#training",
    "title": "Genre Deep Learning",
    "section": "",
    "text": "With the data processed, a training function can be made. This function initializes an optimizer and iterates through a batch of the data, clearing the gradients, receiving predictions from the model, computing the loss and gradient, then performing a step of the optimizer. This function also checks for the tags to use lyrics or features to determine what data to pass each model. The lyrics tensor can be indexed with data[0] while the features can be accessed with data[1].\n\nimport time\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\ndef train(model, dataloader, use_lyrics, use_features, lr=.01,):\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    epoch_start_time = time.time()\n    # keep track of some counts for measuring accuracy\n    total_acc, total_count = 0, 0\n    log_interval = 300\n    start_time = time.time()\n\n    for idx, data in enumerate(dataloader):\n        # clear gradients\n        optimizer.zero_grad()\n\n        # slice data, form prediction on batch\n        if (use_lyrics and use_features):\n          predicted_label = model(data[0], data[1])\n        elif use_lyrics:\n          predicted_label = model(data[0])\n        elif use_features:\n          predicted_label = model(data[1])\n        else:\n          print(\"Please indicate which data will be used\")\n        label = data[2]\n\n        # evaluate loss on prediction\n        loss = loss_fn(predicted_label, label)\n        # compute gradient\n        loss.backward()\n        # take an optimization step\n        optimizer.step()\n\n        # for printing accuracy\n        total_acc   += (predicted_label.argmax(1) == label).sum().item()\n        total_count += label.size(0)\n    # print(model.lin1.weight)\n    print(f'| epoch {epoch:3d} | train accuracy {total_acc/total_count:8.3f} | time: {time.time() - epoch_start_time:5.2f}s')\n\ndef evaluate(model, dataloader, use_lyrics, use_features):\n\n    total_acc, total_count = 0, 0\n\n    with torch.no_grad():\n        for idx, (text, features, label) in enumerate(dataloader):\n          if (use_lyrics and use_features):\n            predicted_label = model(text, features)\n          elif use_lyrics:\n            predicted_label = model(text)\n          elif use_features:\n            predicted_label = model(features)\n          else:\n            print(\"Please indicate which data will be used\")\n          total_acc += (predicted_label.argmax(1) == label).sum().item()\n          total_count += label.size(0)\n    return total_acc/total_count\n\nBefore training the different models, I want to check the base rate to see if my model performs better. This can be done by grouping the dataframe by genre and dividing each group’s size by the total number of data entries. The highest frequency of genre is pop with a probability of 25%.\n\ndf.groupby(\"genre\").size() / len(df)\n\ngenre\n0    0.248202\n1    0.191915\n2    0.162273\n3    0.135521\n4    0.088045\n5    0.142182\n6    0.031862\ndtype: float64"
  },
  {
    "objectID": "posts/genre-deep-learning/index.html#model-testing",
    "href": "posts/genre-deep-learning/index.html#model-testing",
    "title": "Genre Deep Learning",
    "section": "",
    "text": "The first model I tested was the lyrics-only model. This was done with an embedding layer that was then dropped out and averaged across the embedding dimensions. Finally, more fully-connected layers were added, ending by converting the dimension to the 7 possible genres.\n\nfrom torch import nn\nfrom torch.nn import ReLU\n\nclass GenreByLyrics(nn.Module):\n\n    def __init__(self, vocab_size, embedding_dim, max_len, num_class):\n        super().__init__()\n\n        self.embed_pipeline = nn.Sequential(\n          nn.Embedding(vocab_size+1, embedding_dim),\n          nn.Dropout(p=0.2)\n        )\n\n        self.linear_pipeline = nn.Sequential(\n\n          nn.Linear(embedding_dim, 512),\n          ReLU(),\n\n          nn.Linear(512, 256),\n          ReLU(),\n\n          nn.Linear(256, 32),\n          ReLU(),\n\n          nn.Linear(32, 16),\n          ReLU(),\n\n          nn.Linear(16, 8),\n          ReLU(),\n\n          nn.Linear(8, num_class)\n        )\n\n    def forward(self, x):\n      x = self.embed_pipeline(x)\n      x = x.mean(axis=1)\n\n      return(self.linear_pipeline(x))\nvocab_size = len(vocab)\nembedding_dim = 3\nlyrics_model = GenreByLyrics(vocab_size, embedding_dim, max_len, 7).to(device)\n\nIn training this model, I achieved an accuracy of 33% on the testing data. There seems to be a fair amount of overfitting, considering the ~38% training accuracy.\n\n#---\nEPOCHS = 20\nfor epoch in range(1, EPOCHS + 1):\n    train(lyrics_model, train_loader, True, False, lr=0.005)\n#---\n\n| epoch   1 | train accuracy    0.254 | time: 37.79s\n| epoch   2 | train accuracy    0.280 | time: 33.23s\n| epoch   3 | train accuracy    0.298 | time: 35.02s\n| epoch   4 | train accuracy    0.315 | time: 32.46s\n| epoch   5 | train accuracy    0.327 | time: 33.51s\n| epoch   6 | train accuracy    0.326 | time: 32.48s\n| epoch   7 | train accuracy    0.331 | time: 38.45s\n| epoch   8 | train accuracy    0.341 | time: 47.11s\n| epoch   9 | train accuracy    0.351 | time: 43.49s\n| epoch  10 | train accuracy    0.352 | time: 43.63s\n| epoch  11 | train accuracy    0.356 | time: 42.64s\n| epoch  12 | train accuracy    0.357 | time: 45.15s\n| epoch  13 | train accuracy    0.357 | time: 43.58s\n| epoch  14 | train accuracy    0.363 | time: 43.70s\n| epoch  15 | train accuracy    0.367 | time: 43.65s\n| epoch  16 | train accuracy    0.372 | time: 48.58s\n| epoch  17 | train accuracy    0.374 | time: 53.36s\n| epoch  18 | train accuracy    0.376 | time: 54.66s\n| epoch  19 | train accuracy    0.383 | time: 48.10s\n| epoch  20 | train accuracy    0.382 | time: 48.77s\n\n\n\nevaluate(lyrics_model, val_loader, True, False)\n\n0.33198237885462556\n\n\n\n\nWe can examine the embedding layer using processes from the lecture. First, we take weights from the model’s embedding pipeline:\n\nembedding_matrix = lyrics_model.embed_pipeline[0].cpu().weight.data.numpy()\nembedding_matrix\n\narray([[-9.5287114e-01,  1.8091378e-01, -3.0193642e-01],\n       [ 2.5800321e-01, -1.8974684e-01, -5.5249542e-01],\n       [-4.6847956e-03, -2.0129973e-02,  7.3867464e-01],\n       ...,\n       [-4.3154783e+00, -5.9403486e+00, -1.3008388e+00],\n       [-4.9993868e+00, -3.2404058e+00, -1.9059108e+00],\n       [ 8.8998146e-02, -4.9715936e-02,  3.2505211e-01]], dtype=float32)\n\n\nNext, we can use principal component analysis to convert the embedding matrix to 2-dimensional data. This data can then be converted to a dataframe using the tokens information.\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nweights = pca.fit_transform(embedding_matrix)\n\ntokens = vocab.get_itos()\ntokens.append(\" \")\nembedding_df = pd.DataFrame({\n    'word' : tokens,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\n\nembedding_df\n\n\n\n\n\n\n\n\n\nword\nx0\nx1\n\n\n\n\n0\n&lt;unk&gt;\n0.415862\n-0.077251\n\n\n1\nknow\n0.013589\n-0.474422\n\n\n2\nlike\n-0.676194\n0.546537\n\n\n3\ntime\n-0.382320\n-0.284132\n\n\n4\ncome\n-0.090594\n0.622675\n\n\n...\n...\n...\n...\n\n\n2897\nthighs\n4.808725\n-2.019642\n\n\n2898\ntint\n6.167381\n0.360619\n\n\n2899\nwobble\n6.274076\n3.378024\n\n\n2900\nwrath\n5.639404\n1.722761\n\n\n2901\n\n-0.474028\n0.203610\n\n\n\n\n2902 rows × 3 columns\n\n\n\n\nWe can then plot this data iteractively using plotly. Compared to the visualization in the lecture, it is harder to find distinct points for each target variable. This is likely due to the fact that this model is trained to predict 7 target variables compared to the lecture’s three. Also interestingly, reggae and country seem to be the most defined points on the plot (i.e. lying on the wider edges of the plot), lying far in the right and left respectively. There are also huge outliers like “lyric” and “commercial” that lie far to the top right of the plot. These genres extend to around 10 points on the x-axis further than most words plotted. Similarly, it’s interesting how wide of a range exists on the x-axis compared to the y-axis (around -20 to 20 versus -10 to 10).\n\nimport plotly.express as px\nimport plotly.io as pio\n\n# for VSCode plotly rendering\npio.renderers.default = \"notebook\"\n\npio.templates.default = \"plotly_white\"\n\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 10,\n                 hover_name = \"word\")\n\nfig.show()\n\n                                                \n\n\n\n\n\nNext, I trained a model on the engineered features. This was done using only fully-connected layers, including Linear layers and ReLU. This model performed better than the lyrics-only model, achieving a testing accuracy of 35%. Interestingly, this was despite a training accuracy of 34%.\n\nclass GenreByFeatures(nn.Module):\n  def __init__(self, num_class):\n    super().__init__()\n    self.pipeline = nn.Sequential(\n        nn.Linear(22, 512, dtype=torch.float64),\n        nn.ReLU(),\n\n        nn.Linear(512, 256, dtype=torch.float64),\n        nn.ReLU(),\n\n        nn.Linear(256, 128, dtype=torch.float64),\n        nn.ReLU(),\n\n        nn.Linear(128, 64, dtype=torch.float64),\n        nn.ReLU(),\n\n        nn.Linear(64, 32, dtype=torch.float64),\n        nn.ReLU(),\n\n        nn.Flatten(1),\n        nn.Linear(32, num_class, dtype=torch.float64),\n    )\n\n  def forward(self, x):\n    return self.pipeline(x)\n\n\nfeatures_model = GenreByFeatures(7).to(device)\nfor epoch in range(1, EPOCHS + 1):\n    train(features_model, train_loader, False, True, lr=0.01)\n\n| epoch   1 | train accuracy    0.259 | time: 55.45s\n| epoch   2 | train accuracy    0.310 | time: 52.04s\n| epoch   3 | train accuracy    0.319 | time: 57.36s\n| epoch   4 | train accuracy    0.324 | time: 51.28s\n| epoch   5 | train accuracy    0.327 | time: 49.28s\n| epoch   6 | train accuracy    0.328 | time: 53.08s\n| epoch   7 | train accuracy    0.331 | time: 52.06s\n| epoch   8 | train accuracy    0.331 | time: 55.44s\n| epoch   9 | train accuracy    0.336 | time: 53.19s\n| epoch  10 | train accuracy    0.330 | time: 51.03s\n| epoch  11 | train accuracy    0.336 | time: 49.87s\n| epoch  12 | train accuracy    0.338 | time: 49.01s\n| epoch  13 | train accuracy    0.298 | time: 51.94s\n| epoch  14 | train accuracy    0.290 | time: 53.90s\n| epoch  15 | train accuracy    0.318 | time: 58.61s\n| epoch  16 | train accuracy    0.328 | time: 53.22s\n| epoch  17 | train accuracy    0.322 | time: 48.93s\n| epoch  18 | train accuracy    0.329 | time: 49.14s\n| epoch  19 | train accuracy    0.336 | time: 49.22s\n| epoch  20 | train accuracy    0.342 | time: 53.90s\n\n\n\nevaluate(features_model, val_loader, False, True)\n\n0.3501321585903084\n\n\nFinally, I trained a model using both the lyrics and engineered features. This uses the first model’s embedding and dropout layers for processing the lyrics alongside a few linear and ReLU layers for the engineered features. The two tensors of data are then concatenated and run through more linear layers before returning the prediction. This model performed significantly better than the other models, achieving a training accuracy of almost 50% and a testing accuracy of 43%.\n\nclass GenreByLyricsAndFeatures(GenreByFeatures):\n  def __init__(self, vocab_size, embedding_dim, max_len, num_class):\n        super().__init__(num_class)\n\n        self.embed_pipeline = nn.Sequential(\n          nn.Embedding(vocab_size+1, embedding_dim),\n          nn.Dropout(p=0.2)\n        )\n\n        self.linear_pipeline = nn.Sequential(\n          nn.Linear(embedding_dim, 512),\n          ReLU(),\n\n          nn.Linear(512, 256),\n          ReLU(),\n\n          nn.Linear(256, 32),\n          ReLU(),\n\n          nn.Linear(32, 16),\n          ReLU(),\n\n          nn.Linear(16, 8),\n          ReLU(),\n\n          nn.Linear(8, num_class)\n        )\n\n        self.fc = nn.Linear(max_len*embedding_dim, num_class)\n        self.num_class = num_class\n\n        self.cat_pipeline = nn.Sequential(\n          nn.Linear(10, 64, dtype=torch.float64),\n          nn.ReLU(),\n\n          nn.Linear(64, 16, dtype=torch.float64),\n          nn.ReLU(),\n\n          nn.Flatten(1),\n          nn.Linear(16, self.num_class, dtype=torch.float64)\n        )\n\n  def forward(self, x1, x2):\n    # embed x1\n    x1 = self.embed_pipeline(x1)\n    x1 = x1.mean(axis=1)\n\n    # fully connected layers for x2\n    x2 = self.pipeline(x2)\n\n    # flatten each to be 2d tensors before concatenating\n    x = torch.cat((x1, x2), 1)\n\n    return self.cat_pipeline(x)\n\n\ndual_model = GenreByLyricsAndFeatures(vocab_size, embedding_dim, max_len, 7).to(device)\nfor epoch in range(1, EPOCHS + 1):\n    train(dual_model, train_loader, True, True, lr=0.001)\n\n| epoch   1 | train accuracy    0.267 | time: 54.33s\n| epoch   2 | train accuracy    0.319 | time: 44.52s\n| epoch   3 | train accuracy    0.363 | time: 44.24s\n| epoch   4 | train accuracy    0.387 | time: 46.43s\n| epoch   5 | train accuracy    0.395 | time: 44.80s\n| epoch   6 | train accuracy    0.408 | time: 45.50s\n| epoch   7 | train accuracy    0.412 | time: 44.31s\n| epoch   8 | train accuracy    0.421 | time: 47.05s\n| epoch   9 | train accuracy    0.431 | time: 44.49s\n| epoch  10 | train accuracy    0.440 | time: 43.84s\n| epoch  11 | train accuracy    0.445 | time: 44.33s\n| epoch  12 | train accuracy    0.450 | time: 46.05s\n| epoch  13 | train accuracy    0.455 | time: 44.49s\n| epoch  14 | train accuracy    0.464 | time: 44.18s\n| epoch  15 | train accuracy    0.466 | time: 44.80s\n| epoch  16 | train accuracy    0.475 | time: 45.80s\n| epoch  17 | train accuracy    0.477 | time: 44.06s\n| epoch  18 | train accuracy    0.488 | time: 44.01s\n| epoch  19 | train accuracy    0.491 | time: 44.15s\n| epoch  20 | train accuracy    0.498 | time: 46.03s\n\n\n\nevaluate(dual_model, val_loader, True, True)\n\n0.4299559471365639"
  },
  {
    "objectID": "posts/genre-deep-learning/index.html#discussion",
    "href": "posts/genre-deep-learning/index.html#discussion",
    "title": "Genre Deep Learning",
    "section": "",
    "text": "In the process of training these models, I found that the model trained with both the lyrics and features resulted in the highest accuracy. Further, I found that embeddings at higher dimensions are harder to visualize in a way that conveys consistently clear information about the target variable. In doing this blog post, I gained a lot of knowledge about data processing and loading (the creation of torch datasets, vocabularies, dataloaders, etc.) and practiced the creation of layers in different models. In particular, I learned more about model embeddings and how they can be used in tandem with fully-connected linear layers. The training process also showed me that including varied types of data (lyric versus engineered features) is extremely important in model creation."
  },
  {
    "objectID": "posts/newton-optimizer/index.html",
    "href": "posts/newton-optimizer/index.html",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "In this blog post, I modified the code from my logistic regression blog post to include an implementation of Newton’s Method for logistic regression. This involves the creation of a Hessian matrix, a computation of the second derivatives of the loss function. After this I experiment on the model, comparing it to the standard form of gradient descent optimization. These experiments include checking for α values in which the model using Newton’s method converges at a low loss value significantly faster. Additionally, I look for values of α where the model fails to converge. I then analyze the operation size of each optimization method, comparing when each method should be used.\n\n\nhttps://github.com/jblake05/jblake05.github.io/blob/main/posts/newton-optimizer/logistic.py\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom logistic import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nFirst, I create the data needed for the experiments using the classification_data function from the LinearRegression assignment. This creates linearly inseparable, noisy data (in this case in two dimensions).\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\n\n\n\n\nNext, I used the graphing functions from the linear regression assignment (adapted from the perceptron lecture). This includes functions one to plot data, draw lines for decision boundaries, plot the decision boundaries, and plot the loss values.\nThe plot_data function places two-dimensional data on a graph, using markers to dillineate data points with different target values.\n\n# from perceptron lecture\n\ndef plot_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nNext, the draw_line function plots the model’s weight across a 2D plane.\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nplot_decisions uses the previous two functions to plot the first 5 and last 5 iterations of a training loop based on the parameterized weight and loss vectors. In the process, it labels the graph with the loss the respective weight achieves.\n\n# plots the first and last 5 iterations of the decision boundary\ndef plot_decisions(X, y, weight_vec, loss_vec):\n        # set up the figure\n        current_ax = 0\n        plt.rcParams[\"figure.figsize\"] = (10, 8)\n        fig, axarr = plt.subplots(2, 5, sharex = True, sharey = True)\n\n        # iter through weight_vec\n\n        for i in range(5):\n                ax = axarr.ravel()[current_ax]\n                plot_data(X, y, ax)\n                draw_line(weight_vec[i], x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n                draw_line(weight_vec[i+1], x_min = -1, x_max = 2, ax = ax, color = \"black\")\n                ax.set_title(f\"loss = {loss_vec[i]:.3f}\")\n                ax.set(xlim = (-1, 2), ylim = (-1, 2))\n                current_ax += 1\n        \n        for i in range(len(weight_vec) - 6, len(weight_vec) - 1):\n                ax = axarr.ravel()[current_ax]\n                plot_data(X, y, ax)\n                draw_line(weight_vec[i], x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n                draw_line(weight_vec[i+1], x_min = -1, x_max = 2, ax = ax, color = \"black\")\n                ax.set_title(f\"loss = {loss_vec[i]:.3f}\")\n                ax.set(xlim = (-1, 2), ylim = (-1, 2))\n                current_ax += 1\n        fig.suptitle(\"LR Decision Boundaries over Time\")\n\nFinally, I implemented a plot_loss function to plot the loss vector changes over time.\n\ndef plot_loss(loss_vec, color=\"slategrey\", alpha=1):\n    plt.plot(loss_vec, color = color, alpha=alpha)\n    plt.scatter(torch.arange(len(loss_vec)), loss_vec, color = color, alpha=alpha)\n    labs = plt.gca().set(xlabel = \"LR Iteration (Updates Only)\", ylabel = \"loss\")\n    plt.title(\"LR Loss Iterations over Time\")\n\n\n\n\nNext, I initialized two training loops – one for the standard gradient descent optimization and one for the Newton optimizer. Each loop calculates the loss, performs an optimization step, then updates loss and weight vectors for future graphing.\n\ndef train_LR_Grad(X, y, alpha, beta, num_steps=100): \n    LR = LogisticRegression() \n    opt = GradientDescentOptimizer(LR)\n\n    # initialize for main loop\n    loss = 1\n    loss_vec = []\n    weight_vec = [torch.rand(X.size()[1])]\n\n    for _ in range(num_steps):\n        prev_loss = loss\n        loss = LR.loss(X, y)\n\n        opt.step(X, y, alpha, beta)\n\n        if (prev_loss != loss):\n            loss_vec.append(loss)\n            weight_vec.append(LR.w)\n        if (loss == 0):\n            break\n    return LR, (loss_vec, weight_vec)\n\n\ndef train_LR_Newton(X, y, alpha, num_steps=100): \n    LR = LogisticRegression() \n    opt = NewtonOptimizer(LR)\n\n    # initialize for main loop\n    loss = 1\n    loss_vec = []\n    weight_vec = [torch.rand(X.size()[1])]\n\n    for _ in range(num_steps):\n        prev_loss = loss\n        loss = LR.loss(X, y)\n\n        opt.step(X, y, alpha)\n\n        if (prev_loss != loss):\n            loss_vec.append(loss)\n            weight_vec.append(LR.w)\n        if (loss == 0):\n            break\n    return LR, (loss_vec, weight_vec)\n\n\n\n\nI first ensure that Newton’s optimization method works by training both models on the same data and parameters. To check for convergence to a similar value, I plotted the loss over time. In doing so, I see that both models converged at just below 0.2 loss.\n\nLR_grad, (loss_vec_grad, weight_vec_grad) = train_LR_Grad(X, y, 10, 0, 1000)\nplot_loss(loss_vec_grad)\n\nLR_new, (loss_vec_new, weight_vec_new) = train_LR_Newton(X, y, 10, 1000)\nplot_loss(loss_vec_new, color=\"red\", alpha=0.25)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext, I ran an experiment to see the progression of the decision boundaries over time. By plotting the boundaries, we see weight convergence toward a boundary that seems to provide a possible best possible separation line considering linearly inseparable data.\n\nLR, (loss_vec, weight_vec) = train_LR_Newton(X, y, 100, 1000)\nplot_decisions(X, y, weight_vec, loss_vec)\n\n\n\n\n\n\n\n\n\n\n\nAfter this, I tested for convergence of the Newton optimizer at a faster rate than the standard gradient descent optimizer. With an α value of 100, I found that my model converged in ~40 steps compared to the standard optimizer’s ~400 steps.\n\nLR_grad, (loss_vec_grad, weight_vec_grad) = train_LR_Grad(X, y, 10, 0, 1000)\nplot_loss(loss_vec_grad)\n\nLR, (loss_vec, weight_vec) = train_LR_Newton(X, y, 100, 1000)\nplot_loss(loss_vec, color=\"red\", alpha=0.5)\n\n\n\n\n\n\n\n\n\n\n\nFinally, I tried to find an α value where the model does not converge. This occurred when an inversion could no longer occur on the hessian matrix at an α value of 1000.\n\nLR_new, (loss_vec_new, weight_vec_new) = train_LR_Newton(X, y, 1000, 1000)\n\n_LinAlgError: linalg.inv: The diagonal element 1 is zero, the inversion could not be completed because the input matrix is singular.\n\n\nWe can see this loss increase over time by plotting its first few points. Here, the iterations that don’t have points indicate a loss value of nan. The increase of loss (alongside the above error message) implies a divergence of the model’s parameters:\n\nplot_loss(loss_vec_new[0:17])\n\n\n\n\n\n\n\n\n\n\n\n\nTo compare the two optimizers in terms of computation units, we need to make assumptions about the different steps of each optimizer. Computing the loss costs \\(c\\) units, gradient computation costs \\(2c\\), Hessian computation costs \\(pc\\) units, the inversion of a \\(p x p\\) matrix costs \\(k_1p^γ\\) units (where \\(2 \\le γ \\le 3\\)), and the Newton method’s matrix-vector multiplication costs \\(k_2p^2\\) units.\n\n\nOne call of the Newton step function includes a call to gradient, a computation of the Hessian, an inversion of the matrix, and a matrix-vector multiplication. As such:\nmodel.step = \\(2c + pc + k_2p^2 + k_1p^γ = (2+p)c + k_2p^2 + k_1p^γ\\)\nAdding the call to loss in one step of the training loop, we get:\n\\(t_0 = (3+p)c + k_2p^2 + k_1p^γ\\)\nAnd accounting for the full training loop of length \\(t_{nm}\\):\n\\(t_s = t_{nm}((3+p)c + k_2p^2 + k_1p^γ)\\)\n\n\n\nFor the gradient descent optimizer, one training step calls one instance of the gradient function and one instance of the loss function:\n\\(t_0 = 2c + c = 3c\\)\nAs such, the full training loop of length \\(t_{gd}\\) would be:\n\\(t_s = t_{gd}3c\\)\nComparing the inequalities, we can reduce to see the case where the Newton’s method requires fewer computations to complete:\n\\(t_{nm}(3c + pc + k_2p^2 + k_1p^γ) &lt; 3ct_{gd}\\)\n\n\\(t_{nm}(pc + k_2p^2 + k_1p^γ) &lt; t_{gd}\\)\nThus, when \\(t_{nm}(pc + k_2p^2 + k_1p^γ) &lt; t_{gd}\\), Newton’s method will require fewer computational units to complete compared to the standard method. When p is very high, then it becomes extremely hard for Newton’s method to require fewer computational units. If \\(p = 100\\), then the amount of computational units for one timestep will equal \\(100c + 10000k_2 + 100^γk_1\\) units, meaning \\(t_{gd}\\) will need to be units of magnitude higher for Newton’s method to be worth it computationally.\n\n\n\n\nIn the course of this blog post, I found that Newton’s optimizer can be made to converge similar to the standard gradient descent optimizer. Further, at specific α values, the Newton optimizer converges in many fewer training steps compared to the standard optimizer. However, at specific α values and very high dimensional values, Newton’s optimizer becomes extremely unusable or inefficient. In the process of this blog post, I learned about the creation of Hessian matrices (and Newton’s optimizer steps), using torch’s functions for inversion and diagonalization. I also learned about when to use specific optimizers based on dimensionality."
  },
  {
    "objectID": "posts/newton-optimizer/index.html#introduction",
    "href": "posts/newton-optimizer/index.html#introduction",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "In this blog post, I modified the code from my logistic regression blog post to include an implementation of Newton’s Method for logistic regression. This involves the creation of a Hessian matrix, a computation of the second derivatives of the loss function. After this I experiment on the model, comparing it to the standard form of gradient descent optimization. These experiments include checking for α values in which the model using Newton’s method converges at a low loss value significantly faster. Additionally, I look for values of α where the model fails to converge. I then analyze the operation size of each optimization method, comparing when each method should be used.\n\n\nhttps://github.com/jblake05/jblake05.github.io/blob/main/posts/newton-optimizer/logistic.py\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom logistic import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nFirst, I create the data needed for the experiments using the classification_data function from the LinearRegression assignment. This creates linearly inseparable, noisy data (in this case in two dimensions).\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)"
  },
  {
    "objectID": "posts/newton-optimizer/index.html#graphing-functions",
    "href": "posts/newton-optimizer/index.html#graphing-functions",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "Next, I used the graphing functions from the linear regression assignment (adapted from the perceptron lecture). This includes functions one to plot data, draw lines for decision boundaries, plot the decision boundaries, and plot the loss values.\nThe plot_data function places two-dimensional data on a graph, using markers to dillineate data points with different target values.\n\n# from perceptron lecture\n\ndef plot_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nNext, the draw_line function plots the model’s weight across a 2D plane.\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nplot_decisions uses the previous two functions to plot the first 5 and last 5 iterations of a training loop based on the parameterized weight and loss vectors. In the process, it labels the graph with the loss the respective weight achieves.\n\n# plots the first and last 5 iterations of the decision boundary\ndef plot_decisions(X, y, weight_vec, loss_vec):\n        # set up the figure\n        current_ax = 0\n        plt.rcParams[\"figure.figsize\"] = (10, 8)\n        fig, axarr = plt.subplots(2, 5, sharex = True, sharey = True)\n\n        # iter through weight_vec\n\n        for i in range(5):\n                ax = axarr.ravel()[current_ax]\n                plot_data(X, y, ax)\n                draw_line(weight_vec[i], x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n                draw_line(weight_vec[i+1], x_min = -1, x_max = 2, ax = ax, color = \"black\")\n                ax.set_title(f\"loss = {loss_vec[i]:.3f}\")\n                ax.set(xlim = (-1, 2), ylim = (-1, 2))\n                current_ax += 1\n        \n        for i in range(len(weight_vec) - 6, len(weight_vec) - 1):\n                ax = axarr.ravel()[current_ax]\n                plot_data(X, y, ax)\n                draw_line(weight_vec[i], x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n                draw_line(weight_vec[i+1], x_min = -1, x_max = 2, ax = ax, color = \"black\")\n                ax.set_title(f\"loss = {loss_vec[i]:.3f}\")\n                ax.set(xlim = (-1, 2), ylim = (-1, 2))\n                current_ax += 1\n        fig.suptitle(\"LR Decision Boundaries over Time\")\n\nFinally, I implemented a plot_loss function to plot the loss vector changes over time.\n\ndef plot_loss(loss_vec, color=\"slategrey\", alpha=1):\n    plt.plot(loss_vec, color = color, alpha=alpha)\n    plt.scatter(torch.arange(len(loss_vec)), loss_vec, color = color, alpha=alpha)\n    labs = plt.gca().set(xlabel = \"LR Iteration (Updates Only)\", ylabel = \"loss\")\n    plt.title(\"LR Loss Iterations over Time\")"
  },
  {
    "objectID": "posts/newton-optimizer/index.html#training-loops",
    "href": "posts/newton-optimizer/index.html#training-loops",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "Next, I initialized two training loops – one for the standard gradient descent optimization and one for the Newton optimizer. Each loop calculates the loss, performs an optimization step, then updates loss and weight vectors for future graphing.\n\ndef train_LR_Grad(X, y, alpha, beta, num_steps=100): \n    LR = LogisticRegression() \n    opt = GradientDescentOptimizer(LR)\n\n    # initialize for main loop\n    loss = 1\n    loss_vec = []\n    weight_vec = [torch.rand(X.size()[1])]\n\n    for _ in range(num_steps):\n        prev_loss = loss\n        loss = LR.loss(X, y)\n\n        opt.step(X, y, alpha, beta)\n\n        if (prev_loss != loss):\n            loss_vec.append(loss)\n            weight_vec.append(LR.w)\n        if (loss == 0):\n            break\n    return LR, (loss_vec, weight_vec)\n\n\ndef train_LR_Newton(X, y, alpha, num_steps=100): \n    LR = LogisticRegression() \n    opt = NewtonOptimizer(LR)\n\n    # initialize for main loop\n    loss = 1\n    loss_vec = []\n    weight_vec = [torch.rand(X.size()[1])]\n\n    for _ in range(num_steps):\n        prev_loss = loss\n        loss = LR.loss(X, y)\n\n        opt.step(X, y, alpha)\n\n        if (prev_loss != loss):\n            loss_vec.append(loss)\n            weight_vec.append(LR.w)\n        if (loss == 0):\n            break\n    return LR, (loss_vec, weight_vec)"
  },
  {
    "objectID": "posts/newton-optimizer/index.html#part-a-check-implementation",
    "href": "posts/newton-optimizer/index.html#part-a-check-implementation",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "I first ensure that Newton’s optimization method works by training both models on the same data and parameters. To check for convergence to a similar value, I plotted the loss over time. In doing so, I see that both models converged at just below 0.2 loss.\n\nLR_grad, (loss_vec_grad, weight_vec_grad) = train_LR_Grad(X, y, 10, 0, 1000)\nplot_loss(loss_vec_grad)\n\nLR_new, (loss_vec_new, weight_vec_new) = train_LR_Newton(X, y, 10, 1000)\nplot_loss(loss_vec_new, color=\"red\", alpha=0.25)"
  },
  {
    "objectID": "posts/newton-optimizer/index.html#part-b-experiments",
    "href": "posts/newton-optimizer/index.html#part-b-experiments",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "Next, I ran an experiment to see the progression of the decision boundaries over time. By plotting the boundaries, we see weight convergence toward a boundary that seems to provide a possible best possible separation line considering linearly inseparable data.\n\nLR, (loss_vec, weight_vec) = train_LR_Newton(X, y, 100, 1000)\nplot_decisions(X, y, weight_vec, loss_vec)\n\n\n\n\n\n\n\n\n\n\n\nAfter this, I tested for convergence of the Newton optimizer at a faster rate than the standard gradient descent optimizer. With an α value of 100, I found that my model converged in ~40 steps compared to the standard optimizer’s ~400 steps.\n\nLR_grad, (loss_vec_grad, weight_vec_grad) = train_LR_Grad(X, y, 10, 0, 1000)\nplot_loss(loss_vec_grad)\n\nLR, (loss_vec, weight_vec) = train_LR_Newton(X, y, 100, 1000)\nplot_loss(loss_vec, color=\"red\", alpha=0.5)\n\n\n\n\n\n\n\n\n\n\n\nFinally, I tried to find an α value where the model does not converge. This occurred when an inversion could no longer occur on the hessian matrix at an α value of 1000.\n\nLR_new, (loss_vec_new, weight_vec_new) = train_LR_Newton(X, y, 1000, 1000)\n\n_LinAlgError: linalg.inv: The diagonal element 1 is zero, the inversion could not be completed because the input matrix is singular.\n\n\nWe can see this loss increase over time by plotting its first few points. Here, the iterations that don’t have points indicate a loss value of nan. The increase of loss (alongside the above error message) implies a divergence of the model’s parameters:\n\nplot_loss(loss_vec_new[0:17])"
  },
  {
    "objectID": "posts/newton-optimizer/index.html#part-c-operation-counting",
    "href": "posts/newton-optimizer/index.html#part-c-operation-counting",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "To compare the two optimizers in terms of computation units, we need to make assumptions about the different steps of each optimizer. Computing the loss costs \\(c\\) units, gradient computation costs \\(2c\\), Hessian computation costs \\(pc\\) units, the inversion of a \\(p x p\\) matrix costs \\(k_1p^γ\\) units (where \\(2 \\le γ \\le 3\\)), and the Newton method’s matrix-vector multiplication costs \\(k_2p^2\\) units.\n\n\nOne call of the Newton step function includes a call to gradient, a computation of the Hessian, an inversion of the matrix, and a matrix-vector multiplication. As such:\nmodel.step = \\(2c + pc + k_2p^2 + k_1p^γ = (2+p)c + k_2p^2 + k_1p^γ\\)\nAdding the call to loss in one step of the training loop, we get:\n\\(t_0 = (3+p)c + k_2p^2 + k_1p^γ\\)\nAnd accounting for the full training loop of length \\(t_{nm}\\):\n\\(t_s = t_{nm}((3+p)c + k_2p^2 + k_1p^γ)\\)\n\n\n\nFor the gradient descent optimizer, one training step calls one instance of the gradient function and one instance of the loss function:\n\\(t_0 = 2c + c = 3c\\)\nAs such, the full training loop of length \\(t_{gd}\\) would be:\n\\(t_s = t_{gd}3c\\)\nComparing the inequalities, we can reduce to see the case where the Newton’s method requires fewer computations to complete:\n\\(t_{nm}(3c + pc + k_2p^2 + k_1p^γ) &lt; 3ct_{gd}\\)\n\n\\(t_{nm}(pc + k_2p^2 + k_1p^γ) &lt; t_{gd}\\)\nThus, when \\(t_{nm}(pc + k_2p^2 + k_1p^γ) &lt; t_{gd}\\), Newton’s method will require fewer computational units to complete compared to the standard method. When p is very high, then it becomes extremely hard for Newton’s method to require fewer computational units. If \\(p = 100\\), then the amount of computational units for one timestep will equal \\(100c + 10000k_2 + 100^γk_1\\) units, meaning \\(t_{gd}\\) will need to be units of magnitude higher for Newton’s method to be worth it computationally."
  },
  {
    "objectID": "posts/newton-optimizer/index.html#conclusion",
    "href": "posts/newton-optimizer/index.html#conclusion",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "In the course of this blog post, I found that Newton’s optimizer can be made to converge similar to the standard gradient descent optimizer. Further, at specific α values, the Newton optimizer converges in many fewer training steps compared to the standard optimizer. However, at specific α values and very high dimensional values, Newton’s optimizer becomes extremely unusable or inefficient. In the process of this blog post, I learned about the creation of Hessian matrices (and Newton’s optimizer steps), using torch’s functions for inversion and diagonalization. I also learned about when to use specific optimizers based on dimensionality."
  },
  {
    "objectID": "posts/penguin-post/penguin.html",
    "href": "posts/penguin-post/penguin.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "This blog post aims to explore the Palmer’s Penguins dataset. This will be done with visualizations and models, leading to predictions of species on the data. In particular, I will perform cross-validation techniques on the data to see which models and parameters yield the highest accuracy predictions. After choosing the most accurate models, I will graph the model’s decision boundaries to learn about the predictions being made."
  },
  {
    "objectID": "posts/penguin-post/penguin.html#introduction",
    "href": "posts/penguin-post/penguin.html#introduction",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "This blog post aims to explore the Palmer’s Penguins dataset. This will be done with visualizations and models, leading to predictions of species on the data. In particular, I will perform cross-validation techniques on the data to see which models and parameters yield the highest accuracy predictions. After choosing the most accurate models, I will graph the model’s decision boundaries to learn about the predictions being made."
  },
  {
    "objectID": "posts/penguin-post/penguin.html#data-collection-and-preparation",
    "href": "posts/penguin-post/penguin.html#data-collection-and-preparation",
    "title": "Classifying Palmer Penguins",
    "section": "Data Collection and Preparation",
    "text": "Data Collection and Preparation\nTo classify the Palmer Penguins, I first had to import the data set into my Python environment:\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nI then checked the elements of set. This would give me an idea of what could be used for visualization and to train the models.\n\ntrain.keys()\n\nIndex(['studyName', 'Sample Number', 'Species', 'Region', 'Island', 'Stage',\n       'Individual ID', 'Clutch Completion', 'Date Egg', 'Culmen Length (mm)',\n       'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Sex',\n       'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)', 'Comments'],\n      dtype='object')\n\n\nLet’s look at the top of the data:\n\ntrain.head()\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\nThis provided function prepares the data by dropping non-helpful elements (i.e. identifying or helpful information to scientists working with the data that isn’t useful to the model). Species values are also encoded into numerical values so they can be output by the computer. Useless values like “.” sex penguins and NA value entires are also dropped from the set. This function creates X_train and y_train data that will be used throughout the fitting process, with X_train being the valid columns of the dataframe and y_train being the species (result) column.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue"
  },
  {
    "objectID": "posts/penguin-post/penguin.html#cross-validation-testing",
    "href": "posts/penguin-post/penguin.html#cross-validation-testing",
    "title": "Classifying Palmer Penguins",
    "section": "Cross Validation Testing",
    "text": "Cross Validation Testing\nThe following code tests different models on a variety of elements in the data. The qualitative and quantitative columns are put into individual arrays and are matched in groups of three (one qualitative column and two quantitative columns) using the combinations function from itertools. The different models – logistic regression, decision tree, random forest, and support vector from sklearn’s library – are tested using cross-validation to ensure models that overfit aren’t chosen. For the support vector and decision tree classifiers, other parameters (gamma and max depth respectively) are iterated over to ensure the highest score possible. For each model fit, the columns used, score, and extra parameters are appended to their results array. These results are sorted in descending order by highest score and a truncated version of each is printed out at the end of fitting. This allows for me to choose the best model and elements to fit to predict the test data.\n\nfrom sklearn.exceptions import ConvergenceWarning\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nwarnings.filterwarnings(\"ignore\") # just ignores the convergence warning in this case\n\nresults_LR = []\nresults_SVC = []\nresults_DTC = []\nresults_RFC = []\n\nall_qual_cols = [\"Island\", \"Stage\", \"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)',\n                  'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\n    LR = LogisticRegression()\n\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    new_score_LR = cv_scores_LR.mean()\n    results_LR.append((new_score_LR, cols))\n    \n    for g in 10.0**np.arange(-5, 5):\n      SVM = SVC(gamma = g)\n      cv_scores_SVC = cross_val_score(SVM, X_train[cols], y_train, cv = 5)\n      new_score_SVC = cv_scores_SVC.mean()\n      results_SVC.append((new_score_SVC, cols, SVM.gamma))\n\n    for m in np.arange(2, 25):\n      DTC = DecisionTreeClassifier(max_depth = m)\n\n      cv_scores_DTC = cross_val_score(DTC, X_train[cols], y_train, cv = 5)\n      new_score_DTC = cv_scores_DTC.mean()\n      results_DTC.append((new_score_DTC, cols, DTC.max_depth))\n\n    RFC = RandomForestClassifier()\n\n    cv_scores_RFC = cross_val_score(RFC, X_train[cols], y_train, cv = 5)\n    new_score_RFC = cv_scores_RFC.mean()\n    results_RFC.append((new_score_RFC, cols))\n      \nprint(\"LR:\")\n# lamba key tip on tuples from https://docs.python.org/3/howto/sorting.html\nresults_LR.sort(reverse=True, key=lambda x : x[0])\nprint(results_LR[:3])\n\nprint(\"SVC:\")\nresults_SVC.sort(reverse=True, key=lambda x : x[0])\nprint(results_SVC[:3])\n\nprint(\"DTC:\")\nresults_DTC.sort(reverse=True, key=lambda x : x[0])\nprint(results_DTC[:3])\n\nprint(\"RFC:\")\nresults_RFC.sort(reverse=True, key=lambda x : x[0])\nprint(results_RFC[:3])\n\nLR:\n[(0.9961538461538462, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']), (0.9844645550527904, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']), (0.9726244343891401, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Delta 13 C (o/oo)'])]\nSVC:\n[(0.9805429864253394, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 0.1), (0.9687782805429863, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 0.1), (0.9649321266968325, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 1.0)]\nDTC:\n[(0.9765460030165913, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 5), (0.9765460030165913, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 6), (0.9765460030165913, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 10)]\nRFC:\n[(0.9843891402714933, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']), (0.9843137254901961, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Delta 13 C (o/oo)']), (0.9806184012066363, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)'])]"
  },
  {
    "objectID": "posts/penguin-post/penguin.html#visualizations",
    "href": "posts/penguin-post/penguin.html#visualizations",
    "title": "Classifying Palmer Penguins",
    "section": "Visualizations",
    "text": "Visualizations\nBefore creating the models, I wanted to create the visualizations and tables out of the data I just received. I picked the data that I didn’t plan on using for the models, but still scored highly in the model fitting. This way, the data representations will be strongly related to the species of the penguin but won’t be redundant with the final models. I also had to choose between the different quantitative factors, as I felt that isolating to one factor would clearly show its relation to species. The data table, then, was made by grouping by qualitative factors and returning a quantitative factor.\n\ngb = train.groupby([\"Island\", \"Species\"])[\"Culmen Length (mm)\"].aggregate(\"mean\")\nprint(gb)\n\nIsland     Species                                  \nBiscoe     Adelie Penguin (Pygoscelis adeliae)          38.845455\n           Gentoo penguin (Pygoscelis papua)            47.073196\nDream      Adelie Penguin (Pygoscelis adeliae)          38.826667\n           Chinstrap penguin (Pygoscelis antarctica)    48.826316\nTorgersen  Adelie Penguin (Pygoscelis adeliae)          39.229268\nName: Culmen Length (mm), dtype: float64\n\n\nFor the visualizations, I chose two quantiative factors from high scoring models, using species as a color factor. This can be achieved easily with seaborn:\n\nimport seaborn as sns\n\nsns.scatterplot(data=train, x=\"Culmen Length (mm)\", y=\"Flipper Length (mm)\", hue=\"Species\")\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data=train, x=\"Flipper Length (mm)\", y=\"Delta 13 C (o/oo)\", hue=\"Species\")"
  },
  {
    "objectID": "posts/penguin-post/penguin.html#model-creation",
    "href": "posts/penguin-post/penguin.html#model-creation",
    "title": "Classifying Palmer Penguins",
    "section": "Model Creation",
    "text": "Model Creation\nTo get the models to work, I took the highest scoring three models from the previous cross validation tests. Here, I used sklearn’s logistic regression for two of the models and their random forest classifier for the last one.\n\nfrom sklearn.linear_model import LogisticRegression\n\nLR_cols_1 = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'] # LR\nLR_cols_2 = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE'] # LR\nRFC_cols = ['Culmen Length (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'] # RFC\n\nLR_1 = LogisticRegression()\nLR_1.fit(X_train[LR_cols_1], y_train)\n\nLR_2 = LogisticRegression()\nLR_2.fit(X_train[LR_cols_2], y_train)\n\nRFC = RandomForestClassifier()\nRFC.fit(X_train[RFC_cols], y_train)\n\nFinally, the test data is prepared through the same prepare_data function. Then, the models are scored on the test data, with the logistic regression models achieving 100% accuracy!\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\n# Chose top three from above, tested on test data\n\nprint(\"LR1:\")\nprint(LR_1.score(X_test[LR_cols_1], y_test))\n\nprint(\"LR2:\")\nprint(LR_2.score(X_test[LR_cols_2], y_test))\n\nprint(\"RFC: \")\nprint(RFC.score(X_test[RFC_cols], y_test))\n\nLR1:\n1.0\nLR2:\n1.0\nRFC: \n0.9852941176470589"
  },
  {
    "objectID": "posts/penguin-post/penguin.html#decision-boundaries",
    "href": "posts/penguin-post/penguin.html#decision-boundaries",
    "title": "Classifying Palmer Penguins",
    "section": "Decision Boundaries",
    "text": "Decision Boundaries\nThis provided plot_regions function takes in a model alongside the X and y columns of data and plots the its decision regions, allowing for easy visualization.\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nFinally, I plotted my two logistic regression models (the models that achieved 100% prediction accuracy) using the plot_regions function.\n\nplot_regions(LR_1, X_train[LR_cols_1], y_train)\nplot_regions(LR_2, X_train[LR_cols_2], y_train)"
  },
  {
    "objectID": "posts/penguin-post/penguin.html#confusion-matrix",
    "href": "posts/penguin-post/penguin.html#confusion-matrix",
    "title": "Classifying Palmer Penguins",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nAs one more point of information, I created confusion matrices for my LogisticRegression models. In doing so, I showed that each of these models predicts a given species as the correct species across all test data (e.g. there is no instance across either model where an Gentoo is misclassified as a Chinstrap).\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred_1 = LR_1.predict(X_test[LR_cols_1])\nC_1 = confusion_matrix(y_test, y_test_pred_1)\n\ny_test_pred_2 = LR_2.predict(X_test[LR_cols_2])\nC_2 = confusion_matrix(y_test, y_test_pred_2)\n\nC_1, C_2\n\n(array([[31,  0,  0],\n        [ 0, 11,  0],\n        [ 0,  0, 26]], dtype=int64),\n array([[31,  0,  0],\n        [ 0, 11,  0],\n        [ 0,  0, 26]], dtype=int64))"
  },
  {
    "objectID": "posts/penguin-post/penguin.html#discussion",
    "href": "posts/penguin-post/penguin.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nThrough the creation of this blog post, I discovered the best sets of prediction parameters for the given dataset and models (culmen length and depth alongside island of origin and sex, respectively) and showed that logistic regression decision boundaries are linear. Further, I showed other correlations that exist in the dataset – like the relation of flipper length and delta 13 C to species – through visualizations of the dataset. In doing this, I improved my skills with dataframes and data cleaning, an essential process for training models. Further, I gained practice with seaborn for visualizing data and learned how to train and cross-validate the prebuilt models from scikit-learn, leading to my first model-based predictions being made on a dataset."
  }
]