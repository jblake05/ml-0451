[
  {
    "objectID": "posts/penguin-post/penguin.html",
    "href": "posts/penguin-post/penguin.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "To classify the Palmer Penguins, I first had to import the data set into my Python environment:\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nI then checked the elements of set. This would give me an idea of what could be used for visualization and to train the models.\n\ntrain.keys()\n\nIndex(['studyName', 'Sample Number', 'Species', 'Region', 'Island', 'Stage',\n       'Individual ID', 'Clutch Completion', 'Date Egg', 'Culmen Length (mm)',\n       'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Sex',\n       'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)', 'Comments'],\n      dtype='object')\n\n\nLet’s look at the top of the data:\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nThis provided function prepares the data by dropping non-helpful elements (i.e. identifying or helpful information to scientists working with the data that isn’t useful to the model). Species values are also encoded into numerical values so they can be output by the computer. Useless values like “.” sex penguins and NA value entires are also dropped from the set. This function creates X_train and y_train data that will be used throughout the fitting process, with X_train being the valid columns of the dataframe and y_train being the species (result) column.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nThe following code tests different models on a variety of elements in the data. The qualitative and quantitative columns are put into individual arrays and are matched in groups of three (one qualitative column and two quantitative columns) using the combinations function from itertools. The different models – logistic regression, decision tree, random forest, and support vector from sklearn’s library – are tested using cross-validation to ensure models that overfit aren’t chosen. For the support vector and decision tree classifiers, other parameters (gamma and max depth respectively) are iterated over to ensure the highest score possible. For each model fit, the columns used, score, and extra parameters are appended to their results array. These results are sorted in descending order by highest score and a truncated version of each is printed out at the end of fitting. This allows for me to choose the best model and elements to fit to predict the test data.\n\nfrom sklearn.exceptions import ConvergenceWarning\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nwarnings.filterwarnings(\"ignore\") # just ignores the convergence warning in this case\n\nresults_LR = []\nresults_SVC = []\nresults_DTC = []\nresults_RFC = []\n\nall_qual_cols = [\"Island\", \"Stage\", \"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)',\n                  'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\n    LR = LogisticRegression()\n\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    new_score_LR = cv_scores_LR.mean()\n    results_LR.append((new_score_LR, cols))\n    \n    for g in 10.0**np.arange(-5, 5):\n      SVM = SVC(gamma = g)\n      cv_scores_SVC = cross_val_score(SVM, X_train[cols], y_train, cv = 5)\n      new_score_SVC = cv_scores_SVC.mean()\n      results_SVC.append((new_score_SVC, cols, SVM.gamma))\n\n    for m in np.arange(2, 25):\n      DTC = DecisionTreeClassifier(max_depth = m)\n\n      cv_scores_DTC = cross_val_score(DTC, X_train[cols], y_train, cv = 5)\n      new_score_DTC = cv_scores_DTC.mean()\n      results_DTC.append((new_score_DTC, cols, DTC.max_depth))\n\n    RFC = RandomForestClassifier()\n\n    cv_scores_RFC = cross_val_score(RFC, X_train[cols], y_train, cv = 5)\n    new_score_RFC = cv_scores_RFC.mean()\n    results_RFC.append((new_score_RFC, cols))\n      \nprint(\"LR:\")\n# lamba key tip on tuples from https://docs.python.org/3/howto/sorting.html\nresults_LR.sort(reverse=True, key=lambda x : x[0])\nprint(results_LR[:3])\n\nprint(\"SVC:\")\nresults_SVC.sort(reverse=True, key=lambda x : x[0])\nprint(results_SVC[:3])\n\nprint(\"DTC:\")\nresults_DTC.sort(reverse=True, key=lambda x : x[0])\nprint(results_DTC[:3])\n\nprint(\"RFC:\")\nresults_RFC.sort(reverse=True, key=lambda x : x[0])\nprint(results_RFC[:3])\n\nLR:\n[(0.9922322775263952, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']), (0.9844645550527904, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']), (0.9726244343891401, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Delta 13 C (o/oo)'])]\nSVC:\n[(0.9805429864253394, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 0.1), (0.9687782805429863, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 0.1), (0.9649321266968325, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 1.0)]\nDTC:\n[(0.9765460030165913, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 8), (0.9765460030165913, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 9), (0.9765460030165913, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 10)]\nRFC:\n[(0.9883107088989442, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']), (0.9843891402714933, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']), (0.9806184012066363, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)'])]\n\n\nBefore creating the models, I wanted to create the visualizations and tables out of the data I just received. I picked the data that I didn’t plan on using for the models, but still scored highly in the model fitting. This way, the data representations will be strongly related to the species of the penguin but won’t be redundant with the final models. I also had to choose between the different quantitative factors, as I felt that isolating to one factor would clearly show its relation to species. The data table, then, was made by grouping by qualitative factors and returning a quantitative factor.\n\ngb = train.groupby([\"Island\", \"Species\"])[\"Culmen Length (mm)\"].aggregate(\"mean\")\nprint(gb)\n\nIsland     Species                                  \nBiscoe     Adelie Penguin (Pygoscelis adeliae)          38.845455\n           Gentoo penguin (Pygoscelis papua)            47.073196\nDream      Adelie Penguin (Pygoscelis adeliae)          38.826667\n           Chinstrap penguin (Pygoscelis antarctica)    48.826316\nTorgersen  Adelie Penguin (Pygoscelis adeliae)          39.229268\nName: Culmen Length (mm), dtype: float64\n\n\nFor the visualizations, I chose two quantiative factors from high scoring models, using species as a color factor. This can be achieved easily with seaborn:\n\nimport seaborn as sns\n\nsns.scatterplot(data=train, x=\"Culmen Length (mm)\", y=\"Flipper Length (mm)\", hue=\"Species\")\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data=train, x=\"Flipper Length (mm)\", y=\"Delta 13 C (o/oo)\", hue=\"Species\")\n\n\n\n\n\n\n\n\nTo get the models to work, I took the highest scoring three models from the previous cross validation tests. Here, I used sklearn’s logistic regression for two of the models and their random forest classifier for the last one.\n\nfrom sklearn.linear_model import LogisticRegression\n\nLR_cols_1 = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'] # LR\nLR_cols_2 = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE'] # LR\nRFC_cols = ['Culmen Length (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'] # RFC\n\nLR_1 = LogisticRegression()\nLR_1.fit(X_train[LR_cols_1], y_train)\n\nLR_2 = LogisticRegression()\nLR_2.fit(X_train[LR_cols_2], y_train)\n\nRFC = RandomForestClassifier()\nRFC.fit(X_train[RFC_cols], y_train)\n\nFinally, the test data is prepared through the same prepare_data function. Then, the models are scored on the test data, with the logistic regression models achieving 100% accuracy!\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\n# Chose top three from above, tested on test data\n\nprint(\"LR1:\")\nprint(LR_1.score(X_test[LR_cols_1], y_test))\n\nprint(\"LR2:\")\nprint(LR_2.score(X_test[LR_cols_2], y_test))\n\nprint(\"RFC: \")\nprint(RFC.score(X_test[RFC_cols], y_test))\n\nLR1:\n1.0\nLR2:\n1.0\nRFC: \n0.9852941176470589\n\n\nThis provided plot_regions function takes in a model alongside the X and y columns of data and plots the its decision regions, allowing for easy visualization.\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nFinally, I plotted my two logistic regression models (the models that achieved 100% prediction accuracy) using the plot_regions function.\n\n\nplot_regions(LR_1, X_train[LR_cols_1], y_train)\nplot_regions(LR_2, X_train[LR_cols_2], y_train)"
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Bias Replication Study\n\n\n\n\n\nReplicating a study on the biases of predictive healthcare models\n\n\n\n\n\nApr 15, 2024\n\n\nJeff Blake\n\n\n\n\n\n\n\n\n\n\n\n\n\"Optimal\" Decision Making\n\n\n\n\n\nMaking “optimal” decisions based on loan-default data and model training. The definition of optimal, though, becomes flawed with profit incentives.\n\n\n\n\n\nMar 27, 2024\n\n\nJeff Blake\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nHow I made models that could predict the species from the Palmer Penguins dataset.\n\n\n\n\n\nFeb 28, 2023\n\n\nJeff Blake\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Testing quarto!"
  },
  {
    "objectID": "posts/bias-replication/bias_replication.html",
    "href": "posts/bias-replication/bias_replication.html",
    "title": "Bias Replication Study",
    "section": "",
    "text": "This blog post aims to replicate the study by Obermeyer et al. on the racial bias that exists in healthcare systems. In doing so, I compare the risk percentile assigned to black patients and white patients alongside their number of chronic illnesses. This recreation found that black patients were given lower risk scores even if they had the same number of chronic illnesses as white patients. I also compared medical expenditure with percentile risk score and number of chronic illnesses separately. This showed that, while costs were around the same by race when looking at different risk percentages, there was less expenditures for black patients when compared to white patients on the axis of chronic illnesses. This is further proven through the modelling done on the logarithmic cost, which estimates an expenditure for black patients at 75% of white patients. If care is supplied based on cost, then, we can see how unfair outcomes by race can arise.\nFirst, I imported the dataset into a dataframe with pandas.\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\nLet’s look at the data’s columns.\n\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\nThen, I created a risk percentile out of the data’s risk score by using pandas’ qcut feature. I grouped the data by this percentile and race, then looked observed the mean number of chronic illnesses per group.\n\nimport numpy as np\n\ndf[\"risk_percentile\"] = pd.qcut(df[\"risk_score_t\"], 101, labels=False)\n\ndf[\"risk_percentile\"]\n\ngb = df.groupby([\"risk_percentile\", \"race\"])[\"gagne_sum_t\"].mean().unstack(level=1)\ngb\n\n\n\n\n\n\n\nrace\nblack\nwhite\n\n\nrisk_percentile\n\n\n\n\n\n\n0\n0.069444\n0.051724\n\n\n1\n0.240741\n0.149847\n\n\n2\n0.277778\n0.105495\n\n\n3\n0.266667\n0.073810\n\n\n4\n0.200000\n0.097514\n\n\n...\n...\n...\n\n\n96\n5.460526\n3.809877\n\n\n97\n4.882353\n4.137681\n\n\n98\n5.761194\n4.821853\n\n\n99\n6.135802\n4.891688\n\n\n100\n7.319149\n6.038012\n\n\n\n\n101 rows × 2 columns\n\n\n\nNext, I graphed the risk percentile and mean number of chronic conditions grouped by race, recreating figure 1 of the study.\n\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nplt.style.use('seaborn-v0_8-whitegrid')\n\nmarkers = [\"o\" , \",\"]\n\ncolumns = [\"black\", \"white\"]\n\nfig, ax = plt.subplots(1, 1)\n\nfor i in range(2):\n    to_plot = gb[columns[i]]\n    ax.scatter(to_plot.values, np.arange(0, 101, 1), c = f\"{['blue', 'orange'][i]}\", vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['black', 'white'][i]}\", cmap = \"BrBG\", marker = markers[i])\n    ax.legend()\n    ax.set(xlabel=\"Mean Number of Chronic Conditions\", ylabel = \"Percentile Risk Score\")\n\n\n\n\n\n\n\n\nThis graph shows that, if two patients have the same number of chronic illnesses but one is white and one is black, the white patient is much more likely to have a higher risk score. This, in turn, makes them more likely to be reccomended to the high-risk care management program when compared to a black patient with the same number of chronic conditions.\nAfter this, I used the cost column alongside the number of chronic illnesses and the previously created risk percentile to make two more graphs. The first graph shows the mean cost compared to the risk percentile grouped by race. The second compares the cost and the number of chronic illnesses, also grouped by race.\n\ngb_cost = df.groupby([\"risk_percentile\", \"race\"])[\"cost_t\"].mean().unstack(level=1)\nfig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n\nplt.yscale(\"log\")\n\ngb_chronic = df.groupby([\"gagne_sum_t\", \"race\"])[\"cost_t\"].mean().unstack(level=1)\n\nfor i in range(2):    \n    to_plot = gb_cost[columns[i]]\n    ax1.scatter(np.arange(0, 101, 1), to_plot.values, c = f\"{['blue', 'orange'][i]}\", vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['black', 'white'][i]}\", cmap = \"BrBG\", marker = markers[i])\n    ax1.legend()\n    ax1.set(xlabel = \"Percentile Risk Score\", ylabel=\"Mean Medical Expenditure\")\n\n    to_plot = gb_chronic[columns[i]]\n    ax2.scatter(np.arange(0, 18, 1), to_plot.values, c = f\"{['blue', 'orange'][i]}\", vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['black', 'white'][i]}\", cmap = \"BrBG\", marker = markers[i])\n    ax2.legend()\n    ax2.set(xlabel = \"Number of Chronic Illnesses\")\n\n\n\n\n\n\n\n\nThis graph shows that, while there isn’t a cost disparity across race when comparing againsst percentile risk score, there is one when looking at the number of chronic illnesses. If we look at the datapoints at and below 5 chronic illnesses, where most patients lie, there is a lower average cost per black patients when compared to white patients. Considering the risk assessment model is done according to cost rather than illness, one can see how black patients would be disproportionately denied access to the program.\nFinally, we can model this cost disparity by looking at the data. This modelling experiment is informed on the equation:\n\\(logcost \\approx w_b\\) \\(*\\) (patient is black) \\(+\\) intercept \\(+\\) $_{i=1} ^{k} w_k * \\((gagne sum)\\)^k$\nWhere \\(w_b\\) is the weight a model gives to the variable that a patient is black and \\(w_k\\) are the weights given to the polynomial data of the number of chronic illnesses.\nFrom this we also find that \\(e^{w_b}\\) is the percentage of average cost incurred by black patients compared to white patients\nTo start, we observe that 95% of the patients have 5 or less chronic illnesses, making it a good subset of the data to generalize over.\n\ndf[df[\"gagne_sum_t\"] &lt;= 5][\"gagne_sum_t\"].count()/df.shape[0]\n\n0.9553952115447688\n\n\nNext, we can create a new dataframe eithin that subset that creates a log-transform of the cost (ignoring any 0-cost patients). This allows us to look at data that spans orders of magnitude. Further, for the model to work, we must encode race as a number. In this case, 0 is coded to be white and 1 black.\n\ndf_log = df.query('gagne_sum_t &lt;= 5 & cost_t != 0')\ndf_log[\"log_cost\"] = np.log(df_log[\"cost_t\"])\n\ndf_log[\"race_num\"] = 1 * (df_log[\"race\"] == \"black\")\n\nThen we can take predictor and target columns of this data, where race and number of chronic illness are the predictors (X) and the log cost is the target (y).\n\npred_X = df_log[[\"race_num\", \"gagne_sum_t\"]]\ntarget_y = df_log[\"log_cost\"]\n\n\ndf_log\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\nrisk_percentile\nlog_cost\nrace_num\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n35\n7.090077\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n4\n3\n86\n7.863267\n0\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n3\n6.214608\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n11\n7.170120\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n1\n1\n98\n7.003065\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48779\n0.611517\n0\n800.0\n0.0\nNaN\nNaN\nNaN\n1.090000\n148.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n7\n6.684612\n0\n\n\n48780\n2.615933\n0\n2200.0\n0.0\n112.0\nNaN\n41.4\n0.810000\n172.0\nwhite\n...\n0\n1\n0\n0\n1\n1\n1\n46\n7.696213\n0\n\n\n48781\n1.358926\n0\n800.0\n0.0\n105.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n1\n0\n0\n1\n1\n0\n23\n6.684612\n0\n\n\n48782\n10.990318\n0\n1300.0\n0.0\n132.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n3\n3\n94\n7.170120\n0\n\n\n48783\n1.681671\n0\n4400.0\n0.0\n115.0\n5.6\n36.6\n0.940000\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n29\n8.389360\n0\n\n\n\n\n44748 rows × 163 columns\n\n\n\nThe function add_polynomial_features returns a dataset with exponential values of the number of chronic illnesses. This is useful for training models that act on non-linear data.\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nPerforming cross-validation on the degrees of the polynomial finds the most accurate to be a degree of 6.\n\nfrom sklearn.linear_model import LinearRegression\n\nrecord = (0, 0)\n\nfor degree in range (1, 15):\n    poly_X = add_polynomial_features(pred_X, degree)\n\n    LinR = LinearRegression()\n    LinR.fit(poly_X, target_y)\n    tempScore = LinR.score(poly_X, target_y)\n    # print(degree, tempScore)\n    if tempScore &gt; record[1]:\n        record = (degree, tempScore)\nrecord\n\n(6, 0.08800788381979874)\n\n\nIf we fit the model to this polynomial dataset and the target variable, we find can find the value of wb by looking at the first coefficient of our linear regression model.\n\npoly_X = add_polynomial_features(pred_X, 6)\nLinR = LinearRegression()\nLinR.fit(poly_X, target_y)\n# LinR.score(poly_X, target_y)\nLinR.coef_[0]\n\n-0.2827181024769897\n\n\nFinally, we can get an percentage of average cost paid by black patients when compared to white patients by calculating \\(e^{w_b}\\).\n\npercent = np.exp(LinR.coef_[0])\npercent\n\n0.7537322331639498\n\n\nThis percentage of around 75% makes sense with regard to the study. Specifically, the study notes that “Blacks generate lower costs than Whites – on average, $1801 less per year…” “… or $1144 less, if we instead hold constant the specific individual illnesses that contribute to the sum.” This roughly lines up with the model’s prediction that black people will have a 75% lower mean total medical expenditure compared to white people."
  },
  {
    "objectID": "posts/bias-replication/bias_replication.html#abstract",
    "href": "posts/bias-replication/bias_replication.html#abstract",
    "title": "Bias Replication Study",
    "section": "",
    "text": "This blog post aims to replicate the study by Obermeyer et al. on the racial bias that exists in healthcare systems. In doing so, I compare the risk percentile assigned to black patients and white patients alongside their number of chronic illnesses. This recreation found that black patients were given lower risk scores even if they had the same number of chronic illnesses as white patients. I also compared medical expenditure with percentile risk score and number of chronic illnesses separately. This showed that, while costs were around the same by race when looking at different risk percentages, there was less expenditures for black patients when compared to white patients on the axis of chronic illnesses. This is further proven through the modelling done on the logarithmic cost, which estimates an expenditure for black patients at 75% of white patients. If care is supplied based on cost, then, we can see how unfair outcomes by race can arise.\nFirst, I imported the dataset into a dataframe with pandas.\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\nLet’s look at the data’s columns.\n\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\nThen, I created a risk percentile out of the data’s risk score by using pandas’ qcut feature. I grouped the data by this percentile and race, then looked observed the mean number of chronic illnesses per group.\n\nimport numpy as np\n\ndf[\"risk_percentile\"] = pd.qcut(df[\"risk_score_t\"], 101, labels=False)\n\ndf[\"risk_percentile\"]\n\ngb = df.groupby([\"risk_percentile\", \"race\"])[\"gagne_sum_t\"].mean().unstack(level=1)\ngb\n\n\n\n\n\n\n\nrace\nblack\nwhite\n\n\nrisk_percentile\n\n\n\n\n\n\n0\n0.069444\n0.051724\n\n\n1\n0.240741\n0.149847\n\n\n2\n0.277778\n0.105495\n\n\n3\n0.266667\n0.073810\n\n\n4\n0.200000\n0.097514\n\n\n...\n...\n...\n\n\n96\n5.460526\n3.809877\n\n\n97\n4.882353\n4.137681\n\n\n98\n5.761194\n4.821853\n\n\n99\n6.135802\n4.891688\n\n\n100\n7.319149\n6.038012\n\n\n\n\n101 rows × 2 columns\n\n\n\nNext, I graphed the risk percentile and mean number of chronic conditions grouped by race, recreating figure 1 of the study.\n\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nplt.style.use('seaborn-v0_8-whitegrid')\n\nmarkers = [\"o\" , \",\"]\n\ncolumns = [\"black\", \"white\"]\n\nfig, ax = plt.subplots(1, 1)\n\nfor i in range(2):\n    to_plot = gb[columns[i]]\n    ax.scatter(to_plot.values, np.arange(0, 101, 1), c = f\"{['blue', 'orange'][i]}\", vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['black', 'white'][i]}\", cmap = \"BrBG\", marker = markers[i])\n    ax.legend()\n    ax.set(xlabel=\"Mean Number of Chronic Conditions\", ylabel = \"Percentile Risk Score\")\n\n\n\n\n\n\n\n\nThis graph shows that, if two patients have the same number of chronic illnesses but one is white and one is black, the white patient is much more likely to have a higher risk score. This, in turn, makes them more likely to be reccomended to the high-risk care management program when compared to a black patient with the same number of chronic conditions.\nAfter this, I used the cost column alongside the number of chronic illnesses and the previously created risk percentile to make two more graphs. The first graph shows the mean cost compared to the risk percentile grouped by race. The second compares the cost and the number of chronic illnesses, also grouped by race.\n\ngb_cost = df.groupby([\"risk_percentile\", \"race\"])[\"cost_t\"].mean().unstack(level=1)\nfig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n\nplt.yscale(\"log\")\n\ngb_chronic = df.groupby([\"gagne_sum_t\", \"race\"])[\"cost_t\"].mean().unstack(level=1)\n\nfor i in range(2):    \n    to_plot = gb_cost[columns[i]]\n    ax1.scatter(np.arange(0, 101, 1), to_plot.values, c = f\"{['blue', 'orange'][i]}\", vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['black', 'white'][i]}\", cmap = \"BrBG\", marker = markers[i])\n    ax1.legend()\n    ax1.set(xlabel = \"Percentile Risk Score\", ylabel=\"Mean Medical Expenditure\")\n\n    to_plot = gb_chronic[columns[i]]\n    ax2.scatter(np.arange(0, 18, 1), to_plot.values, c = f\"{['blue', 'orange'][i]}\", vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['black', 'white'][i]}\", cmap = \"BrBG\", marker = markers[i])\n    ax2.legend()\n    ax2.set(xlabel = \"Number of Chronic Illnesses\")\n\n\n\n\n\n\n\n\nThis graph shows that, while there isn’t a cost disparity across race when comparing againsst percentile risk score, there is one when looking at the number of chronic illnesses. If we look at the datapoints at and below 5 chronic illnesses, where most patients lie, there is a lower average cost per black patients when compared to white patients. Considering the risk assessment model is done according to cost rather than illness, one can see how black patients would be disproportionately denied access to the program.\nFinally, we can model this cost disparity by looking at the data. This modelling experiment is informed on the equation:\n\\(logcost \\approx w_b\\) \\(*\\) (patient is black) \\(+\\) intercept \\(+\\) $_{i=1} ^{k} w_k * \\((gagne sum)\\)^k$\nWhere \\(w_b\\) is the weight a model gives to the variable that a patient is black and \\(w_k\\) are the weights given to the polynomial data of the number of chronic illnesses.\nFrom this we also find that \\(e^{w_b}\\) is the percentage of average cost incurred by black patients compared to white patients\nTo start, we observe that 95% of the patients have 5 or less chronic illnesses, making it a good subset of the data to generalize over.\n\ndf[df[\"gagne_sum_t\"] &lt;= 5][\"gagne_sum_t\"].count()/df.shape[0]\n\n0.9553952115447688\n\n\nNext, we can create a new dataframe eithin that subset that creates a log-transform of the cost (ignoring any 0-cost patients). This allows us to look at data that spans orders of magnitude. Further, for the model to work, we must encode race as a number. In this case, 0 is coded to be white and 1 black.\n\ndf_log = df.query('gagne_sum_t &lt;= 5 & cost_t != 0')\ndf_log[\"log_cost\"] = np.log(df_log[\"cost_t\"])\n\ndf_log[\"race_num\"] = 1 * (df_log[\"race\"] == \"black\")\n\nThen we can take predictor and target columns of this data, where race and number of chronic illness are the predictors (X) and the log cost is the target (y).\n\npred_X = df_log[[\"race_num\", \"gagne_sum_t\"]]\ntarget_y = df_log[\"log_cost\"]\n\n\ndf_log\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\nrisk_percentile\nlog_cost\nrace_num\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n35\n7.090077\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n4\n3\n86\n7.863267\n0\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n3\n6.214608\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n11\n7.170120\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n1\n1\n98\n7.003065\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48779\n0.611517\n0\n800.0\n0.0\nNaN\nNaN\nNaN\n1.090000\n148.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n7\n6.684612\n0\n\n\n48780\n2.615933\n0\n2200.0\n0.0\n112.0\nNaN\n41.4\n0.810000\n172.0\nwhite\n...\n0\n1\n0\n0\n1\n1\n1\n46\n7.696213\n0\n\n\n48781\n1.358926\n0\n800.0\n0.0\n105.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n1\n0\n0\n1\n1\n0\n23\n6.684612\n0\n\n\n48782\n10.990318\n0\n1300.0\n0.0\n132.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n3\n3\n94\n7.170120\n0\n\n\n48783\n1.681671\n0\n4400.0\n0.0\n115.0\n5.6\n36.6\n0.940000\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n29\n8.389360\n0\n\n\n\n\n44748 rows × 163 columns\n\n\n\nThe function add_polynomial_features returns a dataset with exponential values of the number of chronic illnesses. This is useful for training models that act on non-linear data.\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nPerforming cross-validation on the degrees of the polynomial finds the most accurate to be a degree of 6.\n\nfrom sklearn.linear_model import LinearRegression\n\nrecord = (0, 0)\n\nfor degree in range (1, 15):\n    poly_X = add_polynomial_features(pred_X, degree)\n\n    LinR = LinearRegression()\n    LinR.fit(poly_X, target_y)\n    tempScore = LinR.score(poly_X, target_y)\n    # print(degree, tempScore)\n    if tempScore &gt; record[1]:\n        record = (degree, tempScore)\nrecord\n\n(6, 0.08800788381979874)\n\n\nIf we fit the model to this polynomial dataset and the target variable, we find can find the value of wb by looking at the first coefficient of our linear regression model.\n\npoly_X = add_polynomial_features(pred_X, 6)\nLinR = LinearRegression()\nLinR.fit(poly_X, target_y)\n# LinR.score(poly_X, target_y)\nLinR.coef_[0]\n\n-0.2827181024769897\n\n\nFinally, we can get an percentage of average cost paid by black patients when compared to white patients by calculating \\(e^{w_b}\\).\n\npercent = np.exp(LinR.coef_[0])\npercent\n\n0.7537322331639498\n\n\nThis percentage of around 75% makes sense with regard to the study. Specifically, the study notes that “Blacks generate lower costs than Whites – on average, $1801 less per year…” “… or $1144 less, if we instead hold constant the specific individual illnesses that contribute to the sum.” This roughly lines up with the model’s prediction that black people will have a 75% lower mean total medical expenditure compared to white people."
  },
  {
    "objectID": "posts/bias-replication/bias_replication.html#discussion",
    "href": "posts/bias-replication/bias_replication.html#discussion",
    "title": "Bias Replication Study",
    "section": "Discussion",
    "text": "Discussion\nIn the process of replicating this study, I confirmed the findings that black patients are more likely to be placed in lower risk percentiles given equal prevalence of chronic illness when compared to white patients. I also confirmed that, while costs are relatively similar between patients of the same risk percentile and different races, costs are disparate (black patients have less cost) when comparing between chronic illness. This was then quantified through a model, with a prediction of black patients having 75% of the cost of white patients. With the risk score being calculated with respect to cost, then, disparate outcomes across races are seen considering a more realistic risk variable like chronic illness count.\nAs such, the discrimination criteria of calibration bias is present in the formation of the risk score. This is seen through the dissection of risk score when compared against chronic illness versus cost. While there isn’t much bias in calculating risk score across race when cost is considering factor, there is bias when considering risk score in relation to number of chronic illness. That is to say, those with similar risk scores will have similar costs across race (the second graph in the blog post and figure 3A in the study), but will have dissimilar numbers of chronic illnesses, with black patients having more chronic illnesses at similar risk scores on average (the first graph and figure 1 in the study). The model is then calibrated to the variable of cost rather than health outcome, a fact that is mentioned at the beginning of the study: “…assessing how well the algorithmic risk score is calibrated across race for health outcomes \\(H_{i,t}\\). We also ask how well the algorithm is calibrated for costs \\(C_{i,t}\\).”\nThrough this blog post, I improved my graphing skills, especially when needing to graph different groups on the same graph. I also improved my ability in modelling with polynomial datasets and transforming data to be more useful to my models and graphs (log-transforms, qcuts, etc.)."
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html",
    "href": "posts/optimal-decision-making/optimal.html",
    "title": "\"Optimal\" Decision Making",
    "section": "",
    "text": "This blog post will explore the data behind loan granting from various perspectives. The data will first be shown “as is” through visualizations and data tables. Then, a model will be trained on the data through the creation of a scoring function and threshold. Finally, more in-depth quantiative analysis will be done on the profits, costs, and biases that are created from the model. Through this study, a profitable model for the bank was found, but this came at the cost of differing acceptance rates across groups."
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#introduction",
    "href": "posts/optimal-decision-making/optimal.html#introduction",
    "title": "\"Optimal\" Decision Making",
    "section": "",
    "text": "This blog post will explore the data behind loan granting from various perspectives. The data will first be shown “as is” through visualizations and data tables. Then, a model will be trained on the data through the creation of a scoring function and threshold. Finally, more in-depth quantiative analysis will be done on the profits, costs, and biases that are created from the model. Through this study, a profitable model for the bank was found, but this came at the cost of differing acceptance rates across groups."
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#model-building",
    "href": "posts/optimal-decision-making/optimal.html#model-building",
    "title": "\"Optimal\" Decision Making",
    "section": "Model Building",
    "text": "Model Building\nFirst, the data is converted to a dataframe through pandas and checked using the head() function\n\nimport pandas as pd\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nimport warnings\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\nI first investigated the relationship between personal income, financial history, and loan amounts. I did this by creating a table that groups by income intervals of 10000 and checking credit history and the size of the loan. It shows that loan amounts granted increase as personal income increases, while one’s credit history tends to be higher as income increases, but this isn’t as drastic of an upward trend.\n\ndf_train[\"income_group\"] = df_train[\"person_income\"] // 10000\n\ndf_train.groupby([\"income_group\"])[[\"loan_amnt\", \"cb_person_cred_hist_length\"]].aggregate(\"mean\")[:20]\n\n\n\n\n\n\n\n\nloan_amnt\ncb_person_cred_hist_length\n\n\nincome_group\n\n\n\n\n\n\n0\n2093.333333\n5.566667\n\n\n1\n3837.235367\n5.282690\n\n\n2\n5731.833494\n5.532243\n\n\n3\n7298.140863\n5.541371\n\n\n4\n8220.600000\n5.630667\n\n\n5\n9237.460703\n5.595313\n\n\n6\n10044.702093\n5.689211\n\n\n7\n11152.793002\n5.836003\n\n\n8\n11724.719801\n5.957659\n\n\n9\n12059.118852\n6.175410\n\n\n10\n13020.365006\n6.000000\n\n\n11\n12653.756477\n6.257340\n\n\n12\n13438.675214\n6.220513\n\n\n13\n14132.446809\n5.984802\n\n\n14\n14319.277108\n6.799197\n\n\n15\n15255.932203\n6.673729\n\n\n16\n16399.778761\n7.106195\n\n\n17\n15436.965812\n7.427350\n\n\n18\n14102.445652\n6.923913\n\n\n19\n15325.520833\n6.666667\n\n\n\n\n\n\n\nI then mapped aspects about the loan with respect to whether or not the grantee has previously defaulted on their loan. I mapped this as a seaborn scatterplot using loan interest rate and percent income. This revealed that those with a prior default on history are almost never granted an interest rate lower than 12%.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.scatterplot(data=df_train[:1000], x=\"loan_int_rate\", y=\"loan_percent_income\", hue=\"cb_person_default_on_file\")\n\n\n\n\n\n\n\n\nFinally, I looked into the impact one’s age and home ownership status has on their loan status (whether or not they default) using a seaborn lineplot. It shows that defaulting on loans for those who rent stay fairly stable (and high) from around 20-70 years old. The default rate is much lower for those with mortgages than those who pay rent, and it tends to go down with age. While the rate of default for mortgages jumps up at 60, there are only 3 datapoints for this, so it may be an outlier. Home ownership default rates are the lowest, also trending downward over time (reaching a 0% default rate at 60). Finally, the “other” category fluctuates wildly, increasing drastically between those in their 20s and 30s, then decreasing even more between those in their 30s and 40s. No data exists on “other” home ownership exists in this data for those above their 40s.\n\ndf_train[\"age_group\"] = df_train[\"person_age\"] // 10\n\n# axis limiting information from: https://stackoverflow.com/questions/54822884/how-to-change-the-x-axis-range-in-seaborn\nfig, ax = plt.subplots()\n\nsns.barplot(data=df_train, x=\"age_group\", y=\"loan_status\", hue=\"person_home_ownership\", errorbar=None, ax=ax)\nax.set_xlim(-1, 6)\nplt.show()\n\n\n\n\n\n\n\n\nI then wanted to create a model to predict whether or not an applicant would default on their loan. To start, I processed the data to drop loan_grade (which was already a prediction) and loan_status (what the model is predicting). Further, I use pd.get_dummies to convert qualitative columns like “person_home_ownership” to become true/false columns for each possible answer of the column (e.g. person_home_ownership_RENT).\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df_train[\"loan_status\"])\n\ndef prepare_data(df):\n  df = df.drop([\"loan_grade\"], axis = 1)\n  df = df.dropna()\n  y = le.transform(df[\"loan_status\"])\n  df = df.drop([\"loan_status\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(df_train)\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nincome_group\nage_group\nperson_home_ownership_MORTGAGE\n...\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\n9\n2\nFalse\n...\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\n3\n2\nFalse\n...\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\n2\n2\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\n5\n2\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\n2\n2\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n5 rows × 21 columns\n\n\n\nI then used a logistic regression (LR) model from SciKitLearn to figure out the most important variables for the model to consider and the weights of their importance. I did this by procedurally iterating through one qualitative column and different pairs of quantitative column and scoring the LR model through cross-validation. The best scoring columns were saved and output for future use.\n\nwarnings.filterwarnings(\"ignore\") # just ignores the convergence warning in this case\n\nresults_LR = []\n\nall_qual_cols = [\"person_home_ownership\", \"loan_intent\", \"cb_person_default_on_file\"]\nall_quant_cols = [\"person_age\", \"person_income\", \"person_emp_length\", \"loan_amnt\",\n                  \"loan_int_rate\", \"loan_percent_income\", \"cb_person_cred_hist_length\"]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\n    LR = LogisticRegression()\n\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    new_score_LR = cv_scores_LR.mean()\n    results_LR.append((new_score_LR, cols))\n\nresults_LR.sort(reverse=True, key=lambda x : x[0])\nprint(results_LR)\n\n[(0.8486483607400082, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_percent_income', 'cb_person_cred_hist_length']), (0.8485174611967847, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_emp_length', 'loan_percent_income']), (0.8485174326119376, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'loan_percent_income']), (0.8455491439743476, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_int_rate', 'loan_percent_income']), (0.8317106287322877, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_int_rate', 'loan_percent_income']), (0.8234163351539022, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_int_rate', 'loan_percent_income']), (0.82097141601043, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_percent_income', 'cb_person_cred_hist_length']), (0.8209277669488848, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'loan_percent_income']), (0.8208841369439044, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_emp_length', 'loan_percent_income']), (0.8172610552134426, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_emp_length', 'loan_int_rate']), (0.8164753721056293, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_emp_length', 'loan_int_rate']), (0.8161260462173147, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_amnt', 'loan_int_rate']), (0.8156459065403178, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'loan_int_rate']), (0.8156021431393843, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_int_rate', 'cb_person_cred_hist_length']), (0.8155148926577057, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_int_rate', 'cb_person_cred_hist_length']), (0.8153401439590071, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'loan_int_rate']), (0.8151218795947164, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_emp_length', 'loan_percent_income']), (0.8134193089308305, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'loan_percent_income']), (0.8133756979824149, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_percent_income', 'cb_person_cred_hist_length']), (0.8080062917154157, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'loan_amnt']), (0.8080062917154157, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'loan_amnt']), (0.8080062917154157, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'loan_amnt']), (0.8071778170914709, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_amnt', 'loan_int_rate']), (0.8036409282033443, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_emp_length', 'loan_int_rate']), (0.8024182590093959, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'loan_int_rate']), (0.802330970414588, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'loan_int_rate']), (0.8022872927681955, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'loan_int_rate']), (0.7987950343546985, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'loan_int_rate']), (0.7984021356310976, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_int_rate', 'cb_person_cred_hist_length']), (0.7966566781682565, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_amnt', 'loan_percent_income']), (0.788929155410192, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_emp_length', 'loan_amnt']), (0.7889291172970626, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'loan_amnt']), (0.7888418001174075, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_amnt', 'cb_person_cred_hist_length']), (0.7852621482979962, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'loan_amnt']), (0.7851748787597531, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_emp_length', 'loan_amnt']), (0.7851311915850785, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'loan_amnt']), (0.7850875234669684, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_amnt', 'loan_percent_income']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'person_income']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'person_emp_length']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'cb_person_cred_hist_length']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'person_emp_length']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'loan_percent_income']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'cb_person_cred_hist_length']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_emp_length', 'cb_person_cred_hist_length']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'person_income']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'person_emp_length']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'cb_person_cred_hist_length']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'person_emp_length']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'loan_percent_income']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'cb_person_cred_hist_length']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_emp_length', 'cb_person_cred_hist_length']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_amnt', 'loan_percent_income']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'person_income']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'person_emp_length']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'cb_person_cred_hist_length']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'person_emp_length']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'loan_percent_income']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'cb_person_cred_hist_length']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_emp_length', 'cb_person_cred_hist_length']), (0.784781922866458, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_emp_length', 'loan_amnt']), (0.7846946152150853, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_amnt', 'cb_person_cred_hist_length']), (0.7843018022460259, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_amnt', 'cb_person_cred_hist_length']), (0.778234630332658, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_amnt', 'loan_int_rate'])]\n\n\nWith the columns defined, we can get the weights of each column using LogisticRegression.coef_.\n\ncols = ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN',\n         'person_home_ownership_RENT', 'loan_percent_income', 'cb_person_cred_hist_length']\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\n\nLR.coef_\n\narray([[-1.64154192e-01,  4.97682620e-01, -1.20631381e+00,\n         8.71319225e-01,  8.28169156e+00, -4.40583292e-03]])\n\n\nA simple linear score function is defined here as the inner product between the values of the relevant columns (X) and the values of w (our weights, defined above). X_train is then given profit, cost, and score columns based on the profit and cost functions provided.\n\ndef linear_score(w, X):\n    return X@w\n\n\nX_train[\"profit\"] = X_train[\"loan_amnt\"]*(1 + 0.25*0.01*X_train[\"loan_int_rate\"])**10 - X_train[\"loan_amnt\"]\n\nX_train[\"cost\"] = X_train[\"loan_amnt\"]*(1 + 0.25*0.01*X_train[\"loan_int_rate\"])**3 - 1.7*X_train[\"loan_amnt\"]\n\nX_train[\"score\"] = linear_score(np.transpose(LR.coef_), X_train[cols])\n\nThe model is then optimized for maximum profit for the bank. This algorithm checks for each threshold value between the minimum and maximum scores in the training data. The profit for each threshold is calculated as 0 if the model predicts a default (the loan being denied), the value of the profit column if the value of loan_status is 0 (no default), and the value of the cost column is the value of loan_status is 1.\n\nX_train[\"score\"].min(), X_train[\"score\"].max()\n\n(-1.1719610565334115, 7.23941006007273)\n\n\n\nmax_profit = float('-inf')\nthresh = -1.17\n\nfor t in np.arange(-1.17, 7.23, 0.01, dtype=float):\n    temp_prof = ((X_train[\"score\"] &lt; t) * ((y_train == 0) * X_train[\"profit\"] + (y_train == 1) * X_train[\"cost\"])).mean()\n    if (temp_prof &gt; max_profit):\n        max_profit = temp_prof\n        thresh = t\n\nmax_profit, thresh\n\n(1448.4426481197982, 3.350000000000004)\n\n\nWith a max profit of $1448 and a threshold of 3.35, I apply the scoring function and threshold to the testing data. This threshold and set of weights achieved a testing profit of $1392.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\nX_test, y_test = prepare_data(df_test)\n\n\nthresh = 3.35\n\nX_test[\"profit\"] = X_test[\"loan_amnt\"]*(1 + 0.25*0.01*X_test[\"loan_int_rate\"])**10 - X_test[\"loan_amnt\"]\nX_test[\"cost\"] = X_test[\"loan_amnt\"]*(1 + 0.25*0.01*X_test[\"loan_int_rate\"])**3 - 1.7*X_test[\"loan_amnt\"]\n\nX_test[\"score\"] = linear_score(np.transpose(LR.coef_), X_test[cols])\n\n# if the score is under the threshold, the person is predicted not to default, so the loan is given\n((X_test[\"score\"] &lt; thresh) * ((y_test == 0) * X_test[\"profit\"] + (y_test == 1) * X_test[\"cost\"])).mean()\n\n1392.1021914117252\n\n\nI also added a “loan_granted” column, which just negates the prediction of the model (i.e. if no default is predicted, the loan is granted). This makes some of the logic easier by preventing negations of the prediction column later on.\n\n# prediction is whether or not a default will occur\nX_test[\"pred\"] = X_test[\"score\"] &gt; thresh\nX_test[\"loan_granted\"] = ~X_test[\"pred\"]\nX_test\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\nprofit\ncost\nscore\npred\nloan_granted\n\n\n\n\n0\n21\n42000\n5.0\n1000\n15.58\n0.02\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nTrue\nFalse\n465.367227\n-578.539601\n1.01933\nFalse\nTrue\n\n\n1\n32\n51000\n2.0\n15000\n11.36\n0.29\n9\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\n4847.780062\n-9185.361205\n2.197884\nFalse\nTrue\n\n\n2\n35\n54084\n2.0\n3000\n12.61\n0.06\n6\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\n1091.841800\n-1807.236578\n1.341786\nFalse\nTrue\n\n\n3\n28\n66300\n11.0\n12000\n14.11\n0.15\n6\nTrue\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nTrue\nFalse\n4972.214553\n-7084.777554\n1.051665\nFalse\nTrue\n\n\n4\n22\n70550\n0.0\n7000\n15.88\n0.08\n3\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nTrue\nFalse\n3331.859215\n-4032.764115\n1.520637\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6511\n29\n78000\n2.0\n18000\n6.62\n0.23\n5\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nTrue\nFalse\n3210.941787\n-11691.427669\n1.718606\nFalse\nTrue\n\n\n6513\n27\n44640\n0.0\n12800\n11.83\n0.29\n9\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nTrue\nFalse\n4331.281644\n-7790.401145\n3.233357\nFalse\nTrue\n\n\n6514\n24\n48000\n5.0\n10400\n7.37\n0.22\n3\nFalse\nFalse\nTrue\n...\nFalse\nFalse\nTrue\nTrue\nFalse\n2083.140437\n-6694.483153\n0.602441\nFalse\nTrue\n\n\n6515\n26\n65000\n6.0\n6000\n9.07\n0.09\n3\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\n1508.058449\n-3782.525248\n0.567981\nFalse\nTrue\n\n\n6516\n29\n61000\n12.0\n10000\n16.07\n0.16\n9\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nTrue\nFalse\n4827.369675\n-5745.680644\n2.156737\nFalse\nTrue\n\n\n\n\n5731 rows × 24 columns\n\n\n\nWith a complete model, it’s important to do analysis on its impact. First, we can see through the creation of an age group column that acceptance rates tend to rise from ages 20 to 44. After this, we see a dip at 45-54 year olds before the rate rises again. Another large outlier is those in group 13 (the 65-69 year old age range), with an acceptance rate of 60%.\n\n# F1\nX_test[\"age_group\"] = X_test[\"person_age\"] // 5\nX_test.groupby(\"age_group\")[\"loan_granted\"].mean()\n\nage_group\n4     0.909465\n5     0.916070\n6     0.926056\n7     0.940758\n8     0.942408\n9     0.913043\n10    0.906250\n11    0.900000\n12    1.000000\n13    0.600000\n14    1.000000\nName: loan_granted, dtype: float64\n\n\nOne reason for these large flucuations and 100% acceptance rates, however, could be the low count of applicants in the higher age groups. Although group 13 has the lowest acceptance rate, they also have the lowest sample size with just 5 people. This rise is still notable, especially in groups with larger sample sizes like those in the 20-24 range compared to those in the 40-44 range.\n\nX_test.groupby(\"age_group\")[\"age_group\"].count()\n\nage_group\n4     2187\n5     1954\n6      852\n7      422\n8      191\n9       69\n10      32\n11      10\n12       7\n13       5\n14       2\nName: age_group, dtype: int64\n\n\nThe next test compares the model’s acceptance of those based on loan intent, comparing to its average acceptance rate. Here, we can see that loans for debt consolidaiton, medical expenses, and personal expenses are denied more often than average (where education, home improvement, and venture expenses are approved more often). Here, it is also notable that the model approves loans (i.e. sees no default) much more often than the data dictates that no default will occur.\n\n# F2\nintent_cols = [col for col in X_train.columns if \"loan_intent\" in col ]\n\nmodel_average = X_test[\"loan_granted\"].mean()\nprint(f'Model average acceptance: {round(model_average, 6)}\\n')\n\nsum = 0\nfor intent in intent_cols:\n    intent_count = X_test[intent].sum()\n    model_score = (X_test[intent] & X_test[\"loan_granted\"]).sum()/intent_count\n    result = (X_test[intent] & ~y_test).sum()/intent_count\n\n    sum += intent_count\n    print(f'{intent[12:]}:')\n    print(f'Model grant amount: {round(model_score, 6)}')\n    print(f'Actual grant amount: {round(result, 6)}')\n    print(f'Model\\'s group grant to averge grant ratio: {round(model_score/model_average, 6)}')\n    print(\"\")\n\nModel average acceptance: 0.917466\n\nDEBTCONSOLIDATION:\nModel grant amount: 0.904867\nActual grant amount: 0.712389\nModel's group grant to averge grant ratio: 0.986267\n\nEDUCATION:\nModel grant amount: 0.922619\nActual grant amount: 0.832483\nModel's group grant to averge grant ratio: 1.005616\n\nHOMEIMPROVEMENT:\nModel grant amount: 0.962662\nActual grant amount: 0.75\nModel's group grant to averge grant ratio: 1.049262\n\nMEDICAL:\nModel grant amount: 0.898416\nActual grant amount: 0.71575\nModel's group grant to averge grant ratio: 0.979235\n\nPERSONAL:\nModel grant amount: 0.912826\nActual grant amount: 0.779559\nModel's group grant to averge grant ratio: 0.994942\n\nVENTURE:\nModel grant amount: 0.920124\nActual grant amount: 0.853734\nModel's group grant to averge grant ratio: 1.002897\n\n\n\nFinally, I checked how the model grants loans to those of different income groups. I did this by (again) dividing the incomes into groups of 10000. When sorting by these groups, it becomes clear that the likelihood of being granted a loan by this model rises drastically as one’s income does. Above an income of 110000, this likelihood is 100% (the amount of income groups included is truncated for readability).\n\n# F3\nX_test[\"income_group\"] = X_test[\"person_income\"] // 10000\nX_test.groupby(\"income_group\")[\"loan_granted\"].mean()[:20]\n\nincome_group\n0     0.636364\n1     0.775194\n2     0.805195\n3     0.827586\n4     0.890346\n5     0.939153\n6     0.959762\n7     0.962891\n8     0.976401\n9     0.992883\n10    0.994737\n11    1.000000\n12    1.000000\n13    1.000000\n14    1.000000\n15    1.000000\n16    1.000000\n17    1.000000\n18    1.000000\n19    1.000000\nName: loan_granted, dtype: float64"
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#medical-credit",
    "href": "posts/optimal-decision-making/optimal.html#medical-credit",
    "title": "\"Optimal\" Decision Making",
    "section": "Medical Credit",
    "text": "Medical Credit\nI would say that it is not fair for those seeking loans for medical expenses to obtain access to credit. Here, I’m defining fairness in a line with a middle view of equality of opportunity; I tend to view these problems from the broad sense, but since we’re the taking the view of the decision maker, a broad approach is less applicabale. Specifically, we should act “to avoid perpetuating injustice” (from “Relative Notions of Fairness” in Fairness and Machine Learning).\nThe middle view here would counteract the unfairness of the medical (particularly healthcare/insurance) system, and the ability of many to access it. The decision maker (the bank) should then act in spite of the situation that left the applicants medically vulnerable and in support of their health and safety."
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#conclusion",
    "href": "posts/optimal-decision-making/optimal.html#conclusion",
    "title": "\"Optimal\" Decision Making",
    "section": "Conclusion",
    "text": "Conclusion\nThrough the creation of this model, I found that the creation of a model that maximizes profit will often deprioritize the health and safety of those affected by it. This was seen in the investigation of different groups the model decides on. Those with medical expenses and less income, for instance, are disproportionately denied by this model. Maximizing for profit, then, shouldn’t be seen as the total goal for decision makers. Through this investigation, I also learned that those with previous credit issues are saddled with higher interest rates, increasing the risk of default occuring again. Beyond domain specific findings, I gained practice in thresholding, weighting, and building decision-making models."
  }
]