[
  {
    "objectID": "posts/penguin-post/penguin.html",
    "href": "posts/penguin-post/penguin.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "To classify the Palmer Penguins, I first had to import the data set into my Python environment:\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nI then checked the elements of set. This would give me an idea of what could be used for visualization and to train the models.\n\ntrain.keys()\n\nIndex(['studyName', 'Sample Number', 'Species', 'Region', 'Island', 'Stage',\n       'Individual ID', 'Clutch Completion', 'Date Egg', 'Culmen Length (mm)',\n       'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Sex',\n       'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)', 'Comments'],\n      dtype='object')\n\n\nLet’s look at the top of the data:\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nThis provided function prepares the data by dropping non-helpful elements (i.e. identifying or helpful information to scientists working with the data that isn’t useful to the model). Species values are also encoded into numerical values so they can be output by the computer. Useless values like “.” sex penguins and NA value entires are also dropped from the set. This function creates X_train and y_train data that will be used throughout the fitting process, with X_train being the valid columns of the dataframe and y_train being the species (result) column.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nThe following code tests different models on a variety of elements in the data. The qualitative and quantitative columns are put into individual arrays and are matched in groups of three (one qualitative column and two quantitative columns) using the combinations function from itertools. The different models – logistic regression, decision tree, random forest, and support vector from sklearn’s library – are tested using cross-validation to ensure models that overfit aren’t chosen. For the support vector and decision tree classifiers, other parameters (gamma and max depth respectively) are iterated over to ensure the highest score possible. For each model fit, the columns used, score, and extra parameters are appended to their results array. These results are sorted in descending order by highest score and a truncated version of each is printed out at the end of fitting. This allows for me to choose the best model and elements to fit to predict the test data.\n\nfrom sklearn.exceptions import ConvergenceWarning\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nwarnings.filterwarnings(\"ignore\") # just ignores the convergence warning in this case\n\nresults_LR = []\nresults_SVC = []\nresults_DTC = []\nresults_RFC = []\n\nall_qual_cols = [\"Island\", \"Stage\", \"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)',\n                  'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\n    LR = LogisticRegression()\n\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    new_score_LR = cv_scores_LR.mean()\n    results_LR.append((new_score_LR, cols))\n    \n    for g in 10.0**np.arange(-5, 5):\n      SVM = SVC(gamma = g)\n      cv_scores_SVC = cross_val_score(SVM, X_train[cols], y_train, cv = 5)\n      new_score_SVC = cv_scores_SVC.mean()\n      results_SVC.append((new_score_SVC, cols, SVM.gamma))\n\n    for m in np.arange(2, 25):\n      DTC = DecisionTreeClassifier(max_depth = m)\n\n      cv_scores_DTC = cross_val_score(DTC, X_train[cols], y_train, cv = 5)\n      new_score_DTC = cv_scores_DTC.mean()\n      results_DTC.append((new_score_DTC, cols, DTC.max_depth))\n\n    RFC = RandomForestClassifier()\n\n    cv_scores_RFC = cross_val_score(RFC, X_train[cols], y_train, cv = 5)\n    new_score_RFC = cv_scores_RFC.mean()\n    results_RFC.append((new_score_RFC, cols))\n      \nprint(\"LR:\")\n# lamba key tip on tuples from https://docs.python.org/3/howto/sorting.html\nresults_LR.sort(reverse=True, key=lambda x : x[0])\nprint(results_LR[:3])\n\nprint(\"SVC:\")\nresults_SVC.sort(reverse=True, key=lambda x : x[0])\nprint(results_SVC[:3])\n\nprint(\"DTC:\")\nresults_DTC.sort(reverse=True, key=lambda x : x[0])\nprint(results_DTC[:3])\n\nprint(\"RFC:\")\nresults_RFC.sort(reverse=True, key=lambda x : x[0])\nprint(results_RFC[:3])\n\nLR:\n[(0.9922322775263952, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']), (0.9844645550527904, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']), (0.9726244343891401, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Delta 13 C (o/oo)'])]\nSVC:\n[(0.9805429864253394, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 0.1), (0.9687782805429863, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 0.1), (0.9649321266968325, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 1.0)]\nDTC:\n[(0.9765460030165913, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 8), (0.9765460030165913, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 9), (0.9765460030165913, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 10)]\nRFC:\n[(0.9883107088989442, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']), (0.9843891402714933, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']), (0.9806184012066363, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)'])]\n\n\nBefore creating the models, I wanted to create the visualizations and tables out of the data I just received. I picked the data that I didn’t plan on using for the models, but still scored highly in the model fitting. This way, the data representations will be strongly related to the species of the penguin but won’t be redundant with the final models. I also had to choose between the different quantitative factors, as I felt that isolating to one factor would clearly show its relation to species. The data table, then, was made by grouping by qualitative factors and returning a quantitative factor.\n\ngb = train.groupby([\"Island\", \"Species\"])[\"Culmen Length (mm)\"].aggregate(\"mean\")\nprint(gb)\n\nIsland     Species                                  \nBiscoe     Adelie Penguin (Pygoscelis adeliae)          38.845455\n           Gentoo penguin (Pygoscelis papua)            47.073196\nDream      Adelie Penguin (Pygoscelis adeliae)          38.826667\n           Chinstrap penguin (Pygoscelis antarctica)    48.826316\nTorgersen  Adelie Penguin (Pygoscelis adeliae)          39.229268\nName: Culmen Length (mm), dtype: float64\n\n\nFor the visualizations, I chose two quantiative factors from high scoring models, using species as a color factor. This can be achieved easily with seaborn:\n\nimport seaborn as sns\n\nsns.scatterplot(data=train, x=\"Culmen Length (mm)\", y=\"Flipper Length (mm)\", hue=\"Species\")\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data=train, x=\"Flipper Length (mm)\", y=\"Delta 13 C (o/oo)\", hue=\"Species\")\n\n\n\n\n\n\n\n\nTo get the models to work, I took the highest scoring three models from the previous cross validation tests. Here, I used sklearn’s logistic regression for two of the models and their random forest classifier for the last one.\n\nfrom sklearn.linear_model import LogisticRegression\n\nLR_cols_1 = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'] # LR\nLR_cols_2 = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE'] # LR\nRFC_cols = ['Culmen Length (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'] # RFC\n\nLR_1 = LogisticRegression()\nLR_1.fit(X_train[LR_cols_1], y_train)\n\nLR_2 = LogisticRegression()\nLR_2.fit(X_train[LR_cols_2], y_train)\n\nRFC = RandomForestClassifier()\nRFC.fit(X_train[RFC_cols], y_train)\n\nFinally, the test data is prepared through the same prepare_data function. Then, the models are scored on the test data, with the logistic regression models achieving 100% accuracy!\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\n# Chose top three from above, tested on test data\n\nprint(\"LR1:\")\nprint(LR_1.score(X_test[LR_cols_1], y_test))\n\nprint(\"LR2:\")\nprint(LR_2.score(X_test[LR_cols_2], y_test))\n\nprint(\"RFC: \")\nprint(RFC.score(X_test[RFC_cols], y_test))\n\nLR1:\n1.0\nLR2:\n1.0\nRFC: \n0.9852941176470589\n\n\nThis provided plot_regions function takes in a model alongside the X and y columns of data and plots the its decision regions, allowing for easy visualization.\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nFinally, I plotted my two logistic regression models (the models that achieved 100% prediction accuracy) using the plot_regions function.\n\n\nplot_regions(LR_1, X_train[LR_cols_1], y_train)\nplot_regions(LR_2, X_train[LR_cols_2], y_train)"
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Testing quarto!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "\"Optimal\" Decision Making\n\n\n\n\n\nMaking “optimal” decisions based on loan-default data and model training. The definition of optimal, though, becomes flawed with profit incentives.\n\n\n\n\n\nMar 27, 2023\n\n\nJeff Blake\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nHow I made models that could predict the species from the Palmer Penguins dataset.\n\n\n\n\n\nFeb 28, 2023\n\n\nJeff Blake\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html",
    "href": "posts/optimal-decision-making/optimal.html",
    "title": "\"Optimal\" Decision Making",
    "section": "",
    "text": "This blog post will explore the data behind loan granting from various perspectives. The data will first be shown “as is” through visualizations and data tables. Then, a model will be trained on the data through the creation of a scoring function and threshold. Finally, more in-depth quantiative analysis will be done on the profits, costs, and biases that are created from the model. Through this study, a profitable model for the bank was found, but this came at the cost of differing acceptance rates across groups."
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#introduction",
    "href": "posts/optimal-decision-making/optimal.html#introduction",
    "title": "\"Optimal\" Decision Making",
    "section": "",
    "text": "This blog post will explore the data behind loan granting from various perspectives. The data will first be shown “as is” through visualizations and data tables. Then, a model will be trained on the data through the creation of a scoring function and threshold. Finally, more in-depth quantiative analysis will be done on the profits, costs, and biases that are created from the model. Through this study, a profitable model for the bank was found, but this came at the cost of differing acceptance rates across groups."
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#model-building",
    "href": "posts/optimal-decision-making/optimal.html#model-building",
    "title": "\"Optimal\" Decision Making",
    "section": "Model Building",
    "text": "Model Building\nFirst, the data is converted to a dataframe through pandas and checked using the head() function\n\nimport pandas as pd\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nimport warnings\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\nI first investigated the relationship between personal income, financial history, and loan amounts. I did this by creating a table that groups by income intervals of 10000 and checking credit history and the size of the loan. It shows that loan amounts granted increase as personal income increases, while one’s credit history tends to be higher as income increases, but this isn’t as drastic of an upward trend.\n\ndf_train[\"income_group\"] = df_train[\"person_income\"] // 10000\n\ndf_train.groupby([\"income_group\"])[[\"loan_amnt\", \"cb_person_cred_hist_length\"]].aggregate(\"mean\")[:20]\n\n\n\n\n\n\n\n\nloan_amnt\ncb_person_cred_hist_length\n\n\nincome_group\n\n\n\n\n\n\n0\n2093.333333\n5.566667\n\n\n1\n3837.235367\n5.282690\n\n\n2\n5731.833494\n5.532243\n\n\n3\n7298.140863\n5.541371\n\n\n4\n8220.600000\n5.630667\n\n\n5\n9237.460703\n5.595313\n\n\n6\n10044.702093\n5.689211\n\n\n7\n11152.793002\n5.836003\n\n\n8\n11724.719801\n5.957659\n\n\n9\n12059.118852\n6.175410\n\n\n10\n13020.365006\n6.000000\n\n\n11\n12653.756477\n6.257340\n\n\n12\n13438.675214\n6.220513\n\n\n13\n14132.446809\n5.984802\n\n\n14\n14319.277108\n6.799197\n\n\n15\n15255.932203\n6.673729\n\n\n16\n16399.778761\n7.106195\n\n\n17\n15436.965812\n7.427350\n\n\n18\n14102.445652\n6.923913\n\n\n19\n15325.520833\n6.666667\n\n\n\n\n\n\n\nI then mapped aspects about the loan with respect to whether or not the grantee has previously defaulted on their loan. I mapped this as a seaborn scatterplot using loan interest rate and percent income. This revealed that those with a prior default on history are almost never granted an interest rate lower than 12%.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.scatterplot(data=df_train[:1000], x=\"loan_int_rate\", y=\"loan_percent_income\", hue=\"cb_person_default_on_file\")\n\n\n\n\n\n\n\n\nFinally, I looked into the impact one’s age and home ownership status has on their loan status (whether or not they default) using a seaborn lineplot. It shows that defaulting on loans for those who rent stay fairly stable (and high) from around 20-70 years old. The default rate is much lower for those with mortgages than those who pay rent, and it tends to go down with age. While the rate of default for mortgages jumps up at 60, there are only 3 datapoints for this, so it may be an outlier. Home ownership default rates are the lowest, also trending downward over time (reaching a 0% default rate at 60). Finally, the “other” category fluctuates wildly, increasing drastically between those in their 20s and 30s, then decreasing even more between those in their 30s and 40s. No data exists on “other” home ownership exists in this data for those above their 40s.\n\ndf_train[\"age_group\"] = df_train[\"person_age\"] // 10\n\n# axis limiting information from: https://stackoverflow.com/questions/54822884/how-to-change-the-x-axis-range-in-seaborn\nfig, ax = plt.subplots()\n\nsns.barplot(data=df_train, x=\"age_group\", y=\"loan_status\", hue=\"person_home_ownership\", errorbar=None, ax=ax)\nax.set_xlim(-1, 6)\nplt.show()\n\n\n\n\n\n\n\n\nI then wanted to create a model to predict whether or not an applicant would default on their loan. To start, I processed the data to drop loan_grade (which was already a prediction) and loan_status (what the model is predicting). Further, I use pd.get_dummies to convert qualitative columns like “person_home_ownership” to become true/false columns for each possible answer of the column (e.g. person_home_ownership_RENT).\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df_train[\"loan_status\"])\n\ndef prepare_data(df):\n  df = df.drop([\"loan_grade\"], axis = 1)\n  df = df.dropna()\n  y = le.transform(df[\"loan_status\"])\n  df = df.drop([\"loan_status\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(df_train)\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nincome_group\nage_group\nperson_home_ownership_MORTGAGE\n...\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\n9\n2\nFalse\n...\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\n3\n2\nFalse\n...\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\n2\n2\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\n5\n2\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\n2\n2\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n5 rows × 21 columns\n\n\n\nI then used a logistic regression (LR) model from SciKitLearn to figure out the most important variables for the model to consider and the weights of their importance. I did this by procedurally iterating through one qualitative column and different pairs of quantitative column and scoring the LR model through cross-validation. The best scoring columns were saved and output for future use.\n\nwarnings.filterwarnings(\"ignore\") # just ignores the convergence warning in this case\n\nresults_LR = []\n\nall_qual_cols = [\"person_home_ownership\", \"loan_intent\", \"cb_person_default_on_file\"]\nall_quant_cols = [\"person_age\", \"person_income\", \"person_emp_length\", \"loan_amnt\",\n                  \"loan_int_rate\", \"loan_percent_income\", \"cb_person_cred_hist_length\"]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\n    LR = LogisticRegression()\n\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    new_score_LR = cv_scores_LR.mean()\n    results_LR.append((new_score_LR, cols))\n\nresults_LR.sort(reverse=True, key=lambda x : x[0])\nprint(results_LR)\n\n[(0.8486483607400082, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_percent_income', 'cb_person_cred_hist_length']), (0.8485174611967847, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_emp_length', 'loan_percent_income']), (0.8485174326119376, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'loan_percent_income']), (0.8455491439743476, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_int_rate', 'loan_percent_income']), (0.8317106287322877, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_int_rate', 'loan_percent_income']), (0.8234163351539022, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_int_rate', 'loan_percent_income']), (0.82097141601043, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_percent_income', 'cb_person_cred_hist_length']), (0.8209277669488848, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'loan_percent_income']), (0.8208841369439044, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_emp_length', 'loan_percent_income']), (0.8172610552134426, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_emp_length', 'loan_int_rate']), (0.8164753721056293, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_emp_length', 'loan_int_rate']), (0.8161260462173147, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_amnt', 'loan_int_rate']), (0.8156459065403178, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'loan_int_rate']), (0.8156021431393843, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_int_rate', 'cb_person_cred_hist_length']), (0.8155148926577057, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_int_rate', 'cb_person_cred_hist_length']), (0.8153401439590071, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'loan_int_rate']), (0.8151218795947164, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_emp_length', 'loan_percent_income']), (0.8134193089308305, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'loan_percent_income']), (0.8133756979824149, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_percent_income', 'cb_person_cred_hist_length']), (0.8080062917154157, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'loan_amnt']), (0.8080062917154157, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'loan_amnt']), (0.8080062917154157, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'loan_amnt']), (0.8071778170914709, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_amnt', 'loan_int_rate']), (0.8036409282033443, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_emp_length', 'loan_int_rate']), (0.8024182590093959, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'loan_int_rate']), (0.802330970414588, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'loan_int_rate']), (0.8022872927681955, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'loan_int_rate']), (0.7987950343546985, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'loan_int_rate']), (0.7984021356310976, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_int_rate', 'cb_person_cred_hist_length']), (0.7966566781682565, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_amnt', 'loan_percent_income']), (0.788929155410192, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_emp_length', 'loan_amnt']), (0.7889291172970626, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'loan_amnt']), (0.7888418001174075, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_amnt', 'cb_person_cred_hist_length']), (0.7852621482979962, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'loan_amnt']), (0.7851748787597531, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_emp_length', 'loan_amnt']), (0.7851311915850785, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'loan_amnt']), (0.7850875234669684, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_amnt', 'loan_percent_income']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'person_income']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'person_emp_length']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'cb_person_cred_hist_length']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'person_emp_length']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'loan_percent_income']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'cb_person_cred_hist_length']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_emp_length', 'cb_person_cred_hist_length']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'person_income']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'person_emp_length']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'cb_person_cred_hist_length']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'person_emp_length']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'loan_percent_income']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'cb_person_cred_hist_length']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_emp_length', 'cb_person_cred_hist_length']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_amnt', 'loan_percent_income']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'person_income']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'person_emp_length']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'cb_person_cred_hist_length']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'person_emp_length']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'loan_percent_income']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'cb_person_cred_hist_length']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_emp_length', 'cb_person_cred_hist_length']), (0.784781922866458, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_emp_length', 'loan_amnt']), (0.7846946152150853, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_amnt', 'cb_person_cred_hist_length']), (0.7843018022460259, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_amnt', 'cb_person_cred_hist_length']), (0.778234630332658, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_amnt', 'loan_int_rate'])]\n\n\nWith the columns defined, we can get the weights of each column using LogisticRegression.coef_.\n\ncols = ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN',\n         'person_home_ownership_RENT', 'loan_percent_income', 'cb_person_cred_hist_length']\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\n\nLR.coef_\n\narray([[-1.64154192e-01,  4.97682620e-01, -1.20631381e+00,\n         8.71319225e-01,  8.28169156e+00, -4.40583292e-03]])\n\n\nA simple linear score function is defined here as the inner product between the values of the relevant columns (X) and the values of w (our weights, defined above). X_train is then given profit, cost, and score columns based on the profit and cost functions provided.\n\ndef linear_score(w, X):\n    return X@w\n\n\nX_train[\"profit\"] = X_train[\"loan_amnt\"]*(1 + 0.25*0.01*X_train[\"loan_int_rate\"])**10 - X_train[\"loan_amnt\"]\n\nX_train[\"cost\"] = X_train[\"loan_amnt\"]*(1 + 0.25*0.01*X_train[\"loan_int_rate\"])**3 - 1.7*X_train[\"loan_amnt\"]\n\nX_train[\"score\"] = linear_score(np.transpose(LR.coef_), X_train[cols])\n\nThe model is then optimized for maximum profit for the bank. This algorithm checks for each threshold value between the minimum and maximum scores in the training data. The profit for each threshold is calculated as 0 if the model predicts a default (the loan being denied), the value of the profit column if the value of loan_status is 0 (no default), and the value of the cost column is the value of loan_status is 1.\n\nX_train[\"score\"].min(), X_train[\"score\"].max()\n\n(-1.1719610565334115, 7.23941006007273)\n\n\n\nmax_profit = float('-inf')\nthresh = -1.17\n\nfor t in np.arange(-1.17, 7.23, 0.01, dtype=float):\n    temp_prof = ((X_train[\"score\"] &lt; t) * ((y_train == 0) * X_train[\"profit\"] + (y_train == 1) * X_train[\"cost\"])).mean()\n    if (temp_prof &gt; max_profit):\n        max_profit = temp_prof\n        thresh = t\n\nmax_profit, thresh\n\n(1448.4426481197982, 3.350000000000004)\n\n\nWith a max profit of $1448 and a threshold of 3.35, I apply the scoring function and threshold to the testing data. This threshold and set of weights achieved a testing profit of $1392.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\nX_test, y_test = prepare_data(df_test)\n\n\nthresh = 3.35\n\nX_test[\"profit\"] = X_test[\"loan_amnt\"]*(1 + 0.25*0.01*X_test[\"loan_int_rate\"])**10 - X_test[\"loan_amnt\"]\nX_test[\"cost\"] = X_test[\"loan_amnt\"]*(1 + 0.25*0.01*X_test[\"loan_int_rate\"])**3 - 1.7*X_test[\"loan_amnt\"]\n\nX_test[\"score\"] = linear_score(np.transpose(LR.coef_), X_test[cols])\n\n# if the score is under the threshold, the person is predicted not to default, so the loan is given\n((X_test[\"score\"] &lt; thresh) * ((y_test == 0) * X_test[\"profit\"] + (y_test == 1) * X_test[\"cost\"])).mean()\n\n1392.1021914117252\n\n\nI also added a “loan_granted” column, which just negates the prediction of the model (i.e. if no default is predicted, the loan is granted). This makes some of the logic easier by preventing negations of the prediction column later on.\n\n# prediction is whether or not a default will occur\nX_test[\"pred\"] = X_test[\"score\"] &gt; thresh\nX_test[\"loan_granted\"] = ~X_test[\"pred\"]\nX_test\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\nprofit\ncost\nscore\npred\nloan_granted\n\n\n\n\n0\n21\n42000\n5.0\n1000\n15.58\n0.02\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nTrue\nFalse\n465.367227\n-578.539601\n1.01933\nFalse\nTrue\n\n\n1\n32\n51000\n2.0\n15000\n11.36\n0.29\n9\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\n4847.780062\n-9185.361205\n2.197884\nFalse\nTrue\n\n\n2\n35\n54084\n2.0\n3000\n12.61\n0.06\n6\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\n1091.841800\n-1807.236578\n1.341786\nFalse\nTrue\n\n\n3\n28\n66300\n11.0\n12000\n14.11\n0.15\n6\nTrue\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nTrue\nFalse\n4972.214553\n-7084.777554\n1.051665\nFalse\nTrue\n\n\n4\n22\n70550\n0.0\n7000\n15.88\n0.08\n3\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nTrue\nFalse\n3331.859215\n-4032.764115\n1.520637\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6511\n29\n78000\n2.0\n18000\n6.62\n0.23\n5\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nTrue\nFalse\n3210.941787\n-11691.427669\n1.718606\nFalse\nTrue\n\n\n6513\n27\n44640\n0.0\n12800\n11.83\n0.29\n9\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nTrue\nFalse\n4331.281644\n-7790.401145\n3.233357\nFalse\nTrue\n\n\n6514\n24\n48000\n5.0\n10400\n7.37\n0.22\n3\nFalse\nFalse\nTrue\n...\nFalse\nFalse\nTrue\nTrue\nFalse\n2083.140437\n-6694.483153\n0.602441\nFalse\nTrue\n\n\n6515\n26\n65000\n6.0\n6000\n9.07\n0.09\n3\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\n1508.058449\n-3782.525248\n0.567981\nFalse\nTrue\n\n\n6516\n29\n61000\n12.0\n10000\n16.07\n0.16\n9\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nTrue\nFalse\n4827.369675\n-5745.680644\n2.156737\nFalse\nTrue\n\n\n\n\n5731 rows × 24 columns\n\n\n\nWith a complete model, it’s important to do analysis on its impact. First, we can see through the creation of an age group column that acceptance rates tend to rise from ages 20 to 44. After this, we see a dip at 45-54 year olds before the rate rises again. Another large outlier is those in group 13 (the 65-69 year old age range), with an acceptance rate of 60%.\n\n# F1\nX_test[\"age_group\"] = X_test[\"person_age\"] // 5\nX_test.groupby(\"age_group\")[\"loan_granted\"].mean()\n\nage_group\n4     0.909465\n5     0.916070\n6     0.926056\n7     0.940758\n8     0.942408\n9     0.913043\n10    0.906250\n11    0.900000\n12    1.000000\n13    0.600000\n14    1.000000\nName: loan_granted, dtype: float64\n\n\nOne reason for these large flucuations and 100% acceptance rates, however, could be the low count of applicants in the higher age groups. Although group 13 has the lowest acceptance rate, they also have the lowest sample size with just 5 people. This rise is still notable, especially in groups with larger sample sizes like those in the 20-24 range compared to those in the 40-44 range.\n\nX_test.groupby(\"age_group\")[\"age_group\"].count()\n\nage_group\n4     2187\n5     1954\n6      852\n7      422\n8      191\n9       69\n10      32\n11      10\n12       7\n13       5\n14       2\nName: age_group, dtype: int64\n\n\nThe next test compares the model’s acceptance of those based on loan intent, comparing to its average acceptance rate. Here, we can see that loans for debt consolidaiton, medical expenses, and personal expenses are denied more often than average (where education, home improvement, and venture expenses are approved more often). Here, it is also notable that the model approves loans (i.e. sees no default) much more often than the data dictates that no default will occur.\n\n# F2\nintent_cols = [col for col in X_train.columns if \"loan_intent\" in col ]\n\nmodel_average = X_test[\"loan_granted\"].mean()\nprint(f'Model average acceptance: {round(model_average, 6)}\\n')\n\nsum = 0\nfor intent in intent_cols:\n    intent_count = X_test[intent].sum()\n    model_score = (X_test[intent] & X_test[\"loan_granted\"]).sum()/intent_count\n    result = (X_test[intent] & ~y_test).sum()/intent_count\n\n    sum += intent_count\n    print(f'{intent[12:]}:')\n    print(f'Model grant amount: {round(model_score, 6)}')\n    print(f'Actual grant amount: {round(result, 6)}')\n    print(f'Model\\'s group grant to averge grant ratio: {round(model_score/model_average, 6)}')\n    print(\"\")\n\nModel average acceptance: 0.917466\n\nDEBTCONSOLIDATION:\nModel grant amount: 0.904867\nActual grant amount: 0.712389\nModel's group grant to averge grant ratio: 0.986267\n\nEDUCATION:\nModel grant amount: 0.922619\nActual grant amount: 0.832483\nModel's group grant to averge grant ratio: 1.005616\n\nHOMEIMPROVEMENT:\nModel grant amount: 0.962662\nActual grant amount: 0.75\nModel's group grant to averge grant ratio: 1.049262\n\nMEDICAL:\nModel grant amount: 0.898416\nActual grant amount: 0.71575\nModel's group grant to averge grant ratio: 0.979235\n\nPERSONAL:\nModel grant amount: 0.912826\nActual grant amount: 0.779559\nModel's group grant to averge grant ratio: 0.994942\n\nVENTURE:\nModel grant amount: 0.920124\nActual grant amount: 0.853734\nModel's group grant to averge grant ratio: 1.002897\n\n\n\nFinally, I checked how the model grants loans to those of different income groups. I did this by (again) dividing the incomes into groups of 10000. When sorting by these groups, it becomes clear that the likelihood of being granted a loan by this model rises drastically as one’s income does. Above an income of 110000, this likelihood is 100% (the amount of income groups included is truncated for readability).\n\n# F3\nX_test[\"income_group\"] = X_test[\"person_income\"] // 10000\nX_test.groupby(\"income_group\")[\"loan_granted\"].mean()[:20]\n\nincome_group\n0     0.636364\n1     0.775194\n2     0.805195\n3     0.827586\n4     0.890346\n5     0.939153\n6     0.959762\n7     0.962891\n8     0.976401\n9     0.992883\n10    0.994737\n11    1.000000\n12    1.000000\n13    1.000000\n14    1.000000\n15    1.000000\n16    1.000000\n17    1.000000\n18    1.000000\n19    1.000000\nName: loan_granted, dtype: float64"
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#medical-credit",
    "href": "posts/optimal-decision-making/optimal.html#medical-credit",
    "title": "\"Optimal\" Decision Making",
    "section": "Medical Credit",
    "text": "Medical Credit\nI would say that it is not fair for those seeking loans for medical expenses to obtain access to credit. Here, I’m defining fairness in a line with a middle view of equality of opportunity; I tend to view these problems from the broad sense, but since we’re the taking the view of the decision maker, a broad approach is less applicabale. Specifically, we should act “to avoid perpetuating injustice” (from “Relative Notions of Fairness” in Fairness and Machine Learning).\nThe middle view here would counteract the unfairness of the medical (particularly healthcare/insurance) system, and the ability of many to access it. The decision maker (the bank) should then act in spite of the situation that left the applicants medically vulnerable and in support of their health and safety."
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#conclusion",
    "href": "posts/optimal-decision-making/optimal.html#conclusion",
    "title": "\"Optimal\" Decision Making",
    "section": "Conclusion",
    "text": "Conclusion\nThrough the creation of this model, I found that the creation of a model that maximizes profit will often deprioritize the health and safety of those affected by it. This was seen in the investigation of different groups the model decides on. Those with medical expenses and less income, for instance, are disproportionately denied by this model. Maximizing for profit, then, shouldn’t be seen as the total goal for decision makers. Through this investigation, I also learned that those with previous credit issues are saddled with higher interest rates, increasing the risk of default occuring again. Beyond domain specific findings, I gained practice in thresholding, weighting, and building decision-making models."
  }
]