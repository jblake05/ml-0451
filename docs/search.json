[
  {
    "objectID": "posts/perceptron-post/index.html",
    "href": "posts/perceptron-post/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "This blog post aims to investigate the function of the perceptron algorithm by performing various experiments with it. These experiments includes tests of the algorithm on linearly separable and inseparable data and data with dimensions greater than 2. These tests will show how the algorithm functions of different data, graphing the perceptron’s loss and decision boundaries over time. Then, the algorithm will be tested for performance with minibatching, examining loss convergance at different batch sizes.\n\n\nhttps://github.com/jblake05/jblake05.github.io/blob/main/posts/perceptron-post/perceptron.py\n\n\n\nMy perceptron class has two implementations of the grad function, one with minibatching and one without. The grad function \\(1[s_iy_i &lt; 0]y_ix_i\\) is implemented by first calculating \\(s_i\\) as \\(\\langle w_i, x_i\\rangle\\) using torch.inner. Then I cast the expression \\(s_iy_i &lt; 0\\) to a boolean by multiplying it by 1. Finally I multiplied that expression by y_i and the vector x_i, flattening the expression to one dimension to add it to the weight.\nFor the minibatching function grade_k, I implemented the equation \\(\\frac{α}{k} \\sum \\limits _{l=1} ^{k} 1[\\langle w, x_{i_k} \\rangle y_{i_k} &lt; 0]y_{i_k} x_{i_k}\\) The score was formed the same way as the previous equation. The boolean map and y vector are resized from size n to size (n, 1) using y[:, None] to allow for multiplication with X. This matrix is then summed along each column to create a tensor the same size as the weight. Finally, \\(\\frac{α}{k}\\) is multiplied by the tensor to complete the equation.\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nfrom perceptron import Perceptron, PerceptronOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\n\nTo test the perceptron, I first generated data to test using code from the perceptron lecture. Also using code from the lecture, I plotted the data, differentiating the groups of X (i.e. if they corrolate with y = 1 or -1) by color.\n\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nTo test my perceptron, I ran the minimal training loop provided in the perceptron lecture. In doing so, I generate random values of X and y to act as inputs for the perceptron’s optimizer. I also collect a vector of all unique (i.e. different from the previous) loss values, which are graphed below.\n\ntorch.manual_seed(130)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    if loss == 0:\n        break\n    \n    # not part of the update: just for tracking our progress    \n    prev_loss = loss\n\n    loss = p.loss(X, y) \n    if (prev_loss != loss):\n        loss_vec.append(loss)\n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nHere, we see that the perceptron’s loss converges to 0 on linearly separable data. Due to the randomly indexed data, the loss does not decrease monotonically, but trends lower over time.\nSimilar to the previous test, this experiment tests the perceptron’s performance on a 2D dataset that is linearly separable. Instead of just plotting the loss, this code uses the draw_line function to show the decision boundary change over time relative to the dataset.\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nLike the last experiment, this block of code keeps track of the loss over time, graphing the dataset, x_i, and decision boundary at each step.\n\n# adapted from lecture 7 (Perceptron)\ntorch.manual_seed(3619)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (10, 6)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    ax = axarr.ravel()[current_ax]\n\n    if loss == 0:\n        break\n    \n    # not part of the update: just for tracking our progress    \n    prev_loss = loss\n\n    loss = p.loss(X, y) \n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    old_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w)\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n\n    if (prev_loss != loss):\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\n\n\n\n\n\n\n\n\nThis data shows the decision boundary improve over time, though not consistently (see the jump in loss between the second and third plots). Eventually, the loss converges on zero, plotting the decision boundary directly between the two groups.\nTo test the perceptron’s behavior on linearly inseparable data, I create new data that has a higher amount of noise. This will result in data that doesn’t have as clear groups, with outliers from one group that appear to be in the other group based on their values.\n\nX2, y2 = perceptron_data(noise = 0.35)\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X2, y2, ax)\n\n\n\n\n\n\n\n\nHere, we graph the decision boundary over time in a similar way to the linearly separable experiment. Since the data is linearly inseparable, though, the perceptron cannot achieve a loss of 0. As such, using the same function would result in an infinite loop and thus needs to break after a certain amount of iterations (here I chose 1000). For better visibility, only the first and last loss changes’ decision boundaries were plotted. The number of iterations the plot was created at are included in the plot’s label.\n\n# adapted from lecture 7 (Perceptron)\ntorch.manual_seed(634)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X2.size()[0]\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (12, 4)\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\niter = 0\nmax_steps = 1000\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    if (iter == max_steps):\n        break\n\n    ax = axarr.ravel()[current_ax]\n\n    if loss == 0:\n        break\n    \n    # not part of the update: just for tracking our progress    \n    prev_loss = loss\n\n    loss = p.loss(X2, y2) \n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X2[[i],:]\n    y_i = y2[i]\n\n    old_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w)\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    if (prev_loss != loss):\n        loss = p.loss(X2, y2).item()\n        loss_vec.append(loss)\n        \n        if iter == 0 or iter == 949:\n            plot_perceptron_data(X2, y2, ax)\n            draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n            draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n            \n            ax.scatter(X2[i,0],X2[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n            ax.set_title(f\"loss = {loss:.3f}, iter = {iter}\")\n            ax.set(xlim = (-1, 2), ylim = (-1, 2))\n            \n            if iter == 0:\n                current_ax += 1\n    \n    iter += 1\n\n\n\n\n\n\n\n\nHere we can see that, although the data is linearly inseparable, the loss still decreases immensely over the course of 1000 iterations. The decision boundary, then, gets much closer to separating the data over time. The loss can be seen more clearly by plotting its values over time.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Loss Vector Entry\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAgain, the loss values decrease over time, but not steadily. Instead of decreasing to 0, of course, the loss stays around 0 without converging to it.\nFor the final experiment on non-minibatch data, I tested the perceptron’s results on data with more than 2 dimensions. To test the progress of the experiment, I kept track of the loss values over time and printed them out at the end of the loop.\n\nX3, y3 = perceptron_data(p_dims = 5)\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\nloss_vec = []\n\nstep = 0\nmax_steps = 10000\n\nwhile loss &gt; 0:\n    if step == max_steps:\n        break\n\n    prev_loss = loss\n\n    loss = p.loss(X3, y3) \n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X3[[i],:]\n    y_i = y3[i]\n\n    opt.step(x_i, y_i)\n\n    if prev_loss != loss:\n        loss = p.loss(X3, y3).item()\n        loss_vec.append(loss)\n    step += 1\n\nprint(loss_vec, step)\n\n[0.25333333015441895, 0.23333333432674408, 0.2566666603088379, 0.009999999776482582, 0.0] 46\n\n\nThis shows that the perceptron can converge on 5-dimensional data that is linearly separable. As the printed loss vector shows, the perceptron’s loss converges to 0 in 5 changes of loss over the course of 46 steps. As such, the model correctly predicts data at 100% accuracy for the training data.\nNext, I set up a graphing function for testing perceptron minibatching. Again, this function is similar to the other decision graphing functions above. Instead of randomly choosing one index for the data, however, the function chooses a set of indices using torch’s randperm function. This function also uses my perceptron’s step_k and grad_k functions to test the minibatch on batches of size k. For reference, the grad_k implementation was described at the top of this post.\n\ndef graph_k(X, y, k, alpha, rows=5, cols=5, xdim=10, ydim=6, seed=3619):\n    # adapted from lecture 7 (Perceptron)\n    torch.manual_seed(seed)\n\n    # instantiate a model and an optimizer\n    p = Perceptron() \n    opt = PerceptronOptimizer(p)\n\n    loss = 1.0\n\n    # for keeping track of loss values\n    loss_vec = []\n\n    n = X.size()[0]\n\n    # set up the figure\n    plt.rcParams[\"figure.figsize\"] = (xdim, ydim)\n    fig, axarr = plt.subplots(rows, cols, sharex = True, sharey = True)\n    markers = [\"o\", \",\"]\n    marker_map = {-1 : 0, 1 : 1}\n\n    # initialize for main loop\n    current_ax = 0\n    loss = 1\n    loss_vec = []\n    iter =  0\n\n    while loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n        ax = axarr.ravel()[current_ax]\n\n        if loss == 0:\n            break\n        \n        # not part of the update: just for tracking our progress    \n        prev_loss = loss\n\n        loss = p.loss(X, y) \n\n        # pick a random data point\n        ix = torch.randperm(X.size(0))[:k]\n        x_i = X[ix,:]\n        y_i = y[ix]\n\n        old_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w)\n        \n        # perform a perceptron update using the random data point\n        opt.step_k(x_i, y_i, alpha, k)\n\n        if (prev_loss != loss):\n            plot_perceptron_data(X, y, ax)\n            draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n            loss = p.loss(X, y).item()\n            loss_vec.append(loss)\n            draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n            # try ix instead of i here\n            ax.scatter(X[ix,0],X[ix,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n            # draw_line(w, -10, 10, ax, color = \"black\")\n            ax.set_title(f\"loss = {loss:.3f}\")\n            ax.set(xlim = (-1, 2), ylim = (-1, 2))\n            current_ax += 1\n            iter += 1\n\nFirst, I graphed the minibatch on a batch size of 1. This was done to test the performance of the batch gradient and step functions, particularly in comparison to the non-batched perceptron functions. As the graph shows, the perceptron finds the separating line between the two groups roughly as quickly as the original algorithm.\n\ngraph_k(X, y, 1, 0.1, 2, 4)\n\n\n\n\n\n\n\n\nSecond, this test shows that a minibatch with a batch size of 10 converges on a two-dimensional, linearly-seperable dataset given enough time. This test notably takes more time to converge than a perceptron with a batch size of 1.\n\ngraph_k(X, y, 10, 0.1, rows=5, cols=5, xdim=16, ydim=14, seed=9000)\n\n\n\n\n\n\n\n\nFinally, to test the minibatch performance on linearly inseparable data, I created a block of code that terminates after 10000 steps, calling opt.step_k() with a batch size of n (the size of the data). After running this code for 10000, the loss is plotted over time.\n\n# adapted from lecture 7 (Perceptron)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X2.size()[0]\n\n# initialize for main loop\nloss = 1\nloss_vec = []\n\nmax_steps = 10000\nstep = 0\n\nk = X2.shape[0]\nalpha = 0.01\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    if (step &gt;= max_steps):\n        break\n\n    ax = axarr.ravel()[current_ax]\n\n    if loss == 0:\n        break\n        \n    # not part of the update: just for tracking our progress    \n    prev_loss = loss\n\n    loss = p.loss(X2, y2) \n\n    # pick a random data point\n    # i = torch.randint(n, size = (1,))\n    ix = torch.randperm(X2.size(0))[:k]\n    x_i = X2[ix,:]\n    y_i = y2[ix]\n\n    old_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w)\n        \n    # perform a perceptron update using the random data point\n    opt.step_k(x_i, y_i, alpha, k)\n\n    loss = p.loss(X2, y2).item()\n    loss_vec.append(loss)\n    step += 1\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Minibatch k = n Perceptron Iterations\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\n\nloss_vec[len(loss_vec) - 1]\n\n0.03999999910593033\n\n\nWhile this test doesn’t converge at 0, it does show that running the minibatch with a batchsize of n converges to a low loss value.\n\nloss = p.loss(X, y) # ====\n# y_ = 2*y - 1 # O(n)\n# (1.0*(self.score(X)*y_ &lt; 0)).mean() ==== \n# 1 * torch.matmul(X, torch.t(self.w)) * y &lt; 0 = O(n)\n\n# pick a random data point\ni = torch.randint(n, size = (1,)) # O(1)\nx_i = X[[i],:] # O(1)\ny_i = y[i] # O(1)\n\nold_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w) # O(1) OR O(p)\n    \n# perform a perceptron update using the random data point\nopt.step(x_i, y_i) # grad could be O(p)\n\n\nloss = p.loss(X, y) # O(np)\n\n# pick a random data point\nix = torch.randperm(X.size(0))[:k] #O(k)\nx_i = X[ix,:] # O(k)\ny_i = y[ix] # O(k)\n        \n# perform a perceptron update using the random data point\nopt.step_k(x_i, y_i, alpha, k)\n\n# self.model.loss(X, y) # for an X of dim k * p and a y of size k,\n# this has a runtime of O(kp)\n# self.model.w += self.model.grad_k(X, y, alpha, k)\n\n# s_i = torch.inner(torch.t(self.w), X) # p * (k*p) = O(kp) results in size k*p\n# return torch.sum(((1 * (s_i*y &lt; 0))[:, None]*y[:, None]*X), axis=0)\n# k*p*k * k * k * p\n\nloss = p.loss(X, y).item() # O(np)\n\n\n\n\n\nFor the runtime of the perceptron, a single iteration without minibatching will have a runtime of O(n) if n &gt; p and a runtime of O(p) if p &gt; n. The O(n) appears in the loss calculation’s matrix multiplication and the O(p) appears in the gradient calculation. In the minibatch implementation, the runtime is O(kp) (seen in the grad_k calculation step) if kp &gt; n and O(n) (seen in the matrix multiplication) if n &gt; kp.\n\n\n\nIn the creation and testing of the perceptron algorithm, I found that – while it seems to work well for linearly separable data – it can be a bit awkward to run on linearly inseparable data. On two dimensional data, the perceptron converged to zero quickly for linearly separable data. While it isn’t possible for this to occur on linearly inseparable data, it did progressively improve over time before plateauing before zero. This ability to achieve zero loss with linearly separable data holds at higher dimensions, as well. Similarly, I found that a minibatch perceptron could converge to zero on linearly separable data (though at slower rates than non-batched perceptrons at higher batch sizes) and to a low non-zero value on inseparable data with a large batch size."
  },
  {
    "objectID": "posts/perceptron-post/index.html#abstract",
    "href": "posts/perceptron-post/index.html#abstract",
    "title": "Perceptron",
    "section": "",
    "text": "This blog post aims to investigate the function of the perceptron algorithm by performing various experiments with it. These experiments includes tests of the algorithm on linearly separable and inseparable data and data with dimensions greater than 2. These tests will show how the algorithm functions of different data, graphing the perceptron’s loss and decision boundaries over time. Then, the algorithm will be tested for performance with minibatching, examining loss convergance at different batch sizes.\n\n\nhttps://github.com/jblake05/jblake05.github.io/blob/main/posts/perceptron-post/perceptron.py\n\n\n\nMy perceptron class has two implementations of the grad function, one with minibatching and one without. The grad function \\(1[s_iy_i &lt; 0]y_ix_i\\) is implemented by first calculating \\(s_i\\) as \\(\\langle w_i, x_i\\rangle\\) using torch.inner. Then I cast the expression \\(s_iy_i &lt; 0\\) to a boolean by multiplying it by 1. Finally I multiplied that expression by y_i and the vector x_i, flattening the expression to one dimension to add it to the weight.\nFor the minibatching function grade_k, I implemented the equation \\(\\frac{α}{k} \\sum \\limits _{l=1} ^{k} 1[\\langle w, x_{i_k} \\rangle y_{i_k} &lt; 0]y_{i_k} x_{i_k}\\) The score was formed the same way as the previous equation. The boolean map and y vector are resized from size n to size (n, 1) using y[:, None] to allow for multiplication with X. This matrix is then summed along each column to create a tensor the same size as the weight. Finally, \\(\\frac{α}{k}\\) is multiplied by the tensor to complete the equation.\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nfrom perceptron import Perceptron, PerceptronOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\n\nTo test the perceptron, I first generated data to test using code from the perceptron lecture. Also using code from the lecture, I plotted the data, differentiating the groups of X (i.e. if they corrolate with y = 1 or -1) by color.\n\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nTo test my perceptron, I ran the minimal training loop provided in the perceptron lecture. In doing so, I generate random values of X and y to act as inputs for the perceptron’s optimizer. I also collect a vector of all unique (i.e. different from the previous) loss values, which are graphed below.\n\ntorch.manual_seed(130)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    if loss == 0:\n        break\n    \n    # not part of the update: just for tracking our progress    \n    prev_loss = loss\n\n    loss = p.loss(X, y) \n    if (prev_loss != loss):\n        loss_vec.append(loss)\n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nHere, we see that the perceptron’s loss converges to 0 on linearly separable data. Due to the randomly indexed data, the loss does not decrease monotonically, but trends lower over time.\nSimilar to the previous test, this experiment tests the perceptron’s performance on a 2D dataset that is linearly separable. Instead of just plotting the loss, this code uses the draw_line function to show the decision boundary change over time relative to the dataset.\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nLike the last experiment, this block of code keeps track of the loss over time, graphing the dataset, x_i, and decision boundary at each step.\n\n# adapted from lecture 7 (Perceptron)\ntorch.manual_seed(3619)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (10, 6)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    ax = axarr.ravel()[current_ax]\n\n    if loss == 0:\n        break\n    \n    # not part of the update: just for tracking our progress    \n    prev_loss = loss\n\n    loss = p.loss(X, y) \n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    old_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w)\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n\n    if (prev_loss != loss):\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\n\n\n\n\n\n\n\n\nThis data shows the decision boundary improve over time, though not consistently (see the jump in loss between the second and third plots). Eventually, the loss converges on zero, plotting the decision boundary directly between the two groups.\nTo test the perceptron’s behavior on linearly inseparable data, I create new data that has a higher amount of noise. This will result in data that doesn’t have as clear groups, with outliers from one group that appear to be in the other group based on their values.\n\nX2, y2 = perceptron_data(noise = 0.35)\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X2, y2, ax)\n\n\n\n\n\n\n\n\nHere, we graph the decision boundary over time in a similar way to the linearly separable experiment. Since the data is linearly inseparable, though, the perceptron cannot achieve a loss of 0. As such, using the same function would result in an infinite loop and thus needs to break after a certain amount of iterations (here I chose 1000). For better visibility, only the first and last loss changes’ decision boundaries were plotted. The number of iterations the plot was created at are included in the plot’s label.\n\n# adapted from lecture 7 (Perceptron)\ntorch.manual_seed(634)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X2.size()[0]\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (12, 4)\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\niter = 0\nmax_steps = 1000\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    if (iter == max_steps):\n        break\n\n    ax = axarr.ravel()[current_ax]\n\n    if loss == 0:\n        break\n    \n    # not part of the update: just for tracking our progress    \n    prev_loss = loss\n\n    loss = p.loss(X2, y2) \n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X2[[i],:]\n    y_i = y2[i]\n\n    old_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w)\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    if (prev_loss != loss):\n        loss = p.loss(X2, y2).item()\n        loss_vec.append(loss)\n        \n        if iter == 0 or iter == 949:\n            plot_perceptron_data(X2, y2, ax)\n            draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n            draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n            \n            ax.scatter(X2[i,0],X2[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n            ax.set_title(f\"loss = {loss:.3f}, iter = {iter}\")\n            ax.set(xlim = (-1, 2), ylim = (-1, 2))\n            \n            if iter == 0:\n                current_ax += 1\n    \n    iter += 1\n\n\n\n\n\n\n\n\nHere we can see that, although the data is linearly inseparable, the loss still decreases immensely over the course of 1000 iterations. The decision boundary, then, gets much closer to separating the data over time. The loss can be seen more clearly by plotting its values over time.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Loss Vector Entry\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAgain, the loss values decrease over time, but not steadily. Instead of decreasing to 0, of course, the loss stays around 0 without converging to it.\nFor the final experiment on non-minibatch data, I tested the perceptron’s results on data with more than 2 dimensions. To test the progress of the experiment, I kept track of the loss values over time and printed them out at the end of the loop.\n\nX3, y3 = perceptron_data(p_dims = 5)\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\nloss_vec = []\n\nstep = 0\nmax_steps = 10000\n\nwhile loss &gt; 0:\n    if step == max_steps:\n        break\n\n    prev_loss = loss\n\n    loss = p.loss(X3, y3) \n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X3[[i],:]\n    y_i = y3[i]\n\n    opt.step(x_i, y_i)\n\n    if prev_loss != loss:\n        loss = p.loss(X3, y3).item()\n        loss_vec.append(loss)\n    step += 1\n\nprint(loss_vec, step)\n\n[0.25333333015441895, 0.23333333432674408, 0.2566666603088379, 0.009999999776482582, 0.0] 46\n\n\nThis shows that the perceptron can converge on 5-dimensional data that is linearly separable. As the printed loss vector shows, the perceptron’s loss converges to 0 in 5 changes of loss over the course of 46 steps. As such, the model correctly predicts data at 100% accuracy for the training data.\nNext, I set up a graphing function for testing perceptron minibatching. Again, this function is similar to the other decision graphing functions above. Instead of randomly choosing one index for the data, however, the function chooses a set of indices using torch’s randperm function. This function also uses my perceptron’s step_k and grad_k functions to test the minibatch on batches of size k. For reference, the grad_k implementation was described at the top of this post.\n\ndef graph_k(X, y, k, alpha, rows=5, cols=5, xdim=10, ydim=6, seed=3619):\n    # adapted from lecture 7 (Perceptron)\n    torch.manual_seed(seed)\n\n    # instantiate a model and an optimizer\n    p = Perceptron() \n    opt = PerceptronOptimizer(p)\n\n    loss = 1.0\n\n    # for keeping track of loss values\n    loss_vec = []\n\n    n = X.size()[0]\n\n    # set up the figure\n    plt.rcParams[\"figure.figsize\"] = (xdim, ydim)\n    fig, axarr = plt.subplots(rows, cols, sharex = True, sharey = True)\n    markers = [\"o\", \",\"]\n    marker_map = {-1 : 0, 1 : 1}\n\n    # initialize for main loop\n    current_ax = 0\n    loss = 1\n    loss_vec = []\n    iter =  0\n\n    while loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n        ax = axarr.ravel()[current_ax]\n\n        if loss == 0:\n            break\n        \n        # not part of the update: just for tracking our progress    \n        prev_loss = loss\n\n        loss = p.loss(X, y) \n\n        # pick a random data point\n        ix = torch.randperm(X.size(0))[:k]\n        x_i = X[ix,:]\n        y_i = y[ix]\n\n        old_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w)\n        \n        # perform a perceptron update using the random data point\n        opt.step_k(x_i, y_i, alpha, k)\n\n        if (prev_loss != loss):\n            plot_perceptron_data(X, y, ax)\n            draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n            loss = p.loss(X, y).item()\n            loss_vec.append(loss)\n            draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n            # try ix instead of i here\n            ax.scatter(X[ix,0],X[ix,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n            # draw_line(w, -10, 10, ax, color = \"black\")\n            ax.set_title(f\"loss = {loss:.3f}\")\n            ax.set(xlim = (-1, 2), ylim = (-1, 2))\n            current_ax += 1\n            iter += 1\n\nFirst, I graphed the minibatch on a batch size of 1. This was done to test the performance of the batch gradient and step functions, particularly in comparison to the non-batched perceptron functions. As the graph shows, the perceptron finds the separating line between the two groups roughly as quickly as the original algorithm.\n\ngraph_k(X, y, 1, 0.1, 2, 4)\n\n\n\n\n\n\n\n\nSecond, this test shows that a minibatch with a batch size of 10 converges on a two-dimensional, linearly-seperable dataset given enough time. This test notably takes more time to converge than a perceptron with a batch size of 1.\n\ngraph_k(X, y, 10, 0.1, rows=5, cols=5, xdim=16, ydim=14, seed=9000)\n\n\n\n\n\n\n\n\nFinally, to test the minibatch performance on linearly inseparable data, I created a block of code that terminates after 10000 steps, calling opt.step_k() with a batch size of n (the size of the data). After running this code for 10000, the loss is plotted over time.\n\n# adapted from lecture 7 (Perceptron)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X2.size()[0]\n\n# initialize for main loop\nloss = 1\nloss_vec = []\n\nmax_steps = 10000\nstep = 0\n\nk = X2.shape[0]\nalpha = 0.01\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    if (step &gt;= max_steps):\n        break\n\n    ax = axarr.ravel()[current_ax]\n\n    if loss == 0:\n        break\n        \n    # not part of the update: just for tracking our progress    \n    prev_loss = loss\n\n    loss = p.loss(X2, y2) \n\n    # pick a random data point\n    # i = torch.randint(n, size = (1,))\n    ix = torch.randperm(X2.size(0))[:k]\n    x_i = X2[ix,:]\n    y_i = y2[ix]\n\n    old_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w)\n        \n    # perform a perceptron update using the random data point\n    opt.step_k(x_i, y_i, alpha, k)\n\n    loss = p.loss(X2, y2).item()\n    loss_vec.append(loss)\n    step += 1\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Minibatch k = n Perceptron Iterations\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\n\nloss_vec[len(loss_vec) - 1]\n\n0.03999999910593033\n\n\nWhile this test doesn’t converge at 0, it does show that running the minibatch with a batchsize of n converges to a low loss value.\n\nloss = p.loss(X, y) # ====\n# y_ = 2*y - 1 # O(n)\n# (1.0*(self.score(X)*y_ &lt; 0)).mean() ==== \n# 1 * torch.matmul(X, torch.t(self.w)) * y &lt; 0 = O(n)\n\n# pick a random data point\ni = torch.randint(n, size = (1,)) # O(1)\nx_i = X[[i],:] # O(1)\ny_i = y[i] # O(1)\n\nold_w = torch.tensor([0, 0]) if p.w == None else torch.clone(p.w) # O(1) OR O(p)\n    \n# perform a perceptron update using the random data point\nopt.step(x_i, y_i) # grad could be O(p)\n\n\nloss = p.loss(X, y) # O(np)\n\n# pick a random data point\nix = torch.randperm(X.size(0))[:k] #O(k)\nx_i = X[ix,:] # O(k)\ny_i = y[ix] # O(k)\n        \n# perform a perceptron update using the random data point\nopt.step_k(x_i, y_i, alpha, k)\n\n# self.model.loss(X, y) # for an X of dim k * p and a y of size k,\n# this has a runtime of O(kp)\n# self.model.w += self.model.grad_k(X, y, alpha, k)\n\n# s_i = torch.inner(torch.t(self.w), X) # p * (k*p) = O(kp) results in size k*p\n# return torch.sum(((1 * (s_i*y &lt; 0))[:, None]*y[:, None]*X), axis=0)\n# k*p*k * k * k * p\n\nloss = p.loss(X, y).item() # O(np)"
  },
  {
    "objectID": "posts/perceptron-post/index.html#runtime-discussion",
    "href": "posts/perceptron-post/index.html#runtime-discussion",
    "title": "Perceptron",
    "section": "",
    "text": "For the runtime of the perceptron, a single iteration without minibatching will have a runtime of O(n) if n &gt; p and a runtime of O(p) if p &gt; n. The O(n) appears in the loss calculation’s matrix multiplication and the O(p) appears in the gradient calculation. In the minibatch implementation, the runtime is O(kp) (seen in the grad_k calculation step) if kp &gt; n and O(n) (seen in the matrix multiplication) if n &gt; kp."
  },
  {
    "objectID": "posts/perceptron-post/index.html#conclusion",
    "href": "posts/perceptron-post/index.html#conclusion",
    "title": "Perceptron",
    "section": "",
    "text": "In the creation and testing of the perceptron algorithm, I found that – while it seems to work well for linearly separable data – it can be a bit awkward to run on linearly inseparable data. On two dimensional data, the perceptron converged to zero quickly for linearly separable data. While it isn’t possible for this to occur on linearly inseparable data, it did progressively improve over time before plateauing before zero. This ability to achieve zero loss with linearly separable data holds at higher dimensions, as well. Similarly, I found that a minibatch perceptron could converge to zero on linearly separable data (though at slower rates than non-batched perceptrons at higher batch sizes) and to a low non-zero value on inseparable data with a large batch size."
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html",
    "href": "posts/optimal-decision-making/optimal.html",
    "title": "\"Optimal\" Decision Making",
    "section": "",
    "text": "This blog post will explore the data behind loan granting from various perspectives. The data will first be shown “as is” through visualizations and data tables. Then, a model will be trained on the data through the creation of a scoring function and threshold. Finally, more in-depth quantiative analysis will be done on the profits, costs, and biases that are created from the model. Through this study, a profitable model for the bank was found, but this came at the cost of differing acceptance rates across groups."
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#introduction",
    "href": "posts/optimal-decision-making/optimal.html#introduction",
    "title": "\"Optimal\" Decision Making",
    "section": "",
    "text": "This blog post will explore the data behind loan granting from various perspectives. The data will first be shown “as is” through visualizations and data tables. Then, a model will be trained on the data through the creation of a scoring function and threshold. Finally, more in-depth quantiative analysis will be done on the profits, costs, and biases that are created from the model. Through this study, a profitable model for the bank was found, but this came at the cost of differing acceptance rates across groups."
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#part-b-data-exploration",
    "href": "posts/optimal-decision-making/optimal.html#part-b-data-exploration",
    "title": "\"Optimal\" Decision Making",
    "section": "Part B: Data Exploration",
    "text": "Part B: Data Exploration\nFirst, the data is converted to a dataframe through pandas and checked using the head() function\n\nimport pandas as pd\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nimport warnings\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\nI first investigated the relationship between personal income, financial history, and loan amounts. I did this by creating a table that groups by income intervals of 10000 and checking credit history and the size of the loan. It shows that loan amounts granted increase as personal income increases, while one’s credit history tends to be higher as income increases, but this isn’t as drastic of an upward trend.\n\ndf_train[\"income_group\"] = df_train[\"person_income\"] // 10000\n\ndf_train.groupby([\"income_group\"])[[\"loan_amnt\", \"cb_person_cred_hist_length\"]].aggregate(\"mean\")[:20]\n\n\n\n\n\n\n\n\nloan_amnt\ncb_person_cred_hist_length\n\n\nincome_group\n\n\n\n\n\n\n0\n2093.333333\n5.566667\n\n\n1\n3837.235367\n5.282690\n\n\n2\n5731.833494\n5.532243\n\n\n3\n7298.140863\n5.541371\n\n\n4\n8220.600000\n5.630667\n\n\n5\n9237.460703\n5.595313\n\n\n6\n10044.702093\n5.689211\n\n\n7\n11152.793002\n5.836003\n\n\n8\n11724.719801\n5.957659\n\n\n9\n12059.118852\n6.175410\n\n\n10\n13020.365006\n6.000000\n\n\n11\n12653.756477\n6.257340\n\n\n12\n13438.675214\n6.220513\n\n\n13\n14132.446809\n5.984802\n\n\n14\n14319.277108\n6.799197\n\n\n15\n15255.932203\n6.673729\n\n\n16\n16399.778761\n7.106195\n\n\n17\n15436.965812\n7.427350\n\n\n18\n14102.445652\n6.923913\n\n\n19\n15325.520833\n6.666667\n\n\n\n\n\n\n\n\nData Visualizations\nI then mapped aspects about the loan with respect to whether or not the grantee has previously defaulted on their loan. I mapped this as a seaborn scatterplot using loan interest rate and percent income. This revealed that those with a prior default on history are almost never granted an interest rate lower than 12%. Further, it showed that there isn’t much of a relationship between loan as a percentage of income and previous defaults being on file.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ng1 = sns.scatterplot(data=df_train[:1000], x=\"loan_int_rate\", y=\"loan_percent_income\", hue=\"cb_person_default_on_file\")\ng1.set_title(\"Relation Between Loan Interest Rate, Percentage of Income, and Previous Defaults\")\ng1.set_xlabel(\"Loan Interest Rate\")\ng1.set_ylabel(\"Loan as Percentage of Income\")\nplt.legend(title=\"Previous Default on File\")\n                     \n\n\n\n\n\n\n\n\nFinally, I looked into the impact one’s age and home ownership status has on their loan status (whether or not they default) using a seaborn lineplot. It shows that defaulting on loans for those who rent stay fairly stable (and high) from around 20-70 years old. The default rate is much lower for those with mortgages than those who pay rent, and it tends to go down with age. While the rate of default for mortgages jumps up at 60, there are only 3 datapoints for this, so it may be an outlier. Home ownership default rates are the lowest, also trending downward over time (reaching a 0% default rate at 60). Finally, the “other” category fluctuates wildly, increasing drastically between those in their 20s and 30s, then decreasing even more between those in their 30s and 40s. No data exists on “other” home ownership exists in this data for those above their 40s.\n\ndf_train[\"age_group\"] = df_train[\"person_age\"] // 10\n\n# axis limiting information from: https://stackoverflow.com/questions/54822884/how-to-change-the-x-axis-range-in-seaborn\nfig, ax = plt.subplots()\n\ng2 = sns.barplot(data=df_train, x=\"age_group\", y=\"loan_status\", hue=\"person_home_ownership\", errorbar=None, ax=ax)\n\ng2.set_title(\"Chance of Default by Age Group and Home Ownership\")\ng2.set_xlabel(\"Age Group (Tens of Years)\")\ng2.set_ylabel(\"Loan Status (Average Chance of Default)\")\nplt.legend(title=\"Home Ownership Status\")\n\nax.set_xlim(-1, 6)\nplt.show()"
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#part-c-model-creation",
    "href": "posts/optimal-decision-making/optimal.html#part-c-model-creation",
    "title": "\"Optimal\" Decision Making",
    "section": "Part C: Model Creation",
    "text": "Part C: Model Creation\n\nData Preparation\nI then wanted to create a model to predict whether or not an applicant would default on their loan. To start, I processed the data to drop loan_grade (which was already a prediction) and loan_status (what the model is predicting). Further, I use pd.get_dummies to convert qualitative columns like “person_home_ownership” to become true/false columns for each possible answer of the column (e.g. person_home_ownership_RENT).\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df_train[\"loan_status\"])\n\ndef prepare_data(df):\n  df = df.drop([\"loan_grade\"], axis = 1)\n  df = df.dropna()\n  y = le.transform(df[\"loan_status\"])\n  df = df.drop([\"loan_status\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(df_train)\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nincome_group\nage_group\nperson_home_ownership_MORTGAGE\n...\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\n9\n2\nFalse\n...\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\n3\n2\nFalse\n...\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\n2\n2\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\n5\n2\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\n2\n2\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n5 rows × 21 columns\n\n\n\n\n\nCross Validation and Parameter Choice\nI then used a logistic regression (LR) model from SciKitLearn to figure out the most important variables for the model to consider and the weights of their importance. I did this by procedurally iterating through one qualitative column and different pairs of quantitative column and scoring the LR model through cross-validation. The best scoring columns were saved and output for future use.\n\nwarnings.filterwarnings(\"ignore\") # just ignores the convergence warning in this case\n\nresults_LR = []\n\nall_qual_cols = [\"person_home_ownership\", \"loan_intent\", \"cb_person_default_on_file\"]\nall_quant_cols = [\"person_age\", \"person_income\", \"person_emp_length\", \"loan_amnt\",\n                  \"loan_int_rate\", \"loan_percent_income\", \"cb_person_cred_hist_length\"]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\n    LR = LogisticRegression()\n\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    new_score_LR = cv_scores_LR.mean()\n    results_LR.append((new_score_LR, cols))\n\nresults_LR.sort(reverse=True, key=lambda x : x[0])\nprint(results_LR)\n\n[(0.8488229950993185, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'loan_percent_income']), (0.8486483607400082, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_percent_income', 'cb_person_cred_hist_length']), (0.8484738026069572, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_emp_length', 'loan_percent_income']), (0.8455928025641752, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_int_rate', 'loan_percent_income']), (0.8316669701424602, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_int_rate', 'loan_percent_income']), (0.8313616744469856, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'loan_amnt']), (0.8313612266177142, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_amnt', 'loan_percent_income']), (0.82359095998493, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_int_rate', 'loan_percent_income']), (0.8208840988307748, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_percent_income', 'cb_person_cred_hist_length']), (0.8208404497692298, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'loan_percent_income']), (0.8207968197642493, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_emp_length', 'loan_percent_income']), (0.8199676876888209, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'loan_percent_income']), (0.818526754130582, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'loan_percent_income']), (0.8179595259527067, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'loan_int_rate']), (0.8172610552134426, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_emp_length', 'loan_int_rate']), (0.816475362577347, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_emp_length', 'loan_int_rate']), (0.8160825305517229, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'loan_int_rate']), (0.8156022479504903, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'loan_int_rate']), (0.8154275850063331, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_int_rate', 'cb_person_cred_hist_length']), (0.8154274992517916, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_int_rate', 'cb_person_cred_hist_length']), (0.8153399724499243, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_amnt', 'loan_percent_income']), (0.8152528363076345, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'loan_int_rate']), (0.8151218795947164, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_emp_length', 'loan_percent_income']), (0.8146855128469355, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_amnt', 'loan_int_rate']), (0.8135071596942984, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_amnt', 'loan_percent_income']), (0.8134193089308305, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'loan_percent_income']), (0.8133756979824149, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_percent_income', 'cb_person_cred_hist_length']), (0.812721190738014, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_amnt', 'loan_int_rate']), (0.8117169193043094, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'loan_int_rate']), (0.8080935707819414, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'loan_amnt']), (0.8035536110236892, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_emp_length', 'loan_int_rate']), (0.8004103450086235, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'loan_amnt']), (0.7985767604621254, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'loan_int_rate']), (0.7984894528107527, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_int_rate', 'cb_person_cred_hist_length']), (0.7968743327224751, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_amnt', 'loan_int_rate']), (0.7925530661012202, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'loan_percent_income']), (0.7907191194799921, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'cb_person_cred_hist_length']), (0.7904572060541563, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'person_income']), (0.7898459476834412, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_emp_length', 'loan_amnt']), (0.7896713704938252, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_income', 'person_emp_length']), (0.7894529727335813, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'loan_amnt']), (0.7890163963635881, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_amnt', 'cb_person_cred_hist_length']), (0.7878378621735861, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'person_emp_length']), (0.7868337622489643, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'person_income']), (0.7866591660027835, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_income', 'cb_person_cred_hist_length']), (0.7860043347967822, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_emp_length', 'loan_amnt']), (0.7857860609042093, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'person_income']), (0.7853930859543494, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_amnt', 'cb_person_cred_hist_length']), (0.7853930668977847, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'loan_amnt']), (0.7850001586459016, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_emp_length', 'loan_amnt']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'person_emp_length']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'cb_person_cred_hist_length']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'person_emp_length']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_income', 'cb_person_cred_hist_length']), (0.7849565667540506, ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_emp_length', 'cb_person_cred_hist_length']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'person_emp_length']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'cb_person_cred_hist_length']), (0.7849565667540506, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_emp_length', 'cb_person_cred_hist_length']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'person_emp_length']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_age', 'cb_person_cred_hist_length']), (0.7849565667540506, ['cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'person_emp_length', 'cb_person_cred_hist_length']), (0.7847382738049129, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_amnt', 'cb_person_cred_hist_length']), (0.7846946247433676, ['loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'person_age', 'loan_amnt'])]\n\n\nWith the columns defined, we can get the weights of each column using LogisticRegression.coef_.\n\ncols = ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN',\n         'person_home_ownership_RENT', 'loan_percent_income', 'cb_person_cred_hist_length']\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\n\nLR.coef_\n\narray([[-7.67155908e-01, -1.12954673e-01, -1.80828780e+00,\n         2.68210537e-01,  8.27833450e+00, -4.48028544e-03]])\n\n\nA simple linear score function is defined here as the inner product between the values of the relevant columns (X) and the values of w (our weights, defined above). X_train is then given profit, cost, and score columns based on the profit and cost functions provided.\n\ndef linear_score(w, X):\n    return X@w\n\n\nX_train[\"profit\"] = X_train[\"loan_amnt\"]*(1 + 0.25*0.01*X_train[\"loan_int_rate\"])**10 - X_train[\"loan_amnt\"]\n\nX_train[\"cost\"] = X_train[\"loan_amnt\"]*(1 + 0.25*0.01*X_train[\"loan_int_rate\"])**3 - 1.7*X_train[\"loan_amnt\"]\n\nX_train[\"score\"] = linear_score(np.transpose(LR.coef_), X_train[cols])\n\nThe model is then optimized for maximum profit for the bank. This algorithm checks for each threshold value between the minimum and maximum scores in the training data. The profit for each threshold is calculated as 0 if the model predicts a default (the loan being denied), the value of the profit column if the value of loan_status is 0 (no default), and the value of the cost column is the value of loan_status is 1.\n\nX_train[\"score\"].min(), X_train[\"score\"].max()\n\n(-1.7747875900109253, 6.633567528408647)"
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#part-d-threshold-choice",
    "href": "posts/optimal-decision-making/optimal.html#part-d-threshold-choice",
    "title": "\"Optimal\" Decision Making",
    "section": "Part D: Threshold Choice",
    "text": "Part D: Threshold Choice\n\nmax_profit = float('-inf')\nthresh = -1.17\n\nfor t in np.arange(-1.17, 7.23, 0.01, dtype=float):\n    temp_prof = ((X_train[\"score\"] &lt; t) * ((y_train == 0) * X_train[\"profit\"] + (y_train == 1) * X_train[\"cost\"])).mean()\n    if (temp_prof &gt; max_profit):\n        max_profit = temp_prof\n        thresh = t\n\nmax_profit, thresh\n\n(1447.7771963146645, 2.7500000000000036)\n\n\n\nsns.scatterplot(data=X_train, x=\"profit\", y=\"cost\")"
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#part-e-evaluating-profit",
    "href": "posts/optimal-decision-making/optimal.html#part-e-evaluating-profit",
    "title": "\"Optimal\" Decision Making",
    "section": "Part E: Evaluating Profit",
    "text": "Part E: Evaluating Profit\nWith a max profit of $1448 and a threshold of 3.35, I apply the scoring function and threshold to the testing data. This threshold and set of weights achieved a testing profit of $1392.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\nX_test, y_test = prepare_data(df_test)\n\n\nthresh = 3.35\n\nX_test[\"profit\"] = X_test[\"loan_amnt\"]*(1 + 0.25*0.01*X_test[\"loan_int_rate\"])**10 - X_test[\"loan_amnt\"]\nX_test[\"cost\"] = X_test[\"loan_amnt\"]*(1 + 0.25*0.01*X_test[\"loan_int_rate\"])**3 - 1.7*X_test[\"loan_amnt\"]\n\nX_test[\"score\"] = linear_score(np.transpose(LR.coef_), X_test[cols])\n\n# if the score is under the threshold, the person is predicted not to default, so the loan is given\n((X_test[\"score\"] &lt; thresh) * ((y_test == 0) * X_test[\"profit\"] + (y_test == 1) * X_test[\"cost\"])).mean()\n\n1057.8815263645538\n\n\nI also added a “loan_granted” column, which just negates the prediction of the model (i.e. if no default is predicted, the loan is granted). This makes some of the logic easier by preventing negations of the prediction column later on.\n\n# prediction is whether or not a default will occur\nX_test[\"pred\"] = X_test[\"score\"] &gt; thresh\nX_test[\"loan_granted\"] = ~X_test[\"pred\"]\nX_test\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\nprofit\ncost\nscore\npred\nloan_granted\n\n\n\n\n0\n21\n42000\n5.0\n1000\n15.58\n0.02\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nTrue\nFalse\n465.367227\n-578.539601\n0.415856\nFalse\nTrue\n\n\n1\n32\n51000\n2.0\n15000\n11.36\n0.29\n9\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\n4847.780062\n-9185.361205\n1.593239\nFalse\nTrue\n\n\n2\n35\n54084\n2.0\n3000\n12.61\n0.06\n6\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\n1091.841800\n-1807.236578\n0.738029\nFalse\nTrue\n\n\n3\n28\n66300\n11.0\n12000\n14.11\n0.15\n6\nTrue\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nTrue\nFalse\n4972.214553\n-7084.777554\n0.447713\nFalse\nTrue\n\n\n4\n22\n70550\n0.0\n7000\n15.88\n0.08\n3\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nTrue\nFalse\n3331.859215\n-4032.764115\n0.917036\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6511\n29\n78000\n2.0\n18000\n6.62\n0.23\n5\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nTrue\nFalse\n3210.941787\n-11691.427669\n1.11446\nFalse\nTrue\n\n\n6513\n27\n44640\n0.0\n12800\n11.83\n0.29\n9\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nTrue\nFalse\n4331.281644\n-7790.401145\n2.628605\nFalse\nTrue\n\n\n6514\n24\n48000\n5.0\n10400\n7.37\n0.22\n3\nFalse\nFalse\nTrue\n...\nFalse\nFalse\nTrue\nTrue\nFalse\n2083.140437\n-6694.483153\n-0.000495\nFalse\nTrue\n\n\n6515\n26\n65000\n6.0\n6000\n9.07\n0.09\n3\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\n1508.058449\n-3782.525248\n-0.035547\nFalse\nTrue\n\n\n6516\n29\n61000\n12.0\n10000\n16.07\n0.16\n9\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nTrue\nFalse\n4827.369675\n-5745.680644\n1.552421\nFalse\nTrue\n\n\n\n\n5731 rows × 24 columns"
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#part-f-evaluating-from-a-borrowers-perspective",
    "href": "posts/optimal-decision-making/optimal.html#part-f-evaluating-from-a-borrowers-perspective",
    "title": "\"Optimal\" Decision Making",
    "section": "Part F: Evaluating from a Borrower’s Perspective",
    "text": "Part F: Evaluating from a Borrower’s Perspective\nWith a complete model, it’s important to do analysis on its impact. First, we can see through the creation of an age group column that acceptance rates tend to rise from ages 20 to 44. After this, we see a dip at 45-54 year olds before the rate rises again. Another large outlier is those in group 13 (the 65-69 year old age range), with an acceptance rate of 60%.\n\n# F1\nX_test[\"age_group\"] = X_test[\"person_age\"] // 5\nX_test.groupby(\"age_group\")[\"loan_granted\"].mean()\n\nage_group\n4     0.954733\n5     0.966223\n6     0.967136\n7     0.981043\n8     0.973822\n9     0.971014\n10    0.937500\n11    1.000000\n12    1.000000\n13    0.600000\n14    1.000000\nName: loan_granted, dtype: float64\n\n\nOne reason for these large flucuations and 100% acceptance rates, however, could be the low count of applicants in the higher age groups. Although group 13 has the lowest acceptance rate, they also have the lowest sample size with just 5 people. This rise is still notable, especially in groups with larger sample sizes like those in the 20-24 range compared to those in the 40-44 range.\n\nX_test.groupby(\"age_group\")[\"age_group\"].count()\n\nage_group\n4     2187\n5     1954\n6      852\n7      422\n8      191\n9       69\n10      32\n11      10\n12       7\n13       5\n14       2\nName: age_group, dtype: int64\n\n\nThe next test compares the model’s acceptance of those based on loan intent, comparing to its average acceptance rate. Here, we can see that loans for debt consolidaiton, medical expenses, and personal expenses are denied more often than average (where education, home improvement, and venture expenses are approved more often). Here, it is also notable that the model approves loans (i.e. sees no default) much more often than the data dictates that no default will occur.\n\n# F2\nintent_cols = [col for col in X_train.columns if \"loan_intent\" in col ]\n\nmodel_average = X_test[\"loan_granted\"].mean()\nprint(f'Model average acceptance: {round(model_average, 6)}\\n')\n\nsum = 0\nfor intent in intent_cols:\n    intent_count = X_test[intent].sum()\n    model_score = (X_test[intent] & X_test[\"loan_granted\"]).sum()/intent_count\n    result = (X_test[intent] & ~y_test).sum()/intent_count\n\n    sum += intent_count\n    print(f'{intent[12:]}:')\n    print(f'Model grant amount: {round(model_score, 6)}')\n    print(f'Actual grant amount: {round(result, 6)}')\n    print(f'Model\\'s group grant to averge grant ratio: {round(model_score/model_average, 6)}')\n    print(\"\")\n\nModel average acceptance: 0.963008\n\nDEBTCONSOLIDATION:\nModel grant amount: 0.961283\nActual grant amount: 0.712389\nModel's group grant to averge grant ratio: 0.998209\n\nEDUCATION:\nModel grant amount: 0.966837\nActual grant amount: 0.832483\nModel's group grant to averge grant ratio: 1.003976\n\nHOMEIMPROVEMENT:\nModel grant amount: 0.983766\nActual grant amount: 0.75\nModel's group grant to averge grant ratio: 1.021555\n\nMEDICAL:\nModel grant amount: 0.951538\nActual grant amount: 0.71575\nModel's group grant to averge grant ratio: 0.988089\n\nPERSONAL:\nModel grant amount: 0.957916\nActual grant amount: 0.779559\nModel's group grant to averge grant ratio: 0.994712\n\nVENTURE:\nModel grant amount: 0.96473\nActual grant amount: 0.853734\nModel's group grant to averge grant ratio: 1.001788\n\n\n\nFinally, I checked how the model grants loans to those of different income groups. I did this by (again) dividing the incomes into groups of 10000. When sorting by these groups, it becomes clear that the likelihood of being granted a loan by this model rises drastically as one’s income does. Above an income of 110000, this likelihood is 100% (the amount of income groups included is truncated for readability).\n\n# F3\nX_test[\"income_group\"] = X_test[\"person_income\"] // 10000\nX_test.groupby(\"income_group\")[\"loan_granted\"].mean()[:20]\n\nincome_group\n0     0.727273\n1     0.914729\n2     0.906926\n3     0.908788\n4     0.955900\n5     0.982804\n6     0.983607\n7     0.986328\n8     0.985251\n9     1.000000\n10    1.000000\n11    1.000000\n12    1.000000\n13    1.000000\n14    1.000000\n15    1.000000\n16    1.000000\n17    1.000000\n18    1.000000\n19    1.000000\nName: loan_granted, dtype: float64"
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#medical-credit",
    "href": "posts/optimal-decision-making/optimal.html#medical-credit",
    "title": "\"Optimal\" Decision Making",
    "section": "Medical Credit",
    "text": "Medical Credit\nI would say that it is not fair for those seeking loans for medical expenses to obtain access to credit. Here, I’m defining fairness in a line with a middle view of equality of opportunity; I tend to view these problems from the broad sense, but since we’re the taking the view of the decision maker, a broad approach is less applicabale. Specifically, we should act “to avoid perpetuating injustice” (from “Relative Notions of Fairness” in Fairness and Machine Learning).\nThe middle view here would counteract the unfairness of the medical (particularly healthcare/insurance) system, and the ability of many to access it. The decision maker (the bank) should then act in spite of the situation that left the applicants medically vulnerable and in support of their health and safety."
  },
  {
    "objectID": "posts/optimal-decision-making/optimal.html#conclusion",
    "href": "posts/optimal-decision-making/optimal.html#conclusion",
    "title": "\"Optimal\" Decision Making",
    "section": "Conclusion",
    "text": "Conclusion\nThrough the creation of this model, I found that the creation of a model that maximizes profit will often deprioritize the health and safety of those affected by it. This was seen in the investigation of different groups the model decides on. Those with medical expenses and less income, for instance, are disproportionately denied by this model. Maximizing for profit, then, shouldn’t be seen as the total goal for decision makers. Through this investigation, I also learned that those with previous credit issues are saddled with higher interest rates, increasing the risk of default occuring again. Beyond domain specific findings, I gained practice in thresholding, weighting, and building decision-making models."
  },
  {
    "objectID": "posts/Logistic/index.html",
    "href": "posts/Logistic/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/Logistic/index.html#abstract",
    "href": "posts/Logistic/index.html#abstract",
    "title": "Logistic Regression",
    "section": "Abstract",
    "text": "Abstract\nThis blog post explores the logistic regression model. This first involved the creation of the model, which is documented above. Then, I use the logistic regression model in a variety of experiments, including those with differing momentum and dimensionality values. Specifically, these experiments are a default test (no momentum, 2-dimensions), a test with momentum (keeping other values equal), and a test with significantly more dimensions than data points (p = 100, n = 50). By doing so, we can explore how the logistic regression model can be successfully used and how it reacts to different parameters."
  },
  {
    "objectID": "posts/Logistic/index.html#data-creation",
    "href": "posts/Logistic/index.html#data-creation",
    "title": "Logistic Regression",
    "section": "Data Creation",
    "text": "Data Creation\nFirst, I import my model alongside torch, numpy, and matplotlib.\n\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nThen, using the classification_data method provided in the assignment, I generate linearly inseparable data to test the logstic regression model on.\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)"
  },
  {
    "objectID": "posts/Logistic/index.html#visualization-methods",
    "href": "posts/Logistic/index.html#visualization-methods",
    "title": "Logistic Regression",
    "section": "Visualization Methods",
    "text": "Visualization Methods\nNext, I bring in methods from the perceptron assignment to plot the data and draw lines for decision boundaries. This will be important for the first experiment to show the model’s weight vector changing over time.\n\n# from perceptron lecture\n\ndef plot_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nThese next two methods, also taken from the perceptron assignment, are adapted to better suit these experiments. The plot_decisions function only shows the first and last five decision boundary changes (instead of all changes like in the perceptron). Both were also changed to take in their necessary vectors in order to reduce the amount of times they’re repeated in the code.\n\n# plots the first and last 5 iterations of the decision boundary\ndef plot_decisions(X, y, weight_vec, loss_vec):\n        # set up the figure\n        current_ax = 0\n        plt.rcParams[\"figure.figsize\"] = (10, 8)\n        fig, axarr = plt.subplots(2, 5, sharex = True, sharey = True)\n\n        # iter through weight_vec\n\n        for i in range(5):\n                ax = axarr.ravel()[current_ax]\n                plot_data(X, y, ax)\n                draw_line(weight_vec[i], x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n                draw_line(weight_vec[i+1], x_min = -1, x_max = 2, ax = ax, color = \"black\")\n                ax.set_title(f\"loss = {loss_vec[i]:.3f}\")\n                ax.set(xlim = (-1, 2), ylim = (-1, 2))\n                current_ax += 1\n        \n        for i in range(len(weight_vec) - 6, len(weight_vec) - 1):\n                ax = axarr.ravel()[current_ax]\n                plot_data(X, y, ax)\n                draw_line(weight_vec[i], x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n                draw_line(weight_vec[i+1], x_min = -1, x_max = 2, ax = ax, color = \"black\")\n                ax.set_title(f\"loss = {loss_vec[i]:.3f}\")\n                ax.set(xlim = (-1, 2), ylim = (-1, 2))\n                current_ax += 1\n        fig.suptitle(\"LR Decision Boundaries over Time\")\n\n\ndef plot_loss(loss_vec):\n    plt.plot(loss_vec, color = \"slategrey\")\n    plt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\n    labs = plt.gca().set(xlabel = \"LR Iteration (Updates Only)\", ylabel = \"loss\")\n    plt.title(\"LR Loss Iterations over Time\")"
  },
  {
    "objectID": "posts/Logistic/index.html#training-method",
    "href": "posts/Logistic/index.html#training-method",
    "title": "Logistic Regression",
    "section": "Training Method",
    "text": "Training Method\nThis training method takes in the data X, the target values y, the learning rate alpha, and the momentum value beta (alongside an optional num_steps parameter). It then runs the optimizer on the model for num_steps amount of time, updating the model’s weight vector each step. Further, there is included code to keep track of the weight and loss changes. The model, its losses, and its weights are returned for later use and visualization.\n\ndef train_LR(X, y, alpha, beta, num_steps=100): \n    LR = LogisticRegression() \n    opt = GradientDescentOptimizer(LR)\n\n    # initialize for main loop\n    loss = 1\n    loss_vec = []\n    weight_vec = [torch.rand(X.size()[1])]\n\n    for _ in range(num_steps):\n        prev_loss = loss\n        loss = LR.loss(X, y)\n\n        opt.step(X, y, alpha, beta)\n\n        if (prev_loss != loss):\n            loss_vec.append(loss)\n            weight_vec.append(LR.w)\n            \n    return LR, (loss_vec, weight_vec)"
  },
  {
    "objectID": "posts/Logistic/index.html#part-b-experiments",
    "href": "posts/Logistic/index.html#part-b-experiments",
    "title": "Logistic Regression",
    "section": "Part B: Experiments",
    "text": "Part B: Experiments\nFor the first experiment, the training method is run with two dimensional data, a learning rate of 0.1, and no momentum. To achieve a low loss value, the training loop is run for 1000 steps.\n\nLR, (loss_vec, weight_vec) = train_LR(X, y, 0.1, 0.0, num_steps=1000)\n\nPlotting the decision boundaries, we can see a significant increase in accuracy over time with a decision boundary that more cleanly separates the data.\n\nplot_decisions(X, y, weight_vec, loss_vec)\n\n\n\n\n\n\n\n\nIf we plot the loss over time, we similarly see its reduction. This plot makes it clear how the loss decreases in a logistic manner, starting to plateau near the minimum loss value (since the data is linearly inseparable, the minimum is not 0).\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nFor the next experiment, I used the same data and learning rate, but changed the momentum from 0 to 0.9. Here, we can see that the loss reaches a low value much faster than the model without momentum (in 100 steps rather than 1000).\n\nLR, (loss_vec, weight_vec) = train_LR(X, y, 0.1, 0.9, num_steps=100)\n\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nFinally, this experiment is made to show the effect on high-dimensional data. Since the experiment intends to test accuracy, I first made a function that determines a model’s accuracy (rather than loss) by comparing the model’s prediction results to the y target data. Then, I made a separate training loop that stops once the accuracy on the training data is 100%.\n\ndef accuracy(model, X, y):\n    return (1.0 * (model.predict(X) == y)).mean()\n\ndef train_LR_acc(X, y, alpha, beta): \n    LR = LogisticRegression() \n    opt = GradientDescentOptimizer(LR)\n\n    # initialize for main loop\n    acc_vec = []\n\n    while (True):\n        opt.step(X, y, alpha, beta)\n\n        acc = accuracy(LR, X, y)\n        acc_vec.append(acc)\n\n        if (accuracy(LR, X, y) == 1.):\n            break\n            \n    return LR, acc_vec\n\nSince this test was for overfitting on data where the number of points is half the amount of dimensions (p == 2n), I then created new data (training and testing), both with 100 dimensions and 50 points of data.\n\nX_train, y_train = classification_data(p_dims = 100, n_points = 50)\nX_test, y_test = classification_data(p_dims = 100, n_points = 50)\n\nNext, I trained the model on the training data using the new accuracy-halting training function.\n\nLR, acc_vec = train_LR_acc(X_train, y_train, 0.1, 0.8)\n\nWe can then check the accuracy of the model on the training and testing data by calling the accuracy function. Of course, since the function ensured that training accuracy is equal to 100 before returning, the first function call returns 1.\n\naccuracy(LR, X_train, y_train)\n\ntensor(1.)\n\n\nHere, we can see that a small amount of overfitting has occurred, the model is not 100% accurate on both the training and testing data:\n\naccuracy(LR, X_test, y_test)\n\ntensor(0.9400)\n\n\nI also adapted the plot_loss function to plot accuracy over time:\n\ndef plot_acc(acc_vec):\n    plt.plot(acc_vec, color = \"slategrey\")\n    plt.scatter(torch.arange(len(acc_vec)), acc_vec, color = \"slategrey\")\n    labs = plt.gca().set(xlabel = \"LR Iteration (Updates Only)\", ylabel = \"accuracy\")\n    plt.title(\"LR Accuracy Iterations over Time\")\n\nFor reference, we can see how the accuracy increases over time, noting that it does not take many iterations for the model to achieve 100% accuracy on this low-size training data.\n\nplot_acc(acc_vec)\n\n\n\n\n\n\n\n\nAs we increase the amount of noise in the data (the default being 0.2), the level of overfitting to the training data increases. As we can see here, when the model achieves 100% accuracy on the training data, it only yields 62% accuracy on the testing data.\n\nX_train, y_train = classification_data(p_dims = 100, n_points = 50, noise=5)\nX_test, y_test = classification_data(p_dims = 100, n_points = 50, noise=5)\nLR, acc_vec = train_LR_acc(X_train, y_train, 0.1, 0.8)\naccuracy(LR, X_train, y_train)\n\ntensor(1.)\n\n\n\naccuracy(LR, X_test, y_test)\n\ntensor(0.6200)"
  },
  {
    "objectID": "posts/Logistic/index.html#discussion",
    "href": "posts/Logistic/index.html#discussion",
    "title": "Logistic Regression",
    "section": "Discussion:",
    "text": "Discussion:\nOver the course of this blog post, I built a custom logistic regression model and performed experiments to figure out how it performs under different parameters. In doing so, I found that adding momentum drastically increases the speed of the model. For instance, a model with no momentum achieved a similar loss with 1000 training steps compared to a model using momentum with 100 steps. I also tested the model’s behavior on data where the number of dimensions is double the number of data points. In doing so, I found that the amount of overfitting is often dependent on the amount of noise in the data. While there is some overfitting in less noisy data, the amount of overfitting (and resulting lack of accuracy on the testing data) drastically increases when noise does. In the process of making this model, I learned more about gradient functions and step functions and got to practice the full pipeline of creating, training, and testing models in Python."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Logistic Regression\n\n\n\n\n\nCreating a model for logistic regression in Python.\n\n\n\n\n\nMay 12, 2024\n\n\nJeff Blake\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptron\n\n\n\n\n\nCreating a perceptron implementation and experimenting on its performance on different datasets (linearly separable/non-separable), alongside experiments on minibatch sizes\n\n\n\n\n\nApr 28, 2024\n\n\nJeff Blake\n\n\n\n\n\n\n\n\n\n\n\n\nBias Replication Study\n\n\n\n\n\nReplicating a study on the biases of predictive healthcare models\n\n\n\n\n\nApr 15, 2024\n\n\nJeff Blake\n\n\n\n\n\n\n\n\n\n\n\n\n\"Optimal\" Decision Making\n\n\n\n\n\nMaking “optimal” decisions based on loan-default data and model training. The definition of optimal, though, becomes flawed with profit incentives.\n\n\n\n\n\nMar 27, 2024\n\n\nJeff Blake\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nHow I made models that could predict the species from the Palmer Penguins dataset.\n\n\n\n\n\nFeb 28, 2023\n\n\nJeff Blake\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Testing quarto!"
  },
  {
    "objectID": "posts/bias-replication/bias_replication.html",
    "href": "posts/bias-replication/bias_replication.html",
    "title": "Bias Replication Study",
    "section": "",
    "text": "This blog post aims to replicate the study by Obermeyer et al. on the racial bias that exists in healthcare systems. In doing so, I compare the risk percentile assigned to black patients and white patients alongside their number of chronic illnesses. This recreation found that black patients were given lower risk scores even if they had the same number of chronic illnesses as white patients. I also compared medical expenditure with percentile risk score and number of chronic illnesses separately. This showed that, while costs were around the same by race when looking at different risk percentages, there was less expenditures for black patients when compared to white patients on the axis of chronic illnesses. This is further proven through the modelling done on the logarithmic cost, which estimates an expenditure for black patients at 75% of white patients. If care is supplied based on cost, then, we can see how unfair outcomes by race can arise.\nFirst, I imported the dataset into a dataframe with pandas.\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\nLet’s look at the data’s columns.\n\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\nThen, I created a risk percentile out of the data’s risk score by using pandas’ qcut feature. I grouped the data by this percentile and race, then looked observed the mean number of chronic illnesses per group.\n\nimport numpy as np\n\ndf[\"risk_percentile\"] = pd.qcut(df[\"risk_score_t\"], 101, labels=False)\n\ndf[\"risk_percentile\"]\n\ngb = df.groupby([\"risk_percentile\", \"race\"])[\"gagne_sum_t\"].mean().unstack(level=1)\ngb\n\n\n\n\n\n\n\nrace\nblack\nwhite\n\n\nrisk_percentile\n\n\n\n\n\n\n0\n0.069444\n0.051724\n\n\n1\n0.240741\n0.149847\n\n\n2\n0.277778\n0.105495\n\n\n3\n0.266667\n0.073810\n\n\n4\n0.200000\n0.097514\n\n\n...\n...\n...\n\n\n96\n5.460526\n3.809877\n\n\n97\n4.882353\n4.137681\n\n\n98\n5.761194\n4.821853\n\n\n99\n6.135802\n4.891688\n\n\n100\n7.319149\n6.038012\n\n\n\n\n101 rows × 2 columns\n\n\n\nNext, I graphed the risk percentile and mean number of chronic conditions grouped by race, recreating figure 1 of the study.\n\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nplt.style.use('seaborn-v0_8-whitegrid')\n\nmarkers = [\"o\" , \",\"]\n\ncolumns = [\"black\", \"white\"]\n\nfig, ax = plt.subplots(1, 1)\n\nfor i in range(2):\n    to_plot = gb[columns[i]]\n    ax.scatter(to_plot.values, np.arange(0, 101, 1), c = f\"{['blue', 'orange'][i]}\", vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['black', 'white'][i]}\", cmap = \"BrBG\", marker = markers[i])\n    ax.legend()\n    ax.set(xlabel=\"Mean Number of Chronic Conditions\", ylabel = \"Percentile Risk Score\")\n\n\n\n\n\n\n\n\nThis graph shows that, if two patients have the same number of chronic illnesses but one is white and one is black, the white patient is much more likely to have a higher risk score. This, in turn, makes them more likely to be reccomended to the high-risk care management program when compared to a black patient with the same number of chronic conditions.\nAfter this, I used the cost column alongside the number of chronic illnesses and the previously created risk percentile to make two more graphs. The first graph shows the mean cost compared to the risk percentile grouped by race. The second compares the cost and the number of chronic illnesses, also grouped by race.\n\ngb_cost = df.groupby([\"risk_percentile\", \"race\"])[\"cost_t\"].mean().unstack(level=1)\nfig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n\nplt.yscale(\"log\")\n\ngb_chronic = df.groupby([\"gagne_sum_t\", \"race\"])[\"cost_t\"].mean().unstack(level=1)\n\nfor i in range(2):    \n    to_plot = gb_cost[columns[i]]\n    ax1.scatter(np.arange(0, 101, 1), to_plot.values, c = f\"{['blue', 'orange'][i]}\", vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['black', 'white'][i]}\", cmap = \"BrBG\", marker = markers[i])\n    ax1.legend()\n    ax1.set(xlabel = \"Percentile Risk Score\", ylabel=\"Mean Medical Expenditure\")\n\n    to_plot = gb_chronic[columns[i]]\n    ax2.scatter(np.arange(0, 18, 1), to_plot.values, c = f\"{['blue', 'orange'][i]}\", vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['black', 'white'][i]}\", cmap = \"BrBG\", marker = markers[i])\n    ax2.legend()\n    ax2.set(xlabel = \"Number of Chronic Illnesses\")\n\n\n\n\n\n\n\n\nThis graph shows that, while there isn’t a cost disparity across race when comparing againsst percentile risk score, there is one when looking at the number of chronic illnesses. If we look at the datapoints at and below 5 chronic illnesses, where most patients lie, there is a lower average cost per black patients when compared to white patients. Considering the risk assessment model is done according to cost rather than illness, one can see how black patients would be disproportionately denied access to the program.\nFinally, we can model this cost disparity by looking at the data. This modelling experiment is informed on the equation:\n\\(logcost \\approx w_b\\) \\(*\\) (patient is black) \\(+\\) intercept \\(+\\) $_{i=1} ^{k} w_k * \\((gagne sum)\\)^k$\nWhere \\(w_b\\) is the weight a model gives to the variable that a patient is black and \\(w_k\\) are the weights given to the polynomial data of the number of chronic illnesses.\nFrom this we also find that \\(e^{w_b}\\) is the percentage of average cost incurred by black patients compared to white patients\nTo start, we observe that 95% of the patients have 5 or less chronic illnesses, making it a good subset of the data to generalize over.\n\ndf[df[\"gagne_sum_t\"] &lt;= 5][\"gagne_sum_t\"].count()/df.shape[0]\n\n0.9553952115447688\n\n\nNext, we can create a new dataframe eithin that subset that creates a log-transform of the cost (ignoring any 0-cost patients). This allows us to look at data that spans orders of magnitude. Further, for the model to work, we must encode race as a number. In this case, 0 is coded to be white and 1 black.\n\ndf_log = df.query('gagne_sum_t &lt;= 5 & cost_t != 0')\ndf_log[\"log_cost\"] = np.log(df_log[\"cost_t\"])\n\ndf_log[\"race_num\"] = 1 * (df_log[\"race\"] == \"black\")\n\nThen we can take predictor and target columns of this data, where race and number of chronic illness are the predictors (X) and the log cost is the target (y).\n\npred_X = df_log[[\"race_num\", \"gagne_sum_t\"]]\ntarget_y = df_log[\"log_cost\"]\n\n\ndf_log\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\nrisk_percentile\nlog_cost\nrace_num\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n35\n7.090077\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n4\n3\n86\n7.863267\n0\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n3\n6.214608\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n11\n7.170120\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n1\n1\n98\n7.003065\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48779\n0.611517\n0\n800.0\n0.0\nNaN\nNaN\nNaN\n1.090000\n148.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n7\n6.684612\n0\n\n\n48780\n2.615933\n0\n2200.0\n0.0\n112.0\nNaN\n41.4\n0.810000\n172.0\nwhite\n...\n0\n1\n0\n0\n1\n1\n1\n46\n7.696213\n0\n\n\n48781\n1.358926\n0\n800.0\n0.0\n105.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n1\n0\n0\n1\n1\n0\n23\n6.684612\n0\n\n\n48782\n10.990318\n0\n1300.0\n0.0\n132.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n3\n3\n94\n7.170120\n0\n\n\n48783\n1.681671\n0\n4400.0\n0.0\n115.0\n5.6\n36.6\n0.940000\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n29\n8.389360\n0\n\n\n\n\n44748 rows × 163 columns\n\n\n\nThe function add_polynomial_features returns a dataset with exponential values of the number of chronic illnesses. This is useful for training models that act on non-linear data.\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nPerforming cross-validation on the degrees of the polynomial finds the most accurate to be a degree of 6.\n\nfrom sklearn.linear_model import LinearRegression\n\nrecord = (0, 0)\n\nfor degree in range (1, 15):\n    poly_X = add_polynomial_features(pred_X, degree)\n\n    LinR = LinearRegression()\n    LinR.fit(poly_X, target_y)\n    tempScore = LinR.score(poly_X, target_y)\n    # print(degree, tempScore)\n    if tempScore &gt; record[1]:\n        record = (degree, tempScore)\nrecord\n\n(6, 0.08800788381979874)\n\n\nIf we fit the model to this polynomial dataset and the target variable, we find can find the value of wb by looking at the first coefficient of our linear regression model.\n\npoly_X = add_polynomial_features(pred_X, 6)\nLinR = LinearRegression()\nLinR.fit(poly_X, target_y)\n# LinR.score(poly_X, target_y)\nLinR.coef_[0]\n\n-0.2827181024769897\n\n\nFinally, we can get an percentage of average cost paid by black patients when compared to white patients by calculating \\(e^{w_b}\\).\n\npercent = np.exp(LinR.coef_[0])\npercent\n\n0.7537322331639498\n\n\nThis percentage of around 75% makes sense with regard to the study. Specifically, the study notes that “Blacks generate lower costs than Whites – on average, $1801 less per year…” “… or $1144 less, if we instead hold constant the specific individual illnesses that contribute to the sum.” This roughly lines up with the model’s prediction that black people will have a 75% lower mean total medical expenditure compared to white people."
  },
  {
    "objectID": "posts/bias-replication/bias_replication.html#abstract",
    "href": "posts/bias-replication/bias_replication.html#abstract",
    "title": "Bias Replication Study",
    "section": "",
    "text": "This blog post aims to replicate the study by Obermeyer et al. on the racial bias that exists in healthcare systems. In doing so, I compare the risk percentile assigned to black patients and white patients alongside their number of chronic illnesses. This recreation found that black patients were given lower risk scores even if they had the same number of chronic illnesses as white patients. I also compared medical expenditure with percentile risk score and number of chronic illnesses separately. This showed that, while costs were around the same by race when looking at different risk percentages, there was less expenditures for black patients when compared to white patients on the axis of chronic illnesses. This is further proven through the modelling done on the logarithmic cost, which estimates an expenditure for black patients at 75% of white patients. If care is supplied based on cost, then, we can see how unfair outcomes by race can arise.\nFirst, I imported the dataset into a dataframe with pandas.\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\nLet’s look at the data’s columns.\n\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\nThen, I created a risk percentile out of the data’s risk score by using pandas’ qcut feature. I grouped the data by this percentile and race, then looked observed the mean number of chronic illnesses per group.\n\nimport numpy as np\n\ndf[\"risk_percentile\"] = pd.qcut(df[\"risk_score_t\"], 101, labels=False)\n\ndf[\"risk_percentile\"]\n\ngb = df.groupby([\"risk_percentile\", \"race\"])[\"gagne_sum_t\"].mean().unstack(level=1)\ngb\n\n\n\n\n\n\n\nrace\nblack\nwhite\n\n\nrisk_percentile\n\n\n\n\n\n\n0\n0.069444\n0.051724\n\n\n1\n0.240741\n0.149847\n\n\n2\n0.277778\n0.105495\n\n\n3\n0.266667\n0.073810\n\n\n4\n0.200000\n0.097514\n\n\n...\n...\n...\n\n\n96\n5.460526\n3.809877\n\n\n97\n4.882353\n4.137681\n\n\n98\n5.761194\n4.821853\n\n\n99\n6.135802\n4.891688\n\n\n100\n7.319149\n6.038012\n\n\n\n\n101 rows × 2 columns\n\n\n\nNext, I graphed the risk percentile and mean number of chronic conditions grouped by race, recreating figure 1 of the study.\n\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nplt.style.use('seaborn-v0_8-whitegrid')\n\nmarkers = [\"o\" , \",\"]\n\ncolumns = [\"black\", \"white\"]\n\nfig, ax = plt.subplots(1, 1)\n\nfor i in range(2):\n    to_plot = gb[columns[i]]\n    ax.scatter(to_plot.values, np.arange(0, 101, 1), c = f\"{['blue', 'orange'][i]}\", vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['black', 'white'][i]}\", cmap = \"BrBG\", marker = markers[i])\n    ax.legend()\n    ax.set(xlabel=\"Mean Number of Chronic Conditions\", ylabel = \"Percentile Risk Score\")\n\n\n\n\n\n\n\n\nThis graph shows that, if two patients have the same number of chronic illnesses but one is white and one is black, the white patient is much more likely to have a higher risk score. This, in turn, makes them more likely to be reccomended to the high-risk care management program when compared to a black patient with the same number of chronic conditions.\nAfter this, I used the cost column alongside the number of chronic illnesses and the previously created risk percentile to make two more graphs. The first graph shows the mean cost compared to the risk percentile grouped by race. The second compares the cost and the number of chronic illnesses, also grouped by race.\n\ngb_cost = df.groupby([\"risk_percentile\", \"race\"])[\"cost_t\"].mean().unstack(level=1)\nfig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n\nplt.yscale(\"log\")\n\ngb_chronic = df.groupby([\"gagne_sum_t\", \"race\"])[\"cost_t\"].mean().unstack(level=1)\n\nfor i in range(2):    \n    to_plot = gb_cost[columns[i]]\n    ax1.scatter(np.arange(0, 101, 1), to_plot.values, c = f\"{['blue', 'orange'][i]}\", vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['black', 'white'][i]}\", cmap = \"BrBG\", marker = markers[i])\n    ax1.legend()\n    ax1.set(xlabel = \"Percentile Risk Score\", ylabel=\"Mean Medical Expenditure\")\n\n    to_plot = gb_chronic[columns[i]]\n    ax2.scatter(np.arange(0, 18, 1), to_plot.values, c = f\"{['blue', 'orange'][i]}\", vmin = -0.5, vmax = 1.5, facecolors='none', edgecolors = \"darkgrey\", alpha = 0.5, label = f\"{['black', 'white'][i]}\", cmap = \"BrBG\", marker = markers[i])\n    ax2.legend()\n    ax2.set(xlabel = \"Number of Chronic Illnesses\")\n\n\n\n\n\n\n\n\nThis graph shows that, while there isn’t a cost disparity across race when comparing againsst percentile risk score, there is one when looking at the number of chronic illnesses. If we look at the datapoints at and below 5 chronic illnesses, where most patients lie, there is a lower average cost per black patients when compared to white patients. Considering the risk assessment model is done according to cost rather than illness, one can see how black patients would be disproportionately denied access to the program.\nFinally, we can model this cost disparity by looking at the data. This modelling experiment is informed on the equation:\n\\(logcost \\approx w_b\\) \\(*\\) (patient is black) \\(+\\) intercept \\(+\\) $_{i=1} ^{k} w_k * \\((gagne sum)\\)^k$\nWhere \\(w_b\\) is the weight a model gives to the variable that a patient is black and \\(w_k\\) are the weights given to the polynomial data of the number of chronic illnesses.\nFrom this we also find that \\(e^{w_b}\\) is the percentage of average cost incurred by black patients compared to white patients\nTo start, we observe that 95% of the patients have 5 or less chronic illnesses, making it a good subset of the data to generalize over.\n\ndf[df[\"gagne_sum_t\"] &lt;= 5][\"gagne_sum_t\"].count()/df.shape[0]\n\n0.9553952115447688\n\n\nNext, we can create a new dataframe eithin that subset that creates a log-transform of the cost (ignoring any 0-cost patients). This allows us to look at data that spans orders of magnitude. Further, for the model to work, we must encode race as a number. In this case, 0 is coded to be white and 1 black.\n\ndf_log = df.query('gagne_sum_t &lt;= 5 & cost_t != 0')\ndf_log[\"log_cost\"] = np.log(df_log[\"cost_t\"])\n\ndf_log[\"race_num\"] = 1 * (df_log[\"race\"] == \"black\")\n\nThen we can take predictor and target columns of this data, where race and number of chronic illness are the predictors (X) and the log cost is the target (y).\n\npred_X = df_log[[\"race_num\", \"gagne_sum_t\"]]\ntarget_y = df_log[\"log_cost\"]\n\n\ndf_log\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\nrisk_percentile\nlog_cost\nrace_num\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n35\n7.090077\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n4\n3\n86\n7.863267\n0\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n3\n6.214608\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n11\n7.170120\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n1\n1\n98\n7.003065\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48779\n0.611517\n0\n800.0\n0.0\nNaN\nNaN\nNaN\n1.090000\n148.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n7\n6.684612\n0\n\n\n48780\n2.615933\n0\n2200.0\n0.0\n112.0\nNaN\n41.4\n0.810000\n172.0\nwhite\n...\n0\n1\n0\n0\n1\n1\n1\n46\n7.696213\n0\n\n\n48781\n1.358926\n0\n800.0\n0.0\n105.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n1\n0\n0\n1\n1\n0\n23\n6.684612\n0\n\n\n48782\n10.990318\n0\n1300.0\n0.0\n132.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n3\n3\n94\n7.170120\n0\n\n\n48783\n1.681671\n0\n4400.0\n0.0\n115.0\n5.6\n36.6\n0.940000\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n29\n8.389360\n0\n\n\n\n\n44748 rows × 163 columns\n\n\n\nThe function add_polynomial_features returns a dataset with exponential values of the number of chronic illnesses. This is useful for training models that act on non-linear data.\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nPerforming cross-validation on the degrees of the polynomial finds the most accurate to be a degree of 6.\n\nfrom sklearn.linear_model import LinearRegression\n\nrecord = (0, 0)\n\nfor degree in range (1, 15):\n    poly_X = add_polynomial_features(pred_X, degree)\n\n    LinR = LinearRegression()\n    LinR.fit(poly_X, target_y)\n    tempScore = LinR.score(poly_X, target_y)\n    # print(degree, tempScore)\n    if tempScore &gt; record[1]:\n        record = (degree, tempScore)\nrecord\n\n(6, 0.08800788381979874)\n\n\nIf we fit the model to this polynomial dataset and the target variable, we find can find the value of wb by looking at the first coefficient of our linear regression model.\n\npoly_X = add_polynomial_features(pred_X, 6)\nLinR = LinearRegression()\nLinR.fit(poly_X, target_y)\n# LinR.score(poly_X, target_y)\nLinR.coef_[0]\n\n-0.2827181024769897\n\n\nFinally, we can get an percentage of average cost paid by black patients when compared to white patients by calculating \\(e^{w_b}\\).\n\npercent = np.exp(LinR.coef_[0])\npercent\n\n0.7537322331639498\n\n\nThis percentage of around 75% makes sense with regard to the study. Specifically, the study notes that “Blacks generate lower costs than Whites – on average, $1801 less per year…” “… or $1144 less, if we instead hold constant the specific individual illnesses that contribute to the sum.” This roughly lines up with the model’s prediction that black people will have a 75% lower mean total medical expenditure compared to white people."
  },
  {
    "objectID": "posts/bias-replication/bias_replication.html#discussion",
    "href": "posts/bias-replication/bias_replication.html#discussion",
    "title": "Bias Replication Study",
    "section": "Discussion",
    "text": "Discussion\nIn the process of replicating this study, I confirmed the findings that black patients are more likely to be placed in lower risk percentiles given equal prevalence of chronic illness when compared to white patients. I also confirmed that, while costs are relatively similar between patients of the same risk percentile and different races, costs are disparate (black patients have less cost) when comparing between chronic illness. This was then quantified through a model, with a prediction of black patients having 75% of the cost of white patients. With the risk score being calculated with respect to cost, then, disparate outcomes across races are seen considering a more realistic risk variable like chronic illness count.\nAs such, the discrimination criteria of calibration bias is present in the formation of the risk score. This is seen through the dissection of risk score when compared against chronic illness versus cost. While there isn’t much bias in calculating risk score across race when cost is considering factor, there is bias when considering risk score in relation to number of chronic illness. That is to say, those with similar risk scores will have similar costs across race (the second graph in the blog post and figure 3A in the study), but will have dissimilar numbers of chronic illnesses, with black patients having more chronic illnesses at similar risk scores on average (the first graph and figure 1 in the study). The model is then calibrated to the variable of cost rather than health outcome, a fact that is mentioned at the beginning of the study: “…assessing how well the algorithmic risk score is calibrated across race for health outcomes \\(H_{i,t}\\). We also ask how well the algorithm is calibrated for costs \\(C_{i,t}\\).”\nThrough this blog post, I improved my graphing skills, especially when needing to graph different groups on the same graph. I also improved my ability in modelling with polynomial datasets and transforming data to be more useful to my models and graphs (log-transforms, qcuts, etc.)."
  },
  {
    "objectID": "posts/newton-optimizer/index.html",
    "href": "posts/newton-optimizer/index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom logistic import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\n\n# from perceptron lecture\n\ndef plot_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\n# plots the first and last 5 iterations of the decision boundary\ndef plot_decisions(X, y, weight_vec, loss_vec):\n        # set up the figure\n        current_ax = 0\n        plt.rcParams[\"figure.figsize\"] = (10, 8)\n        fig, axarr = plt.subplots(2, 5, sharex = True, sharey = True)\n\n        # iter through weight_vec\n\n        for i in range(5):\n                ax = axarr.ravel()[current_ax]\n                plot_data(X, y, ax)\n                draw_line(weight_vec[i], x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n                draw_line(weight_vec[i+1], x_min = -1, x_max = 2, ax = ax, color = \"black\")\n                ax.set_title(f\"loss = {loss_vec[i]:.3f}\")\n                ax.set(xlim = (-1, 2), ylim = (-1, 2))\n                current_ax += 1\n        \n        for i in range(len(weight_vec) - 6, len(weight_vec) - 1):\n                ax = axarr.ravel()[current_ax]\n                plot_data(X, y, ax)\n                draw_line(weight_vec[i], x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n                draw_line(weight_vec[i+1], x_min = -1, x_max = 2, ax = ax, color = \"black\")\n                ax.set_title(f\"loss = {loss_vec[i]:.3f}\")\n                ax.set(xlim = (-1, 2), ylim = (-1, 2))\n                current_ax += 1\n        fig.suptitle(\"LR Decision Boundaries over Time\")\n\n\ndef plot_loss(loss_vec):\n    plt.plot(loss_vec, color = \"slategrey\")\n    plt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\n    labs = plt.gca().set(xlabel = \"LR Iteration (Updates Only)\", ylabel = \"loss\")\n    plt.title(\"LR Loss Iterations over Time\")\n\n\ndef train_LR_Grad(X, y, alpha, beta, num_steps=100): \n    LR = LogisticRegression() \n    opt = GradientDescentOptimizer(LR)\n\n    # initialize for main loop\n    loss = 1\n    loss_vec = []\n    weight_vec = [torch.rand(X.size()[1])]\n\n    for _ in range(num_steps):\n        prev_loss = loss\n        loss = LR.loss(X, y)\n\n        opt.step(X, y, alpha, beta)\n\n        if (prev_loss != loss):\n            loss_vec.append(loss)\n            weight_vec.append(LR.w)\n        if (loss == 0):\n            break\n    return LR, (loss_vec, weight_vec)\n\n\ndef train_LR_Newton(X, y, alpha, num_steps=100): \n    LR = LogisticRegression() \n    opt = NewtonOptimizer(LR)\n\n    # initialize for main loop\n    loss = 1\n    loss_vec = []\n    weight_vec = [torch.rand(X.size()[1])]\n\n    for _ in range(num_steps):\n        prev_loss = loss\n        loss = LR.loss(X, y)\n\n        opt.step(X, y, alpha)\n\n        if (prev_loss != loss):\n            loss_vec.append(loss)\n            weight_vec.append(LR.w)\n        if (loss == 0):\n            break\n    return LR, (loss_vec, weight_vec)\n\n\nLR_grad, (loss_vec_grad, weight_vec_grad) = train_LR_Grad(X, y, 0.1, 0, 1000)\nLR_new, (loss_vec_new, weight_vec_new) = train_LR_Newton(X, y, 100, 1000)\nplot_loss(loss_vec_grad)\n\n\n\n\n\n\n\n\n\nplot_loss(loss_vec_new)\n\n\n\n\n\n\n\n\n^ 1 and 2 experiments (separate out)\n3:\n\nLR_new, (loss_vec_new, weight_vec_new) = train_LR_Newton(X, y, 1000, 100)\n\n_LinAlgError: linalg.inv: The diagonal element 1 is zero, the inversion could not be completed because the input matrix is singular."
  },
  {
    "objectID": "posts/penguin-post/penguin.html",
    "href": "posts/penguin-post/penguin.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "This blog post aims to explore the Palmer’s Penguins dataset. This will be done with visualizations and models, leading to predictions of species on the data. In particular, I will perform cross-validation techniques on the data to see which models and parameters yield the highest accuracy predictions. After choosing the most accurate models, I will graph the model’s decision boundaries to learn about the predictions being made."
  },
  {
    "objectID": "posts/penguin-post/penguin.html#introduction",
    "href": "posts/penguin-post/penguin.html#introduction",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "This blog post aims to explore the Palmer’s Penguins dataset. This will be done with visualizations and models, leading to predictions of species on the data. In particular, I will perform cross-validation techniques on the data to see which models and parameters yield the highest accuracy predictions. After choosing the most accurate models, I will graph the model’s decision boundaries to learn about the predictions being made."
  },
  {
    "objectID": "posts/penguin-post/penguin.html#data-collection-and-preparation",
    "href": "posts/penguin-post/penguin.html#data-collection-and-preparation",
    "title": "Classifying Palmer Penguins",
    "section": "Data Collection and Preparation",
    "text": "Data Collection and Preparation\nTo classify the Palmer Penguins, I first had to import the data set into my Python environment:\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nI then checked the elements of set. This would give me an idea of what could be used for visualization and to train the models.\n\ntrain.keys()\n\nIndex(['studyName', 'Sample Number', 'Species', 'Region', 'Island', 'Stage',\n       'Individual ID', 'Clutch Completion', 'Date Egg', 'Culmen Length (mm)',\n       'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Sex',\n       'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)', 'Comments'],\n      dtype='object')\n\n\nLet’s look at the top of the data:\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nThis provided function prepares the data by dropping non-helpful elements (i.e. identifying or helpful information to scientists working with the data that isn’t useful to the model). Species values are also encoded into numerical values so they can be output by the computer. Useless values like “.” sex penguins and NA value entires are also dropped from the set. This function creates X_train and y_train data that will be used throughout the fitting process, with X_train being the valid columns of the dataframe and y_train being the species (result) column.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue"
  },
  {
    "objectID": "posts/penguin-post/penguin.html#cross-validation-testing",
    "href": "posts/penguin-post/penguin.html#cross-validation-testing",
    "title": "Classifying Palmer Penguins",
    "section": "Cross Validation Testing",
    "text": "Cross Validation Testing\nThe following code tests different models on a variety of elements in the data. The qualitative and quantitative columns are put into individual arrays and are matched in groups of three (one qualitative column and two quantitative columns) using the combinations function from itertools. The different models – logistic regression, decision tree, random forest, and support vector from sklearn’s library – are tested using cross-validation to ensure models that overfit aren’t chosen. For the support vector and decision tree classifiers, other parameters (gamma and max depth respectively) are iterated over to ensure the highest score possible. For each model fit, the columns used, score, and extra parameters are appended to their results array. These results are sorted in descending order by highest score and a truncated version of each is printed out at the end of fitting. This allows for me to choose the best model and elements to fit to predict the test data.\n\nfrom sklearn.exceptions import ConvergenceWarning\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nwarnings.filterwarnings(\"ignore\") # just ignores the convergence warning in this case\n\nresults_LR = []\nresults_SVC = []\nresults_DTC = []\nresults_RFC = []\n\nall_qual_cols = [\"Island\", \"Stage\", \"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)',\n                  'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\n    LR = LogisticRegression()\n\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    new_score_LR = cv_scores_LR.mean()\n    results_LR.append((new_score_LR, cols))\n    \n    for g in 10.0**np.arange(-5, 5):\n      SVM = SVC(gamma = g)\n      cv_scores_SVC = cross_val_score(SVM, X_train[cols], y_train, cv = 5)\n      new_score_SVC = cv_scores_SVC.mean()\n      results_SVC.append((new_score_SVC, cols, SVM.gamma))\n\n    for m in np.arange(2, 25):\n      DTC = DecisionTreeClassifier(max_depth = m)\n\n      cv_scores_DTC = cross_val_score(DTC, X_train[cols], y_train, cv = 5)\n      new_score_DTC = cv_scores_DTC.mean()\n      results_DTC.append((new_score_DTC, cols, DTC.max_depth))\n\n    RFC = RandomForestClassifier()\n\n    cv_scores_RFC = cross_val_score(RFC, X_train[cols], y_train, cv = 5)\n    new_score_RFC = cv_scores_RFC.mean()\n    results_RFC.append((new_score_RFC, cols))\n      \nprint(\"LR:\")\n# lamba key tip on tuples from https://docs.python.org/3/howto/sorting.html\nresults_LR.sort(reverse=True, key=lambda x : x[0])\nprint(results_LR[:3])\n\nprint(\"SVC:\")\nresults_SVC.sort(reverse=True, key=lambda x : x[0])\nprint(results_SVC[:3])\n\nprint(\"DTC:\")\nresults_DTC.sort(reverse=True, key=lambda x : x[0])\nprint(results_DTC[:3])\n\nprint(\"RFC:\")\nresults_RFC.sort(reverse=True, key=lambda x : x[0])\nprint(results_RFC[:3])\n\nLR:\n[(0.9922322775263952, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']), (0.9844645550527904, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']), (0.9726244343891401, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Delta 13 C (o/oo)'])]\nSVC:\n[(0.9805429864253394, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 0.1), (0.9687782805429863, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 0.1), (0.9649321266968325, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 1.0)]\nDTC:\n[(0.9765460030165913, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 8), (0.9765460030165913, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 9), (0.9765460030165913, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'], 10)]\nRFC:\n[(0.9883107088989442, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']), (0.9843891402714933, ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']), (0.9806184012066363, ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)'])]"
  },
  {
    "objectID": "posts/penguin-post/penguin.html#visualizations",
    "href": "posts/penguin-post/penguin.html#visualizations",
    "title": "Classifying Palmer Penguins",
    "section": "Visualizations",
    "text": "Visualizations\nBefore creating the models, I wanted to create the visualizations and tables out of the data I just received. I picked the data that I didn’t plan on using for the models, but still scored highly in the model fitting. This way, the data representations will be strongly related to the species of the penguin but won’t be redundant with the final models. I also had to choose between the different quantitative factors, as I felt that isolating to one factor would clearly show its relation to species. The data table, then, was made by grouping by qualitative factors and returning a quantitative factor.\n\ngb = train.groupby([\"Island\", \"Species\"])[\"Culmen Length (mm)\"].aggregate(\"mean\")\nprint(gb)\n\nIsland     Species                                  \nBiscoe     Adelie Penguin (Pygoscelis adeliae)          38.845455\n           Gentoo penguin (Pygoscelis papua)            47.073196\nDream      Adelie Penguin (Pygoscelis adeliae)          38.826667\n           Chinstrap penguin (Pygoscelis antarctica)    48.826316\nTorgersen  Adelie Penguin (Pygoscelis adeliae)          39.229268\nName: Culmen Length (mm), dtype: float64\n\n\nFor the visualizations, I chose two quantiative factors from high scoring models, using species as a color factor. This can be achieved easily with seaborn:\n\nimport seaborn as sns\n\nsns.scatterplot(data=train, x=\"Culmen Length (mm)\", y=\"Flipper Length (mm)\", hue=\"Species\")\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data=train, x=\"Flipper Length (mm)\", y=\"Delta 13 C (o/oo)\", hue=\"Species\")"
  },
  {
    "objectID": "posts/penguin-post/penguin.html#model-creation",
    "href": "posts/penguin-post/penguin.html#model-creation",
    "title": "Classifying Palmer Penguins",
    "section": "Model Creation",
    "text": "Model Creation\nTo get the models to work, I took the highest scoring three models from the previous cross validation tests. Here, I used sklearn’s logistic regression for two of the models and their random forest classifier for the last one.\n\nfrom sklearn.linear_model import LogisticRegression\n\nLR_cols_1 = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'] # LR\nLR_cols_2 = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE'] # LR\nRFC_cols = ['Culmen Length (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen'] # RFC\n\nLR_1 = LogisticRegression()\nLR_1.fit(X_train[LR_cols_1], y_train)\n\nLR_2 = LogisticRegression()\nLR_2.fit(X_train[LR_cols_2], y_train)\n\nRFC = RandomForestClassifier()\nRFC.fit(X_train[RFC_cols], y_train)\n\nFinally, the test data is prepared through the same prepare_data function. Then, the models are scored on the test data, with the logistic regression models achieving 100% accuracy!\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\n# Chose top three from above, tested on test data\n\nprint(\"LR1:\")\nprint(LR_1.score(X_test[LR_cols_1], y_test))\n\nprint(\"LR2:\")\nprint(LR_2.score(X_test[LR_cols_2], y_test))\n\nprint(\"RFC: \")\nprint(RFC.score(X_test[RFC_cols], y_test))\n\nLR1:\n1.0\nLR2:\n1.0\nRFC: \n0.9852941176470589\n\n\nThis provided plot_regions function takes in a model alongside the X and y columns of data and plots the its decision regions, allowing for easy visualization.\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nFinally, I plotted my two logistic regression models (the models that achieved 100% prediction accuracy) using the plot_regions function.\n\n\nplot_regions(LR_1, X_train[LR_cols_1], y_train)\nplot_regions(LR_2, X_train[LR_cols_2], y_train)"
  },
  {
    "objectID": "posts/penguin-post/penguin.html#discussion",
    "href": "posts/penguin-post/penguin.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nThrough the creation of this blog post, I discovered the best sets of prediction parameters for the given dataset and models (culmen length and depth alongside island of origin and sex, respectively) and showed that logistic regression decision boundaries are linear. Further, I showed other correlations that exist in the dataset – like the relation of flipper length and delta 13 C to species – through visualizations of the dataset. In doing this, I improved my skills with dataframes and data cleaning, an essential process for training models. Further, I gained practice with seaborn for visualizing data and learned how to train and cross-validate the prebuilt models from scikit-learn, leading to my first model-based predictions being made on a dataset."
  }
]